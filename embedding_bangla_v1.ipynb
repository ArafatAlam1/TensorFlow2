{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "embedding_bangla_v1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raqueeb/TensorFlow2/blob/master/embedding_bangla_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi3C6LhrrjKx",
        "colab_type": "text"
      },
      "source": [
        "## এমবেডিং, ওয়ার্ড এমবেডিং এবং বাংলায় টেক্সট অ্যানালাইসিস\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4TNlkGyLz4C",
        "colab_type": "text"
      },
      "source": [
        "একটা জিনিস ভেবে দেখেছেন কি? আমরা এ পর্যন্ত মেশিন লার্নিং মডেলের ইনপুট হিসেবে যা দিয়েছি তা সবই সংখ্যা। মনে করে দেখুন, এপর্যন্ত সব মডেলের ক্লাসিফিকেশন অথবা রিগ্রেশন এর জন্য যা দিয়েছি সব সংখ্যায় দিয়েছি। তার পাশাপাশি ইমেজ নিয়ে যখন কাজ করেছি তখনো কিন্তু ইমেজ (সেটা গ্রেস্কেল হোক আর কালার হোক - তার জন্য গ্রেস্কেল ইনটেনসিটি অথবা কালারের আরজিবি চ্যানেলের আউটপুট), সবকিছুই সংখ্যায় গিয়েছে। এর অর্থ হচ্ছে মেশিন লার্নিং/ডিপ লার্নিং মডেল সংখ্যা ছাড়া আর কিছু বোঝেনা। আর বুঝবেই বা কিভাবে? সেতো যন্ত্র। আর মানুষের সবকিছুই কমপ্লেক্স। \n",
        "\n",
        "সেদিক থেকে মানুষের ভাষা আরো অনেক কমপ্লেক্স। আমরা একেকজন একেক ভাষায় কথা বলি, ভাষাগুলোর মধ্যে সংযোগ/সিমিলারিটি এবং কি বলতে গিয়ে কি বলে ফেললাম এবং তার ফলাফল, তার পাশাপাশি অনেক শব্দ একটা ভাষায় যা বোঝায় সেটা অন্য ভাষায় তার বৈপরীত্য দেখায়। এখন আপনি বাংলায় কথা বললেও সেটার মধ্যে ৪০% বাইরের শব্দ ব্যবহার করলে তো আরো সমস্যা। আর এই কারণে টেক্সট নিয়ে কাজ করা বেশ কমপ্লেক্স। \n",
        "\n",
        "যেকোনো ল্যাঙ্গুয়েজে তার প্রতিটা শব্দের একটা অর্থ আছে। তবে এটার অর্থ অনেক সময় নির্ভর করে কনটেক্সটে বা শব্দটা বাক্যের মধ্যে কোথায় এই মুহূর্তে আছে। একই শব্দের আবার অনেকগুলো কনটেক্সচুয়াল অর্থ থাকে সে কারণে শব্দকে শুধুমাত্র শব্দ বা অক্ষর লেভেলে কাজ করলে হবে না। কারণ, ডিপ লার্নিং মডেল যেহেতু সংখ্যা ছাড়া কিছু বোঝেনা, সে কারণে একেক ভাষার একেক বুলি এবং বকাবকি এর পাশাপাশি সেই ভাষাগুলোকে ঠিকমতো সংখ্যায় ট্রান্সফার করা একটা চ্যালেঞ্জ এর কাজ অবশ্যই।\n",
        "\n",
        "## কেন টাইম সিরিজ নিয়ে আলাপ হয়নি?\n",
        "\n",
        "এই বইতে আমি ইচ্ছে করে ‘টাইম সিরিজ’ যোগ করিনি, কারণ সেটার এপ্লিকেশন লেভেল এখনো বেসিক লেভেলে নেই। তবে, এই রিকারেন্ট নিউরাল নেটওয়ার্ককে শিখিয়ে দিলে সে (আরএনএন) ফ্রি ফর্ম (ইচ্ছেমতো) টেক্সট জেনারেট করতে পারে। আমরা যেমন দেখেছি ‘এল এস টি এম’, (লঙ শর্ট টার্ম মেমোরি) নেটওয়ার্কে ‘শেক্সপিয়ার’ ক্লাসিক পড়তে দিলে, সে শেক্সপিয়ারের মতো আরেকটা ক্লাসিক লিখে ফেলেছে, যেখানে এই নেটওয়ার্কের মধ্যে শব্দ, বাক্য এবং ব্যাকরণ তৈরির কোন ধারণা নেই। কারণ, ডিপ লার্নিং প্রচুর ক্লাসিক বই পড়ে বুঝেছে কিভাবে শব্দ, বাক্য বা তার ব্যাকরণ ব্যবহার করতে হয় এর ভেতরে না ঢুকেই। টাইম সিরিজের ব্যাপারটা হচ্ছে সে পরের জিনিসটা প্রেডিক্ট করবে। আগের সময়ে কি ছিলো, সেটাকে ধরে এরপরে কি কি আসতে পারে সেটাই বলবে সে। না বুঝে। যদি শব্দ লেভেলে দেখি, ‘আমি’ এর পর কি আসতে পারে তাহলে ‘ভালো’ আসতে পারে, কারণ ‘আমি ভালো আছি’ একটা বহুল প্রচলিত বাক্য। \n",
        "\n",
        "বড় ব্যাপার হচ্ছে এই অক্ষর থেকেই ‘এল এস টি এম’ আস্তে আস্তে মানবিক ব্যাকরন এবং কিভাবে একটা বাক্য তৈরি করতে হয় সে ধরনের একটা ধারণা পেয়ে থাকে। এটা সে কোন কিছু বুঝে করে না। প্যাটার্ন থেকে করে। আর সে কারণেই এটার উপরে আমরা খুব একটা ভরসা করব না। আমরা চাইব মেশিনকে শেখাতে, যেভাবে মানুষ ভাষা, শব্দ, ব্যাকরণ শেখে। ওই একই কারণে অক্ষর লেভেলে টেক্সট জেনারেশন নিয়ে আমরা এই মুহূর্তে আলাপ করব না। ইন্টারনেটে দেখতে পারেন কিভাবে একেকটা ‘এল এস টি এম’ নেটওয়ার্ক শেক্সপিয়ারের মত বড় বড় নাটক লিখে ফেলছে। এটা আসলে সে একটা ক্লাসিক্যাল লেখার প্যাটার্ন দেখে তার পার্সপেক্টিভ থেকে লিখেছে। বুঝে লেখেনি। \n",
        "\n",
        "## ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং\n",
        "\n",
        "সেজন্য এর মধ্যে এসে যোগ হয়েছে ‘ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং’। ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং এর মধ্যে লিঙ্গুইস্টিকস, কম্পিউটার সায়েন্স, ইনফরমেশন ইঞ্জিনিয়ারিং এবং কৃত্রিম বুদ্ধিমত্তা ব্যাপারগুলো চলে এসেছে কাজের স্বার্থে। এমনিতেই মানুষ এবং যন্ত্রের মধ্যে একটা যোগসূত্র স্থাপন করা বেশ ঝামেলার ব্যাপার। ‘স্পিচ রিকগনিশন’ এর পাশাপাশি স্বয়ংক্রিয়ভাবে একটা ভাষা বুঝতে পারা এবং তার পাশাপাশি সেই ভাষায় বুঝে টেক্সট জেনারেট করা সহজ ব্যাপার নয়, যখন সবকিছুর পেছনে কাজ করে সংখ্যা। সবচেয়ে বড় কথা হচ্ছে আপনি একটা যন্ত্রকে শেখাচ্ছেন সে আপনার সাথে ন্যাচারালি যোগসুত্র স্থাপন করতে পারে। এই ‘ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং’ এর শুরুটা হচ্ছে সেই টেক্সটকে ঠিকমতো পড়তে পারা, সেটাকে নিজের মতো করে বোঝা এবং মানুষ যেভাবে একটা বাক্যের ‘ইনটেন্ট’ বুঝতে পারে সেভাবে তাকে বুঝিয়ে তার কাছ থেকে উত্তর বের করা। সহজ ব্যাপার নয়। তবে শুরু করতে হবে কোথাও। \n",
        "\n",
        "যেহেতু ‘ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং’ একটা বিশাল সাবজেক্ট, আমি এই বইতে ব্যাপারটা আনার ব্যর্থ চেষ্টা করব না। তবে কম্পিউটার কিভাবে ‘টেক্সট’ এর সাথে ‘ইন্টারঅ্যাক্ট’ করে সেটা নিয়েই দেখব আমরা সামনের চ্যাপ্টারগুলোতে। আমরা জানি মেশিন লার্নিং মডেল ভেক্টরকে ইনপুট হিসেবে নেয়। আর এই ভেক্টরগুলো হচ্ছে সংখ্যার অ্যারে। যখন আমরা টেক্সট মানে শব্দ এবং বাক্য নিয়ে আলাপ করব, তখন আমাদের প্রথম কাজ হবে এই স্ট্রিংগুলোকে কিভাবে সংখ্যায় পাল্টানো যায়। অর্থাৎ সেই বাক্য বা শব্দটি কিভাবে ‘ভেক্টরাইজ’ করা যায় মডেলে দেবার আগে। আমাদের এই চ্যাপ্টার থেকে শুরু করব কিভাবে আস্তে আস্তে ডিপ নিউরাল নেটওয়ার্ক দিয়ে বাংলার একটা অ্যাপ্লিকেশনে যাওয়া যায়।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaDQqrJg7p68",
        "colab_type": "code",
        "outputId": "6921cea9-45dc-410b-fa6a-1dd1daa93c8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  # শুধুমাত্র টেন্সর-ফ্লো ২.x ব্যবহার করবো \n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "keras = tf.keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0un3M2_MnHe",
        "colab_type": "text"
      },
      "source": [
        "আমরা যেহেতু ডিপ নিউরাল নেটওয়ার্কের কথা বলছি সেখানে সবকিছুতেই লেয়ার কনসেপ্ট কাজ করে। আজকে এখানে আমরা নতুন একটা লেয়ার নিয়ে কথা বলবো যেটাকে আমরা বলছি এম্বেডিং লেয়ার। শুরুতেই অনেকে বলবেন এম্বেডিং মানে কি? এর সাথে সংখ্যার সম্পর্ক কি?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4bim_ShMw8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "embedding_layer = layers.Embedding(1000, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRmBcA6iMx71",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c689d671-ea75-4c7b-de83-7448f279b944"
      },
      "source": [
        "result = embedding_layer(tf.constant([1,2,3]))\n",
        "result.numpy()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.0339667 , -0.04795448, -0.04234529,  0.02007512, -0.04314665],\n",
              "       [-0.00510416,  0.04672292,  0.0373845 , -0.0407817 , -0.03552971],\n",
              "       [ 0.03556449,  0.04361344,  0.04444368, -0.02098991, -0.03613399]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYLOnO5Uxmmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# অথবা টেন্সর-ফ্লো দিয়ে টোকেনাইজ করে দেখি\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mezzU-TxprR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "text = ['আমি এখন বই পড়ি']\n",
        "# text = tokenizer.fit_on_texts(text)\n",
        "sequences = tokenizer.texts_to_sequences(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVVdIpcqxrxB",
        "colab_type": "code",
        "outputId": "e22c464e-85a0-438e-85e4-fbf60ba7bdef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sequences"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHqel-xSNGyu",
        "colab_type": "text"
      },
      "source": [
        "## ‘এম্বেডিং’ সিমিলারিটি, একটা থেকে আরেকটা শব্দের দূরত্ব\n",
        "\n",
        "মেশিন লার্নিং এর ভাষায় ‘এম্বেডিং’ হচ্ছে সিমিলারিটি, যদি শব্দের কথা বলি তাহলে একটা শব্দ থেকে আরেকটা শব্দ কতটুকু যুক্ত বা একটা শব্দ থেকে আরেকটা শব্দ কত দূরে? তাদের মধ্যে সম্পর্ক আছে কিনা? যেমন, রাজা’র সাথে ‘রানী’ শব্দটা কিন্তু সম্পর্কযুক্ত। যেমন, ‘মা’ শব্দের সাথে ‘বাবা’ সম্পর্কযুক্ত। ‘বাংলাদেশ’ শব্দের সাথে ‘ঢাকা’ সম্পর্কযুক্ত। সেই থেকে ‘আকাশ’ শব্দের সাথে ‘টেবিল’ কিন্তু বহু দূরে মানে তাদের মধ্যে সম্পর্ক টানা বেশ কঠিন, যদি না কোনদিন ‘আকাশ’ থেকে ‘টেবিল’ পড়ে।\n",
        "\n",
        "এই এম্বেডিং ব্যাপারটা এসেছে কিছুটা ‘ওয়ান হট’ এনকোডিং থেকে। তবে, ব্যাপারটা ঠিক সেরকম নয়। শব্দকে যেহেতু আমাদেরকে সংখ্যায় পাল্টাতে হবে তাহলে আর বাকি উপায় কি? মনে আছে, আমাদের এই ‘ওয়ান হট’ এনকোডিং বা ‘ডামি ভেরিয়েবল’ ব্যাপারটা এসেছে শব্দ দিয়ে তৈরি ক্যাটাগরিকাল ভেরিয়েবল থেকে? মেশিন লার্নিং মডেলে আমরা যখন কিছু শব্দকে সংখ্যার ক্যাটাগরিতে ভাগ করতে চাই, যেটা এমুহুর্তে সংখ্যায় নেই। যেমন, আইরিশ ডাটাসেটের তিন প্রজাতির ফুলের জন্য ০,১,২, ভাগে ভাগ করতে পারছিনা, সেখানে চলে এসছে এই ডামি ভেরিয়েবল। আমরা যাকে বলছি শব্দকে দিয়ে তার জন্য একটা করে এনকোডিং। এর অর্থ হচ্ছে, আমাদের ভাষায় যতগুলো শব্দ আছে সেগুলোকে ‘ওয়ান হট’ এনকোডিং করা যেতে পারে। আর সেখানেই সমস্যা। \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/raqueeb/deep_learning_book/master/assets/hot.png\"> চিত্রঃ ‘ওয়ান হট’ এনকোডিং এর উদাহরণ\n",
        "\n",
        "এখানে একটা উদাহরণ দেয়া যাক। একটা বাক্য। ‘আমি এখন বই পড়ি’। এই চারটা শব্দকে যদি আমরা সংখ্যায় পাল্টাতে চাই, তাহলে আমাদের এই ভোকাবুলারির প্রতিটা শব্দকে ‘০’ ভেক্টর দিয়ে শুরু করব। আমাদের এখানে যেহেতু ৪টা ইউনিক শব্দ, সে কারণে এই শূন্য ভেক্টরের দৈর্ঘ্য হবে ৪। প্রতিটি শব্দকে ঠিকমতো রিপ্রেজেন্ট করার জন্য একেকটা শব্দের ইনডেক্সে তার করেসপন্ডিং ‘১’ বসাবো। ছবি দেখি। এখন এই টেবিল থেকে প্রতিটা শব্দের জন্য তার ভেক্টর বের করা সোজা। আমাদের এই চারটা শব্দের জন্য ভেক্টরের দৈর্ঘ্য হচ্ছে ৪ যার বাকি তিনটাই ০। এভাবে আমরা শব্দকে বিভিন্ন ক্যাটাগরিতে ভাগ করতে পারি। তবে ব্যাপারটা সেরকম এফিশিয়েন্ট নয় যখন আমাদের ভোকাবুলারিতে ১০ হাজার শব্দ থাকবে। তার মানে একেকটা ভেক্টরের দৈর্ঘ্য ১০ হাজার হবে - এর মধ্যে ৯৯.৯৯ শতাংশই হচ্ছে ০। এটা দক্ষ সিস্টেম না। আমরা যদি প্রতিটা বাংলা শব্দের জন্য ডামি ভ্যারিয়েবল বানাই, তাহলে কতো বড় ডামি ভ্যারিয়েবলের একেকটা টেবিল হবে?\n",
        "\n",
        "এই সমস্যা থেকে বের হবার উপায় কি? প্রতিটা শব্দকে একটা করে ইউনিক সংখ্যা দিয়ে দেওয়া। আমাদের আগের ভোকাবুলারিটাকে ‘আমি এখন বই পড়ি’কে আমরা একটা ‘ডেন্স’ ভেক্টরের মত আলাদা আলাদা করে সংখ্যা দিয়ে দিতে পারি। এই জিনিসটা আগের থেকে অনেকটাই এফিশিয়েন্ট। একেকটা শব্দের জন্য একেকটা আলাদা আলাদা সংখ্যা। সবচেয়ে বড় ব্যাপার হচ্ছে বিশাল ‘স্পার্স’ হাই-ডাইমেনশন স্পেস থেকে কম ডাইমেনশন স্পেসে চলে এলাম। তবে, এটার সমস্যা দুটো।\n",
        "\n",
        "* ১. আমাদের এই সংখ্যায় এনকোডিং সিস্টেমটাকে ‘ম্যানুয়ালি’ করা হয়েছে। ফলে এদের মধ্যে কোন ‘রিলেশনশিপ’ বের করা যাচ্ছে না। অংকেও এক সংখ্যা থেকে আরেক সংখ্যার মধ্যে যে রিলেশনশিপ সেটা অনুপস্থিত। ফলে আমাদের অংকের ভাষায় কো-সাইন ভেক্টরে কে কোথায় আছে সেটা বের করা মুশকিল।\n",
        "\n",
        "* ২. একটা মডেলের জন্য এই ধরনের ‘ম্যানুয়াল’ এনকোডিং শব্দগুলোর মধ্যে সম্পর্ক না বুঝলে সেটা সমস্যা হয়ে দাঁড়াবে। যেহেতু এই একেকটা ফিচারের জন্য একেকটা ‘ওয়েট’ সে কারণে একটা লিনিয়ার ক্লাসিফায়ারের পক্ষে এই ফিচার এবং ওয়েট এর কম্বিনেশন কোন সম্পর্ক দেখাবেনা। এটা বড় সমস্যা। \n",
        "\n",
        "আর সে কারণেই চলে এসেছে ওয়ার্ড এম্বেডিং।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awPlT5SRBH2G",
        "colab_type": "text"
      },
      "source": [
        "## একটা ছোট্ট উদাহরন, রেস্টুরেন্ট রিভিউ\n",
        "\n",
        "আমরা এখানে একটা এমবেডিং লেয়ারের উদাহরণ নিয়ে আসি। ইনপুট ডাইমেনশনে কতগুলো ভোকাবুলারি আছে? ধরে নিচ্ছি ৫০টা। আমি সংখ্যা না গুনেই বলছি। ৫০টা ডামি ভ্যারিয়েবল। মানে আমরা কতগুলো ক্যাটেগরিতে এনকোড করছি। আমাদের লুক আপ টেবিলে কতগুলো আইটেম আছে সেটাই এখানে আসবে। \n",
        "\n",
        "আমাদের ৫০টা ভোকাবুলারির জন্য 'ওয়ান হট' এনকোডিং ব্যবহার করছি, কেরাসের ভেতরে। এটা ব্যবহার করা ঠিক না তবে ছোটখাট উদাহরণে সেটা করা যায়। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEOvo2pjrjKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Embedding, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS7ZaxjhMJ0w",
        "colab_type": "text"
      },
      "source": [
        "আমরা এখানে ছোট একটা দশটা রেস্টুরেন্টের রিভিউ যোগ করেছি। এর পাশাপাশি তার পাঁচটা নেগেটিভ আর পাঁচটা পজেটিভ লেবেল যোগ করেছি। এর উপর আমরা নিউরাল নেটওয়ার্কে ট্রেইন করব। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ecoq69LrjK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ১০টা েস্টুরেন্ট রিভিউএর ইনপুট\n",
        "reviews = [\n",
        "    'আমি আর আসছি না এখানে!',\n",
        "    'একদম বাজে সার্ভিস',\n",
        "    'কথা শোনে না ওয়েটার',\n",
        "    'একদম ঠান্ডা খাবার',\n",
        "    'বাজে খাবার!',\n",
        "    'অসাধারণ',\n",
        "    'অসাধারণ সার্ভিস!',\n",
        "    'খুব ভালো!',\n",
        "    'মোটামুটি',\n",
        "    'এর থেকে ভালো হয়না']\n",
        "\n",
        "# লেবেল বলে দিচ্ছি (1=নেগেটিভ, 0=পজেটিভ)\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CxmAlaErjK3",
        "colab_type": "code",
        "outputId": "ac42d0b2-2cdb-4fa2-ae0a-b9b2ca2cf739",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(reviews[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "আমি আর আসছি না এখানে!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59EA_3KSKOos",
        "colab_type": "text"
      },
      "source": [
        "শব্দের সংখ্যা না গুনেও একটা ধারনা নিয়েছি ৫০টা মানে ভোকাবুলারি সাইজ ৫০। ছোট্ট উদাহরণ বলে আমরা ‘ওয়ান হট এনকোডিং’ ব্যবহার করছি। এর কাজ হচ্ছে এই বাক্যগুলোকে শব্দে ভেঙে ফেলা। যাকে আমরা বলছি টোকেনাইজিং। এরপর তাকে নিজস্ব ইনডেক্সে পাঠাবে। শেষে আমরা যা পাব সেটা সংখ্যার সিকোয়েন্স। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZk5g6PrjK5",
        "colab_type": "code",
        "outputId": "3cacd0e7-4f3c-4226-c53f-abf7d0e08d53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ওয়ান হট এনকোডিং\n",
        "\n",
        "VOCAB_SIZE = 50\n",
        "encoded_reviews = [one_hot(d, VOCAB_SIZE) for d in reviews]\n",
        "print(f\"Encoded reviews: {encoded_reviews}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded reviews: [[18, 27, 23, 48, 18], [30, 23, 18], [6, 49, 48, 48], [30, 46, 17], [23, 17], [47], [47, 18], [39, 42], [14], [41, 21, 42, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok0ds1g-LOOl",
        "colab_type": "text"
      },
      "source": [
        "যতগুলো শব্দ ততগুলো করে সংখ্যা একেকটা অংশে। তবে এই সিকোয়েন্সে একটা কনসিস্টেন্সি রাখার জন্য একটা লেনথ ঠিক করে দিতে হবে। শব্দ ধরে এখন কোনটা ২ অথবা ৩ এবং ৪ লেনথে, কিন্তু প্যাডিং এর জন্য আমরা MAX_LENGTH = 4 ধরে দিচ্ছি। দেখুন এখানে বাকি অংশ ০ দিয়ে ভরে দিয়েছে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFLBnh2erjK8",
        "colab_type": "code",
        "outputId": "0aac829f-91ff-47e3-fbc7-1efe4181516d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "MAX_LENGTH = 4\n",
        "\n",
        "padded_reviews = pad_sequences(encoded_reviews, maxlen=MAX_LENGTH, padding='post')\n",
        "print(padded_reviews)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[27 23 48 18]\n",
            " [30 23 18  0]\n",
            " [ 6 49 48 48]\n",
            " [30 46 17  0]\n",
            " [23 17  0  0]\n",
            " [47  0  0  0]\n",
            " [47 18  0  0]\n",
            " [39 42  0  0]\n",
            " [14  0  0  0]\n",
            " [41 21 42  1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-10aiXydLsei",
        "colab_type": "text"
      },
      "source": [
        "আমরা আগের মতো সিকোয়েন্সিয়াল নেটওয়ার্ক তৈরি করছি। আউটপুট ডাইমেনশনে কমিয়ে এনেছি সেটা। এখানে একটা ডেন্স লেয়ার, তার মানে একটা ওয়েট ম্যাট্রিক্স, একটা লার্নিং চলছে এখানে। এই এমবেডিং লেয়ারে যে লার্নিং চলছে সেটা শব্দগুলোকে একটা ইউক্লুডিয়ান স্পেসে সংখ্যাগুলোর মধ্যে একটা রিলেশনশিপ তৈরি হচ্ছে।\n",
        "\n",
        "মডেলটা দেখুন।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBM4Eu7OrjLJ",
        "colab_type": "code",
        "outputId": "ad075aba-c48f-4612-d23e-af77f5d847ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model = Sequential()\n",
        "embedding_layer = Embedding(VOCAB_SIZE, 8, input_length=MAX_LENGTH)\n",
        "model.add(embedding_layer)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 4, 8)              400       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVDOD7q7rjLO",
        "colab_type": "code",
        "outputId": "d7e90233-76f7-4057-d1e1-1d0d4ebb5400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# মডেলকে ট্রেইন করি\n",
        "model.fit(padded_reviews, labels, epochs=100, verbose=0)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd0c016c160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQpHhhrbOhdM",
        "colab_type": "text"
      },
      "source": [
        "১০০ ইপকের পর ভালো করে লক্ষ্য করুন, প্রতিটা লাইন হচ্ছে একেকটা শব্দের এমবেডিং। এখানে ওয়েটগুলোর অর্থ এখন খুঁজতে যাবো না এমুহুর্তে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnaII1xnrjLS",
        "colab_type": "code",
        "outputId": "436a3ead-ca83-4909-9695-baf182d9052b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(embedding_layer.get_weights()[0].shape)\n",
        "print(embedding_layer.get_weights())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 8)\n",
            "[array([[ 0.08445652, -0.09346382, -0.1153101 , -0.08687168,  0.12489384,\n",
            "         0.06159101, -0.15283671,  0.16201596],\n",
            "       [ 0.08379133, -0.11405947, -0.14295566, -0.10899172,  0.14112172,\n",
            "        -0.10753997,  0.05359931,  0.12398995],\n",
            "       [-0.00186111,  0.00924151,  0.03309018, -0.0285947 , -0.01509882,\n",
            "        -0.04974305,  0.00719942,  0.0233702 ],\n",
            "       [ 0.02450119, -0.03862574, -0.01956166,  0.01122971, -0.02732921,\n",
            "        -0.01081762, -0.04372646, -0.03618447],\n",
            "       [ 0.03053823, -0.01402362,  0.01821253, -0.01614489,  0.04108104,\n",
            "        -0.01281965, -0.02603277, -0.01150078],\n",
            "       [ 0.04556557,  0.01202744, -0.04501692, -0.00029296,  0.02726028,\n",
            "         0.04015929,  0.00935964, -0.0383476 ],\n",
            "       [-0.13580176, -0.09986364,  0.1245888 ,  0.10950329, -0.05969741,\n",
            "        -0.13374738,  0.11801434,  0.07084785],\n",
            "       [-0.03462787, -0.04161174, -0.00193647, -0.01973833,  0.00555579,\n",
            "        -0.0073867 ,  0.04086855,  0.03668931],\n",
            "       [ 0.03034202, -0.03550383,  0.01019089, -0.01684018, -0.01138564,\n",
            "        -0.03722174,  0.02334425,  0.02600244],\n",
            "       [-0.01782309, -0.03515874,  0.01509918,  0.01448733,  0.03763426,\n",
            "         0.04277556, -0.0483937 ,  0.03370724],\n",
            "       [-0.01864344,  0.01054728, -0.03838096,  0.03708197, -0.00174131,\n",
            "        -0.03475953, -0.02147008, -0.04610926],\n",
            "       [ 0.04927825,  0.01823213, -0.04803229,  0.03793669,  0.03465266,\n",
            "        -0.01268524,  0.04656697, -0.01875308],\n",
            "       [ 0.03507204,  0.03021843,  0.03101729,  0.04129894, -0.00353774,\n",
            "         0.00424438, -0.04779046,  0.04055159],\n",
            "       [-0.01492669,  0.00558136,  0.0396063 ,  0.02567477, -0.03335238,\n",
            "        -0.03839403, -0.00174684,  0.03032352],\n",
            "       [ 0.14647833,  0.13215649, -0.11588909, -0.13516669,  0.05192298,\n",
            "         0.09464284, -0.08781706, -0.11693494],\n",
            "       [ 0.0043363 ,  0.02091683, -0.00626219,  0.01686653, -0.0327148 ,\n",
            "        -0.00318237, -0.04525847,  0.04396423],\n",
            "       [ 0.04012238, -0.00246928, -0.01748494,  0.01347711, -0.00563569,\n",
            "        -0.01146225,  0.00371287, -0.01090829],\n",
            "       [-0.06281376, -0.06462914,  0.12944782,  0.1555098 , -0.11782612,\n",
            "        -0.06493966,  0.14964665,  0.06714069],\n",
            "       [-0.11256032,  0.1291553 , -0.07510326, -0.04701061, -0.14088023,\n",
            "         0.10547351,  0.154619  , -0.1371238 ],\n",
            "       [-0.00632709, -0.02009482,  0.00093987,  0.04629529,  0.0147012 ,\n",
            "         0.01107226,  0.00844356, -0.0199445 ],\n",
            "       [ 0.0300048 , -0.01220687,  0.03148028,  0.04410524,  0.02821238,\n",
            "         0.02893319,  0.00541944,  0.04976596],\n",
            "       [ 0.06171722,  0.07194246, -0.09963364, -0.05164581, -0.08436965,\n",
            "         0.12824573,  0.00404393, -0.10444313],\n",
            "       [-0.02806902,  0.04332988, -0.03125665,  0.01586472, -0.04809911,\n",
            "        -0.02649456, -0.04386466,  0.00259247],\n",
            "       [-0.07529569, -0.13388664,  0.09024495,  0.05910068, -0.13874783,\n",
            "        -0.12642449,  0.13412637,  0.04640692],\n",
            "       [-0.02204589, -0.03568221, -0.04578415, -0.02503923, -0.03328948,\n",
            "        -0.00132366, -0.04011833, -0.03803326],\n",
            "       [-0.01305506, -0.00606529, -0.00737615, -0.00452902, -0.0306281 ,\n",
            "         0.00109341, -0.04935988,  0.04109276],\n",
            "       [-0.00208379, -0.0155655 ,  0.04516334, -0.01084334, -0.00337071,\n",
            "        -0.00649982,  0.00426973,  0.04485694],\n",
            "       [-0.05973502, -0.16552429,  0.05100902,  0.05660563, -0.05017735,\n",
            "        -0.08926029,  0.08085939,  0.06522379],\n",
            "       [-0.00755249, -0.04360185, -0.00441288, -0.02080579,  0.04037927,\n",
            "        -0.00088171, -0.0236199 , -0.01609542],\n",
            "       [-0.03480005, -0.03472563,  0.04753936,  0.02621129, -0.02807474,\n",
            "        -0.02177458,  0.04267355, -0.02265312],\n",
            "       [-0.12453236, -0.15523449,  0.05001187,  0.09206084, -0.08324421,\n",
            "        -0.13988374,  0.14849916,  0.1230205 ],\n",
            "       [-0.03017879,  0.01771047, -0.00670953,  0.02483933,  0.02755591,\n",
            "         0.033303  , -0.00275115,  0.04727778],\n",
            "       [ 0.01398379, -0.02665786, -0.04877567, -0.00871806,  0.04117778,\n",
            "        -0.04992492,  0.04433807,  0.00301765],\n",
            "       [ 0.01098514, -0.04091139, -0.03168316, -0.00994565, -0.02217481,\n",
            "        -0.04010283, -0.00198912, -0.02012138],\n",
            "       [ 0.04695028, -0.00893924, -0.03982973,  0.04432004, -0.04989675,\n",
            "        -0.01409013, -0.00072403, -0.02502413],\n",
            "       [ 0.0007058 ,  0.04067716,  0.0286249 ,  0.00177075, -0.03959855,\n",
            "        -0.00631787, -0.04955935,  0.04365872],\n",
            "       [ 0.01786916,  0.02498722, -0.00366546, -0.00091177,  0.00926037,\n",
            "        -0.0065196 , -0.02352407,  0.01291705],\n",
            "       [ 0.03922602,  0.04806473, -0.01189618,  0.0313418 ,  0.01647948,\n",
            "        -0.01951884,  0.0156103 ,  0.01758213],\n",
            "       [-0.01232153,  0.04085496, -0.01185464,  0.04948903,  0.01345718,\n",
            "        -0.00895287, -0.02396156, -0.03842432],\n",
            "       [ 0.0898768 ,  0.11228142, -0.08146394, -0.12514044,  0.06022448,\n",
            "         0.11549897, -0.10940307, -0.10888475],\n",
            "       [ 0.01504563, -0.04962419, -0.03074594,  0.02880088, -0.01112189,\n",
            "         0.02221265,  0.01645006,  0.01809156],\n",
            "       [ 0.07586189,  0.0825465 , -0.13091575, -0.12055113,  0.07261141,\n",
            "         0.06883164, -0.11215764, -0.07975178],\n",
            "       [ 0.14738509, -0.04444263, -0.08705999, -0.1426208 ,  0.13703753,\n",
            "         0.11334578, -0.09439424, -0.03475396],\n",
            "       [-0.01198994, -0.03043079,  0.04422853,  0.00111809,  0.0028944 ,\n",
            "        -0.04219893,  0.0067734 , -0.0164622 ],\n",
            "       [ 0.01728718,  0.03384146, -0.00531546,  0.03241172,  0.02543676,\n",
            "        -0.04716709, -0.04533749,  0.02263806],\n",
            "       [-0.02748339,  0.04507471,  0.01978396,  0.04732488,  0.02409215,\n",
            "         0.03119839, -0.03548794, -0.00546569],\n",
            "       [-0.10364499, -0.07014279,  0.05355113,  0.08174849,  0.08561265,\n",
            "        -0.10196391,  0.04413695,  0.05704407],\n",
            "       [ 0.05498186,  0.15686621, -0.10026896, -0.12934522,  0.07231759,\n",
            "         0.08391774, -0.13945276, -0.09701125],\n",
            "       [-0.06683259,  0.06129658,  0.03125059,  0.12115926, -0.09845247,\n",
            "         0.13992329,  0.08473587, -0.11699138],\n",
            "       [-0.08307516, -0.09076504,  0.14265145,  0.10629835,  0.04887107,\n",
            "        -0.13336667,  0.03648433,  0.1014622 ]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQAiapl6OdFO",
        "colab_type": "text"
      },
      "source": [
        "অ্যাক্যুরেসি কেন ১ এসেছে? কারণ, দশটা বাক্যের মধ্যে সবগুলোই ইউনিক। একটার সাথে আরেকটার মিল নেই। আর, এতো ছোট উদাহরণে আর কি হবে?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPvlpHsNrjLU",
        "colab_type": "code",
        "outputId": "766b8d26-0063-4a9e-b8a2-f35777ab8aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss, accuracy = model.evaluate(padded_reviews, labels, verbose=0)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVYFsXuIbHK0",
        "colab_type": "text"
      },
      "source": [
        "আরেকটা উদাহরণ দেখি। এসেছে স্ট্যাকওভারফ্লো থেকে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plzrmT4xbUrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGF-jsBsbYE2",
        "colab_type": "text"
      },
      "source": [
        "এখানেও আমরা একটা বাক্যকে ক্লাসিফাই করবো। 'ব্যাগ অফ ওয়ার্ড' (একটা ব্যাগে যতো শব্দ আছে) হচ্ছে একটা মেকানিজম যার মাধ্যমে টেক্সট থেকে ফিচার বের করা যায়। এটা আসলে আমাদের এই তিনটা উদাহরন থেকে যতো ইউনিক শব্দ আছে সেটা থেকে সে ট্রেনিং নেয়। তবে, আমরা তার থেকেও ভালো মডেল মানে সিকোয়েন্সের ওপর জোর দেবো।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9yE6vy9cAtl",
        "colab_type": "code",
        "outputId": "38621ad2-8420-4ba0-ec21-191c9135241d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_classes = 5 \n",
        "max_words = 20\n",
        "sentences = ['আমি আর আসছি না এখানে!',\n",
        "              'কথা শোনে না ওয়েটার',\n",
        "            'একদম ঠান্ডা খাবার']\n",
        "labels = np.random.randint(0, num_classes, 3)\n",
        "y = to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "words = set(w for sent in sentences for w in sent.split())\n",
        "word_map = {w : i+1 for (i, w) in enumerate(words)}\n",
        "sent_ints = [[word_map[w] for w in sent.split()] for sent in sentences]\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI3TxDorl5Hw",
        "colab_type": "text"
      },
      "source": [
        "বাক্যের শব্দের সিকোয়েন্স, ৫, ৪, ৩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1u40lNjlztt",
        "colab_type": "code",
        "outputId": "e7054ada-4033-41ef-eb1a-e9afffbe892e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(sent_ints)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7, 3, 2, 6, 10], [5, 11, 6, 1], [8, 9, 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFHbTsYHiisK",
        "colab_type": "text"
      },
      "source": [
        "আমাদেরকে প্যাড করতে হবে max_words লেনথ ধরে, যা পাবো len(words) এবং ১ যোগ করে। +১ মানে হচ্ছে আমরা ০কে রিজার্ভ রাখবো যাতে সেটার বাইরে না যায়। এখানে ওয়ান হট এনকোডিং এবং প্যাডিং করছি -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx50nUsufCfN",
        "colab_type": "code",
        "outputId": "c8d491f3-5dfd-4300-bfd9-0cde992192dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X = np.array([to_categorical((pad_sequences((sent,), \n",
        "    max_words)).reshape(20,),vocab_size + 1) for sent in sent_ints])\n",
        "print(X.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 20, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi9UPNAlna11",
        "colab_type": "text"
      },
      "source": [
        "এখন দেখি আমাদের ওয়ান হট এনকোডিং এর অবস্থা?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNGqytPrnOtT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "992911e7-bad9-43d2-8e02-28babdb3518f"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltNmRAeWp-nf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cf95f340-382e-4d3f-fde9-f7a6629e63c4"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtHeqlNVj7Oy",
        "colab_type": "text"
      },
      "source": [
        "মডেলে ডেন্স লেয়ার যোগ করছি যাতে ওয়ান হট শব্দগুলোকে ডেন্স ভেক্টরে পাল্টে নেয়া যায়। তো LSTM এর কাজ কি? আমাদের বাক্যের ভেতরে শব্দের ভেক্টরকে কনভার্ট করতে হবে ডেন্স বাক্য ভেক্টরে। একদম শেষ লাইনে সফটম্যাক্স এক্টিভেশন ফাংশন ব্যবহার করছি যাতে ক্লাসের ওপর প্রবাবিলিটি ডিস্ট্রিবিউশন চালাতে পারে। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJgHhxWYfkFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words, vocab_size + 1)))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "          optimizer='adam',\n",
        "          metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwB1YH6If5dw",
        "colab_type": "code",
        "outputId": "25b601ce-1b1d-4269-d5e9-cffed89583dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "model.fit(X,y)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3 samples\n",
            "3/3 [==============================] - 4s 1s/sample - loss: 1.6413 - accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd07a5a5518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bXyPHd6Ntlo",
        "colab_type": "text"
      },
      "source": [
        "## শব্দ এবং সংখ্যার কাছাকাছি এনকোডিং\n",
        "\n",
        "শব্দ যখন সংখ্যা হবে, তখন শব্দের মধ্যে যেরকম সম্পর্ক ছিলো, সেটা আরো ভালো বোঝা যাবে সংখ্যায়। শব্দে \"আমি\" \"তুমি\" যেমন কাছাকাছি, সংখ্যায় সেটা আরো পরিস্কার হবে। এমবেডিং এ এটা এমন ধরনের ‘ডেন্স’ রিপ্রেজেন্টেশন যার মধ্যে একই ধরনের শব্দের একই বা কাছাকাছি ধরনের এনকোডিং হবে। ‘পুরুষ’ এবং ‘মহিলা’ এই দুটো অক্ষরের মধ্যে আকাশ পাতাল পার্থক্য হলেও সম্পর্কের কারণে এদুটো কাছাকাছি থাকবে। সংখ্যার এনকোডিং ও একই ধরনের হবে। সবচেয়ে বড় কথা হচ্ছে এ ধরনের এনকোডিং ম্যানুয়ালি বা আমাদের হাতে লিখে দিতে হবে না। বরং যেহেতু এগুলো ট্রেনিংযোগ্য প্যারামিটার ফলে মডেলের ট্রেনিং এর সময় ওয়েটগুলো ‘এডজাস্ট’ হবে এদের সম্পর্কের ভিত্তিতে। একটা ছোট ডাটাসেটের ওয়ার্ড এম্বেডিং ৮ ডাইমেনশনাল হলেও সেটা বড় ডাটাসেটের জন্য ১০২৪ ডাইমেনশন পর্যন্ত যেতে পারে। যত বেশি ডাইমেনশনাল এম্বেডিং ততবেশি শব্দগুলোর মধ্যে সম্পর্কগুলোকে আরো ভালোভাবে বোঝা যাবে তবে তার জন্য প্রচুর ডাটা লাগবে শিখতে।\n",
        "\n",
        "আমরা যখন ‘ওয়ান হট’ এনকোডিং উদাহরণ দেখছিলাম, সেখানে প্রতিটা শব্দ একটা ৪ ডাইমেনশনাল ফ্লোটিং পয়েন্ট সংখ্যা ভেক্টর দিয়ে রিপ্রেজেন্ট করা হয়েছিল। আমরা এ ধরনের এম্বেডিংকে বলতে পারি ‘লুকআপ’ টেবিল। এই ‘লুকআপ’ টেবিলের ওয়েটগুলো শিখছে যখন আমরা ‘ডেন্স’ ভেক্টরের তার করেসপন্ডিং টেবিল দেখছি। ফলে সেটা ধরে আমরা প্রতিটা শব্দ ধরে এনকোডিং করছি। তবে, এখানে একটা বড় সমস্যা আছে। আমরা যখন এনকোডিং করবো তখন সব বাংলা শব্দকে একসাথে এনকোডিং না করলে আমরা কিভাবে একটা শব্দকে আরেকটা শব্দের সাথে সিমিলারিটি, শব্দগুলোর মধ্যে দূরত্ব বের করবো? সে সমস্যা মেটাতে এসেছে প্রি-ট্রেইনড মডেল। কোটি কোটি শব্দকে একসাথে ট্রেইন করিয়ে তারপর এনকোডিং করেছে একসাথে। ফলে এধরনের প্রি-ট্রেইনড মডেলগুলো অসাধারণ পারদর্শী। শুধু নামিয়ে নিতে হবে দরকারের সময়ে।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPhkHnJ2Lm4o",
        "colab_type": "text"
      },
      "source": [
        "ধারণাগুলো এসেছে জেফ হিটনের নোটবুক এবং স্ট্যাকওভারফ্লো থেকে।\n",
        "\n",
        "https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_11_03_embedding.ipynb"
      ]
    }
  ]
}