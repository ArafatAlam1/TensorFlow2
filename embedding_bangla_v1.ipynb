{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "embedding_bangla_v1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raqueeb/TensorFlow2/blob/master/embedding_bangla_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi3C6LhrrjKx",
        "colab_type": "text"
      },
      "source": [
        "## এমবেডিং, ওয়ার্ড এমবেডিং এবং বাংলায় টেক্সট অ্যানালাইসিস\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4TNlkGyLz4C",
        "colab_type": "text"
      },
      "source": [
        "একটা জিনিস ভেবে দেখেছেন কি? আমরা এ পর্যন্ত মেশিন লার্নিং মডেলের ইনপুট হিসেবে যা দিয়েছি তা সবই সংখ্যা। মনে করে দেখুন, এপর্যন্ত সব মডেলের ক্লাসিফিকেশন অথবা রিগ্রেশন এর জন্য যা দিয়েছি সব সংখ্যায় দিয়েছি। তার পাশাপাশি ইমেজ নিয়ে যখন কাজ করেছি তখনো কিন্তু ইমেজ (সেটা গ্রেস্কেল হোক আর কালার হোক - তার জন্য গ্রেস্কেল ইনটেনসিটি অথবা কালারের আরজিবি চ্যানেলের আউটপুট), সবকিছুই সংখ্যায় গিয়েছে। এর অর্থ হচ্ছে মেশিন লার্নিং/ডিপ লার্নিং মডেল সংখ্যা ছাড়া আর কিছু বোঝেনা। আর বুঝবেই বা কিভাবে? সেতো যন্ত্র। আর মানুষের সবকিছুই কমপ্লেক্স। \n",
        "\n",
        "সেদিক থেকে মানুষের ভাষা আরো অনেক কমপ্লেক্স। আমরা একেকজন একেক ভাষায় কথা বলি, ভাষাগুলোর মধ্যে সংযোগ/সিমিলারিটি এবং কি বলতে গিয়ে কি বলে ফেললাম এবং তার ফলাফল, তার পাশাপাশি অনেক শব্দ একটা ভাষায় যা বোঝায় সেটা অন্য ভাষায় তার বৈপরীত্য দেখায়। এখন আপনি বাংলায় কথা বললেও সেটার মধ্যে ৪০% বাইরের শব্দ ব্যবহার করলে তো আরো সমস্যা। আর এই কারণে টেক্সট নিয়ে কাজ করা বেশ কমপ্লেক্স। \n",
        "\n",
        "যেকোনো ল্যাঙ্গুয়েজে তার প্রতিটা শব্দের একটা অর্থ আছে। তবে এটার অর্থ অনেক সময় নির্ভর করে কনটেক্সটে বা শব্দটা বাক্যের মধ্যে কোথায় এই মুহূর্তে আছে। একই শব্দের আবার অনেকগুলো কনটেক্সচুয়াল অর্থ থাকে সে কারণে শব্দকে শুধুমাত্র শব্দ বা অক্ষর লেভেলে কাজ করলে হবে না। কারণ, ডিপ লার্নিং মডেল যেহেতু সংখ্যা ছাড়া কিছু বোঝেনা, সে কারণে একেক ভাষার একেক বুলি এবং বকাবকি এর পাশাপাশি সেই ভাষাগুলোকে ঠিকমতো সংখ্যায় ট্রান্সফার করা একটা চ্যালেঞ্জ এর কাজ অবশ্যই।\n",
        "\n",
        "## কেন টাইম সিরিজ নিয়ে আলাপ হয়নি?\n",
        "\n",
        "এই বইতে আমি ইচ্ছে করে ‘টাইম সিরিজ’ যোগ করিনি, কারণ সেটার এপ্লিকেশন লেভেল এখনো বেসিক লেভেলে নেই। তবে, এই রিকারেন্ট নিউরাল নেটওয়ার্ককে শিখিয়ে দিলে সে (আরএনএন) ফ্রি ফর্ম (ইচ্ছেমতো) টেক্সট জেনারেট করতে পারে। আমরা যেমন দেখেছি ‘এল এস টি এম’, (লঙ শর্ট টার্ম মেমোরি) নেটওয়ার্কে ‘শেক্সপিয়ার’ ক্লাসিক পড়তে দিলে, সে শেক্সপিয়ারের মতো আরেকটা ক্লাসিক লিখে ফেলেছে, যেখানে এই নেটওয়ার্কের মধ্যে শব্দ, বাক্য এবং ব্যাকরণ তৈরির কোন ধারণা নেই। কারণ, ডিপ লার্নিং প্রচুর ক্লাসিক বই পড়ে বুঝেছে কিভাবে শব্দ, বাক্য বা তার ব্যাকরণ ব্যবহার করতে হয় এর ভেতরে না ঢুকেই। টাইম সিরিজের ব্যাপারটা হচ্ছে সে পরের জিনিসটা প্রেডিক্ট করবে। আগের সময়ে কি ছিলো, সেটাকে ধরে এরপরে কি কি আসতে পারে সেটাই বলবে সে। না বুঝে। যদি শব্দ লেভেলে দেখি, ‘আমি’ এর পর কি আসতে পারে তাহলে ‘ভালো’ আসতে পারে, কারণ ‘আমি ভালো আছি’ একটা বহুল প্রচলিত বাক্য। \n",
        "\n",
        "বড় ব্যাপার হচ্ছে এই অক্ষর থেকেই ‘এল এস টি এম’ আস্তে আস্তে মানবিক ব্যাকরন এবং কিভাবে একটা বাক্য তৈরি করতে হয় সে ধরনের একটা ধারণা পেয়ে থাকে। এটা সে কোন কিছু বুঝে করে না। প্যাটার্ন থেকে করে। আর সে কারণেই এটার উপরে আমরা খুব একটা ভরসা করব না। আমরা চাইব মেশিনকে শেখাতে, যেভাবে মানুষ ভাষা, শব্দ, ব্যাকরণ শেখে। ওই একই কারণে অক্ষর লেভেলে টেক্সট জেনারেশন নিয়ে আমরা এই মুহূর্তে আলাপ করব না। ইন্টারনেটে দেখতে পারেন কিভাবে একেকটা ‘এল এস টি এম’ নেটওয়ার্ক শেক্সপিয়ারের মত বড় বড় নাটক লিখে ফেলছে। এটা আসলে সে একটা ক্লাসিক্যাল লেখার প্যাটার্ন দেখে তার পার্সপেক্টিভ থেকে লিখেছে। বুঝে লেখেনি। \n",
        "\n",
        "## ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং\n",
        "\n",
        "সেজন্য এর মধ্যে এসে যোগ হয়েছে ‘ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং’। ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং এর মধ্যে লিঙ্গুইস্টিকস, কম্পিউটার সায়েন্স, ইনফরমেশন ইঞ্জিনিয়ারিং এবং কৃত্রিম বুদ্ধিমত্তা ব্যাপারগুলো চলে এসেছে কাজের স্বার্থে। এমনিতেই মানুষ এবং যন্ত্রের মধ্যে একটা যোগসূত্র স্থাপন করা বেশ ঝামেলার ব্যাপার। ‘স্পিচ রিকগনিশন’ এর পাশাপাশি স্বয়ংক্রিয়ভাবে একটা ভাষা বুঝতে পারা এবং তার পাশাপাশি সেই ভাষায় বুঝে টেক্সট জেনারেট করা সহজ ব্যাপার নয়, যখন সবকিছুর পেছনে কাজ করে সংখ্যা। সবচেয়ে বড় কথা হচ্ছে আপনি একটা যন্ত্রকে শেখাচ্ছেন সে আপনার সাথে ন্যাচারালি যোগসুত্র স্থাপন করতে পারে। এই ‘ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং’ এর শুরুটা হচ্ছে সেই টেক্সটকে ঠিকমতো পড়তে পারা, সেটাকে নিজের মতো করে বোঝা এবং মানুষ যেভাবে একটা বাক্যের ‘ইনটেন্ট’ বুঝতে পারে সেভাবে তাকে বুঝিয়ে তার কাছ থেকে উত্তর বের করা। সহজ ব্যাপার নয়। তবে শুরু করতে হবে কোথাও। \n",
        "\n",
        "যেহেতু ‘ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং’ একটা বিশাল সাবজেক্ট, আমি এই বইতে ব্যাপারটা আনার ব্যর্থ চেষ্টা করব না। তবে কম্পিউটার কিভাবে ‘টেক্সট’ এর সাথে ‘ইন্টারঅ্যাক্ট’ করে সেটা নিয়েই দেখব আমরা সামনের চ্যাপ্টারগুলোতে। আমরা জানি মেশিন লার্নিং মডেল ভেক্টরকে ইনপুট হিসেবে নেয়। আর এই ভেক্টরগুলো হচ্ছে সংখ্যার অ্যারে। যখন আমরা টেক্সট মানে শব্দ এবং বাক্য নিয়ে আলাপ করব, তখন আমাদের প্রথম কাজ হবে এই স্ট্রিংগুলোকে কিভাবে সংখ্যায় পাল্টানো যায়। অর্থাৎ সেই বাক্য বা শব্দটি কিভাবে ‘ভেক্টরাইজ’ করা যায় মডেলে দেবার আগে। আমাদের এই চ্যাপ্টার থেকে শুরু করব কিভাবে আস্তে আস্তে ডিপ নিউরাল নেটওয়ার্ক দিয়ে বাংলার একটা অ্যাপ্লিকেশনে যাওয়া যায়।\n",
        "\n",
        "## ওয়ার্ড ভেক্টর কি? \n",
        "\n",
        "কম্পিউটার সাইন্সে ভেক্টর মানে হচ্ছে সংখ্যা দিয়ে একটা সারি। এমনটা দেখতে। <img src=\"https://raw.githubusercontent.com/raqueeb/deep_learning_book/master/assets/vector32.png\"> চিত্রঃ একটা ভেক্টর, ইংরেজি সংখ্যায় লেখা\n",
        "\n",
        "প্রোগ্রামিংয়ে ভেক্টরের ইনডেক্স পজিশন শুরু হয় ০ দিয়ে। আমাদের ভেক্টরে পজিশন ০ এর ভ্যালু হচ্ছে ২৩, পজিশন ১ এর ভ্যালু ১৮, এভাবেই চলতে থাকে। \n",
        "\n",
        "ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং এ ওয়ার্ল্ড ভেক্টর খুবই জনপ্রিয় কারণ এই ব্যাপারটা মডেলকে শেখায় কোথায় কোন শব্দটা পাওয়া যেতে পারে ‘কন্টেক্সট’ এর ওপর ভিত্তি করে। এনএলপি মডেলে কন্টেক্সট যোগ করলে এর অ্যাকুরেসি বেড়ে যায় অনেকখানি। একটা ওয়ার্ড ভেক্টরের ভ্যালুগুলো হচ্ছে তার পজিশন, যা সাধারণত আমরা ৩০০ ডাইমেনশন স্পেসে কাজ করি। যেহেতু ডাইমেনশন বড় সেকারণে ওয়ার্ড ভেক্টরগুলোর মধ্যে অংকের সাহায্যে এর কাছাকাছি সিমান্টিক্যালি ম্যাথমেটিক্যাল অপারেশন করা যেতে পারে। বলেন কি? শব্দ দিয়ে যোগ বিয়োগ করা? সেটাও সম্ভব। দেখবো সামনে।\n",
        "\n",
        "আমরা যেহেতু মেশিন লার্নিং মডেলে বিভিন্ন ক্লাস অথবা কন্টিনিউয়াস ভ্যালু প্রেডিক্ট করতে পারি, সেখানে আমরা সিমান্টিক্যালি কোরিলেটেড কন্টেক্সট ডাটাসেট থেকে নিতে পারব না কেন?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaDQqrJg7p68",
        "colab_type": "code",
        "outputId": "13a064eb-c89e-4644-a5d4-9789c1f0511d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  # শুধুমাত্র টেন্সর-ফ্লো ২.x ব্যবহার করবো \n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "keras = tf.keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0un3M2_MnHe",
        "colab_type": "text"
      },
      "source": [
        "আমরা যেহেতু ডিপ নিউরাল নেটওয়ার্কের কথা বলছি সেখানে সবকিছুতেই লেয়ার কনসেপ্ট কাজ করে। আজকে এখানে আমরা নতুন একটা লেয়ার নিয়ে কথা বলবো যেটাকে আমরা বলছি এম্বেডিং লেয়ার। শুরুতেই অনেকে বলবেন এম্বেডিং মানে কি? এর সাথে সংখ্যার সম্পর্ক কি?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4bim_ShMw8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "embedding_layer = layers.Embedding(1000, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRmBcA6iMx71",
        "colab_type": "code",
        "outputId": "f9705e19-3b1e-4671-95d0-8d00fbb22324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# তিনটা সংখ্যা পাঠালাম, দেখি কি করে?\n",
        "# সংখ্যায় ওয়েট \n",
        "\n",
        "result = embedding_layer(tf.constant([1,2,3]))\n",
        "result.numpy()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01450314, -0.02634749, -0.00589106, -0.04408872, -0.00525304],\n",
              "       [ 0.01016078, -0.0135615 ,  0.0368791 ,  0.00061329, -0.00987286],\n",
              "       [ 0.04134339,  0.02245886, -0.0061496 ,  0.00041949, -0.02733678]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYLOnO5Uxmmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# অথবা টেন্সর-ফ্লো দিয়ে টোকেনাইজ করে দেখি\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mezzU-TxprR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "text = ['আমি এখন বই পড়ি', 'এটি আমার অনেক পছন্দের একটা বই']\n",
        "tokenizer.fit_on_texts(text)\n",
        "sequences = tokenizer.texts_to_sequences(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVVdIpcqxrxB",
        "colab_type": "code",
        "outputId": "626f1224-b784-4e1c-ff5a-a944ac2a5a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sequences"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 3, 1, 4], [5, 6, 7, 8, 9, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZxexs73hmRo",
        "colab_type": "text"
      },
      "source": [
        "ইউনিকোড স্ট্রিং দুভাবে টেন্সর-ফ্লোতে দেখানো যায়। স্ট্রিং স্কেলারে কোড পয়েন্টের সিকোয়েন্সকে এনকোড করবে জানা ক্যারেক্টার এনকোডিং দিয়ে। আমরা ভেক্টর নিয়ে দেখালে। এখানে প্রতিটা পজিশনে একটা সিঙ্গেল কোড পয়েন্ট আছে।\n",
        "\n",
        "এখানে কয়েকটা উদাহরণ দেখালাম, তবে মাথা খারাপ করার দরকার নেই। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXxHWH-We9Vg",
        "colab_type": "code",
        "outputId": "ff33eb15-9f6b-46cf-db1e-8fe45e7d7076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "text_chars = tf.constant([ord(char) for char in u\"আমি এখন বই পড়ি\"])\n",
        "text_chars"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=16, shape=(15,), dtype=int32, numpy=\n",
              "array([2438, 2478, 2495,   32, 2447, 2454, 2472,   32, 2476, 2439,   32,\n",
              "       2474, 2465, 2492, 2495], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dxKTGEZkRwZ",
        "colab_type": "text"
      },
      "source": [
        "যা দেখছেন, ইউনিকোড স্ট্রিং, দেখাচ্ছে ইউনিকোড কোড পয়েন্ট ভেক্টরে। ব্যাচে দেখি। সব সংখ্যার খেলা।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AutcJ9tfe9Sh",
        "colab_type": "code",
        "outputId": "9410ab79-d851-45fc-cb69-1ae2b86fd6b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "batch_utf8 = [s.encode('UTF-8') for s in\n",
        "              [u'প্রিয় বন্ধু',  u'কেমন চলছে তোমার?',  u'আমি ভালো', u'😊']]\n",
        "batch_chars_ragged = tf.strings.unicode_decode(batch_utf8,\n",
        "                                               input_encoding='UTF-8')\n",
        "for sentence_chars in batch_chars_ragged.to_list():\n",
        "  print(sentence_chars)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2474, 2509, 2480, 2495, 2527, 32, 2476, 2472, 2509, 2471, 2497]\n",
            "[2453, 2503, 2478, 2472, 32, 2458, 2482, 2459, 2503, 32, 2468, 2507, 2478, 2494, 2480, 63]\n",
            "[2438, 2478, 2495, 32, 2477, 2494, 2482, 2507]\n",
            "[128522]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89HMTwJagkYu",
        "colab_type": "code",
        "outputId": "52682e13-9c10-40a6-de78-9c71d1c4e027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "batch_chars_padded = batch_chars_ragged.to_tensor(default_value=-1)\n",
        "print(batch_chars_padded.numpy())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  2474   2509   2480   2495   2527     32   2476   2472   2509   2471\n",
            "    2497     -1     -1     -1     -1     -1]\n",
            " [  2453   2503   2478   2472     32   2458   2482   2459   2503     32\n",
            "    2468   2507   2478   2494   2480     63]\n",
            " [  2438   2478   2495     32   2477   2494   2482   2507     -1     -1\n",
            "      -1     -1     -1     -1     -1     -1]\n",
            " [128522     -1     -1     -1     -1     -1     -1     -1     -1     -1\n",
            "      -1     -1     -1     -1     -1     -1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuzAjhepliiv",
        "colab_type": "text"
      },
      "source": [
        "মাল্টি ডাইমেনশনে দেখি।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY5U_UCMg8MS",
        "colab_type": "code",
        "outputId": "68ecf30e-2b61-484f-e471-9150e04f41c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tf.strings.unicode_script(batch_chars_ragged))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.RaggedTensor [[4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4], [4, 4, 4, 4, 0, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 0], [4, 4, 4, 0, 4, 4, 4, 4], [0]]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHqel-xSNGyu",
        "colab_type": "text"
      },
      "source": [
        "## ‘এম্বেডিং’ সিমিলারিটি, একটা থেকে আরেকটা শব্দের দূরত্ব\n",
        "\n",
        "মেশিন লার্নিং এর ভাষায় ‘এম্বেডিং’ হচ্ছে সিমিলারিটি, যদি শব্দের কথা বলি তাহলে একটা শব্দ থেকে আরেকটা শব্দ কতটুকু যুক্ত বা একটা শব্দ থেকে আরেকটা শব্দ কত দূরে? তাদের মধ্যে সম্পর্ক আছে কিনা? যেমন, রাজা’র সাথে ‘রানী’ শব্দটা কিন্তু সম্পর্কযুক্ত। যেমন, ‘মা’ শব্দের সাথে ‘বাবা’ সম্পর্কযুক্ত। ‘বাংলাদেশ’ শব্দের সাথে ‘ঢাকা’ সম্পর্কযুক্ত। সেই থেকে ‘আকাশ’ শব্দের সাথে ‘টেবিল’ কিন্তু বহু দূরে মানে তাদের মধ্যে সম্পর্ক টানা বেশ কঠিন, যদি না কোনদিন ‘আকাশ’ থেকে ‘টেবিল’ পড়ে।\n",
        "\n",
        "এই এম্বেডিং ব্যাপারটা এসেছে কিছুটা ‘ওয়ান হট’ এনকোডিং থেকে। তবে, ব্যাপারটা ঠিক সেরকম নয়। শব্দকে যেহেতু আমাদেরকে সংখ্যায় পাল্টাতে হবে তাহলে আর বাকি উপায় কি? মনে আছে, আমাদের এই ‘ওয়ান হট’ এনকোডিং বা ‘ডামি ভেরিয়েবল’ ব্যাপারটা এসেছে শব্দ দিয়ে তৈরি ক্যাটাগরিকাল ভেরিয়েবল থেকে? মেশিন লার্নিং মডেলে আমরা যখন কিছু শব্দকে সংখ্যার ক্যাটাগরিতে ভাগ করতে চাই, যেটা এমুহুর্তে সংখ্যায় নেই। যেমন, আইরিশ ডাটাসেটের তিন প্রজাতির ফুলের জন্য ০,১,২, ভাগে ভাগ করতে পারছিনা, সেখানে চলে এসছে এই ডামি ভেরিয়েবল। আমরা যাকে বলছি শব্দকে দিয়ে তার জন্য একটা করে এনকোডিং। এর অর্থ হচ্ছে, আমাদের ভাষায় যতগুলো শব্দ আছে সেগুলোকে ‘ওয়ান হট’ এনকোডিং করা যেতে পারে। আর সেখানেই সমস্যা। \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/raqueeb/deep_learning_book/master/assets/hot.png\"> চিত্রঃ ‘ওয়ান হট’ এনকোডিং এর উদাহরণ\n",
        "\n",
        "এখানে একটা উদাহরণ দেয়া যাক। একটা বাক্য। ‘আমি এখন বই পড়ি’। এই চারটা শব্দকে যদি আমরা সংখ্যায় পাল্টাতে চাই, তাহলে আমাদের এই ভোকাবুলারির প্রতিটা শব্দকে ‘০’ ভেক্টর দিয়ে শুরু করব। আমাদের এখানে যেহেতু ৪টা ইউনিক শব্দ, সে কারণে এই শূন্য ভেক্টরের দৈর্ঘ্য হবে ৪। প্রতিটি শব্দকে ঠিকমতো রিপ্রেজেন্ট করার জন্য একেকটা শব্দের ইনডেক্সে তার করেসপন্ডিং ‘১’ বসাবো। ছবি দেখি। এখন এই টেবিল থেকে প্রতিটা শব্দের জন্য তার ভেক্টর বের করা সোজা। আমাদের এই চারটা শব্দের জন্য ভেক্টরের দৈর্ঘ্য হচ্ছে ৪ যার বাকি তিনটাই ০। এভাবে আমরা শব্দকে বিভিন্ন ক্যাটাগরিতে ভাগ করতে পারি। তবে ব্যাপারটা সেরকম এফিশিয়েন্ট নয় যখন আমাদের ভোকাবুলারিতে ১০ হাজার শব্দ থাকবে। তার মানে একেকটা ভেক্টরের দৈর্ঘ্য ১০ হাজার হবে - এর মধ্যে ৯৯.৯৯ শতাংশই হচ্ছে ০। এটা দক্ষ সিস্টেম না। আমরা যদি প্রতিটা বাংলা শব্দের জন্য ডামি ভ্যারিয়েবল বানাই, তাহলে কতো বড় ডামি ভ্যারিয়েবলের একেকটা টেবিল হবে?\n",
        "\n",
        "এই সমস্যা থেকে বের হবার উপায় কি? প্রতিটা শব্দকে একটা করে ইউনিক সংখ্যা দিয়ে দেওয়া। আমাদের আগের ভোকাবুলারিটাকে ‘আমি এখন বই পড়ি’কে আমরা একটা ‘ডেন্স’ ভেক্টরের মত আলাদা আলাদা করে সংখ্যা দিয়ে দিতে পারি। এই জিনিসটা আগের থেকে অনেকটাই এফিশিয়েন্ট। একেকটা শব্দের জন্য একেকটা আলাদা আলাদা সংখ্যা। সবচেয়ে বড় ব্যাপার হচ্ছে বিশাল ‘স্পার্স’ হাই-ডাইমেনশন স্পেস থেকে কম ডাইমেনশন স্পেসে চলে এলাম। তবে, এটার সমস্যা দুটো।\n",
        "\n",
        "* ১. আমাদের এই সংখ্যায় এনকোডিং সিস্টেমটাকে ‘ম্যানুয়ালি’ করা হয়েছে। ফলে এদের মধ্যে কোন ‘রিলেশনশিপ’ বের করা যাচ্ছে না। অংকেও এক সংখ্যা থেকে আরেক সংখ্যার মধ্যে যে রিলেশনশিপ সেটা অনুপস্থিত। ফলে আমাদের অংকের ভাষায় কো-সাইন ভেক্টরে কে কোথায় আছে সেটা বের করা মুশকিল।\n",
        "\n",
        "* ২. একটা মডেলের জন্য এই ধরনের ‘ম্যানুয়াল’ এনকোডিং শব্দগুলোর মধ্যে সম্পর্ক না বুঝলে সেটা সমস্যা হয়ে দাঁড়াবে। যেহেতু এই একেকটা ফিচারের জন্য একেকটা ‘ওয়েট’ সে কারণে একটা লিনিয়ার ক্লাসিফায়ারের পক্ষে এই ফিচার এবং ওয়েট এর কম্বিনেশন কোন সম্পর্ক দেখাবেনা। এটা বড় সমস্যা। \n",
        "\n",
        "আর সে কারণেই চলে এসেছে ওয়ার্ড এম্বেডিং।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awPlT5SRBH2G",
        "colab_type": "text"
      },
      "source": [
        "## একটা ছোট্ট উদাহরন, রেস্টুরেন্ট রিভিউ\n",
        "\n",
        "আমরা এখানে একটা এমবেডিং লেয়ারের উদাহরণ নিয়ে আসি। ইনপুট ডাইমেনশনে কতগুলো ভোকাবুলারি আছে? ধরে নিচ্ছি ৫০টা। আমি সংখ্যা না গুনেই বলছি। ৫০টা ডামি ভ্যারিয়েবল। মানে আমরা কতগুলো ক্যাটেগরিতে এনকোড করছি। আমাদের লুক আপ টেবিলে কতগুলো আইটেম আছে সেটাই এখানে আসবে। \n",
        "\n",
        "আমাদের ৫০টা ভোকাবুলারির জন্য 'ওয়ান হট' এনকোডিং ব্যবহার করছি, কেরাসের ভেতরে। এটা ব্যবহার করা ঠিক না তবে ছোটখাট উদাহরণে সেটা করা যায়। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEOvo2pjrjKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Embedding, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS7ZaxjhMJ0w",
        "colab_type": "text"
      },
      "source": [
        "আমরা এখানে ছোট একটা দশটা রেস্টুরেন্টের রিভিউ যোগ করেছি। এর পাশাপাশি তার পাঁচটা নেগেটিভ আর পাঁচটা পজেটিভ লেবেল যোগ করেছি। এর উপর আমরা নিউরাল নেটওয়ার্কে ট্রেইন করব। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ecoq69LrjK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ১০টা েস্টুরেন্ট রিভিউএর ইনপুট\n",
        "reviews = [\n",
        "    'আমি আর আসছি না এখানে!',\n",
        "    'একদম বাজে সার্ভিস',\n",
        "    'কথা শোনে না ওয়েটার',\n",
        "    'একদম ঠান্ডা খাবার',\n",
        "    'বাজে খাবার!',\n",
        "    'অসাধারণ',\n",
        "    'অসাধারণ সার্ভিস!',\n",
        "    'খুব ভালো!',\n",
        "    'মোটামুটি',\n",
        "    'এর থেকে ভালো হয়না']\n",
        "\n",
        "# লেবেল বলে দিচ্ছি (1=নেগেটিভ, 0=পজেটিভ)\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CxmAlaErjK3",
        "colab_type": "code",
        "outputId": "d5a4adf4-2198-49ed-b8eb-7d97352e3477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(reviews[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "আমি আর আসছি না এখানে!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59EA_3KSKOos",
        "colab_type": "text"
      },
      "source": [
        "শব্দের সংখ্যা না গুনেও একটা ধারনা নিয়েছি ৫০টা মানে ভোকাবুলারি সাইজ ৫০। ছোট্ট উদাহরণ বলে আমরা ‘ওয়ান হট এনকোডিং’ ব্যবহার করছি। এর কাজ হচ্ছে এই বাক্যগুলোকে শব্দে ভেঙে ফেলা। যাকে আমরা বলছি টোকেনাইজিং। এরপর তাকে নিজস্ব ইনডেক্সে পাঠাবে। শেষে আমরা যা পাব সেটা সংখ্যার সিকোয়েন্স। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZk5g6PrjK5",
        "colab_type": "code",
        "outputId": "b1236422-a7a6-4166-b078-4288d47297ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ওয়ান হট এনকোডিং\n",
        "\n",
        "VOCAB_SIZE = 50\n",
        "encoded_reviews = [one_hot(d, VOCAB_SIZE) for d in reviews]\n",
        "print(f\"Encoded reviews: {encoded_reviews}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded reviews: [[21, 16, 22, 38, 17], [17, 25, 8], [47, 38, 38, 21], [17, 41, 42], [25, 42], [48], [48, 8], [10, 1], [19], [26, 16, 1, 19]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok0ds1g-LOOl",
        "colab_type": "text"
      },
      "source": [
        "যতগুলো শব্দ ততগুলো করে সংখ্যা একেকটা অংশে। তবে এই সিকোয়েন্সে একটা কনসিস্টেন্সি রাখার জন্য একটা লেনথ ঠিক করে দিতে হবে। শব্দ ধরে এখন কোনটা ২ অথবা ৩ এবং ৪ লেনথে, কিন্তু প্যাডিং এর জন্য আমরা MAX_LENGTH = 4 ধরে দিচ্ছি। দেখুন এখানে বাকি অংশ ০ দিয়ে ভরে দিয়েছে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFLBnh2erjK8",
        "colab_type": "code",
        "outputId": "2bbaf82a-bf12-48db-e1b9-b4c10f3fbf12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "MAX_LENGTH = 4\n",
        "\n",
        "padded_reviews = pad_sequences(encoded_reviews, maxlen=MAX_LENGTH, padding='post')\n",
        "print(padded_reviews)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[16 22 38 17]\n",
            " [17 25  8  0]\n",
            " [47 38 38 21]\n",
            " [17 41 42  0]\n",
            " [25 42  0  0]\n",
            " [48  0  0  0]\n",
            " [48  8  0  0]\n",
            " [10  1  0  0]\n",
            " [19  0  0  0]\n",
            " [26 16  1 19]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-10aiXydLsei",
        "colab_type": "text"
      },
      "source": [
        "আমরা আগের মতো সিকোয়েন্সিয়াল নেটওয়ার্ক তৈরি করছি। আউটপুট ডাইমেনশনে কমিয়ে এনেছি সেটা। এখানে একটা ডেন্স লেয়ার, তার মানে একটা ওয়েট ম্যাট্রিক্স, একটা লার্নিং চলছে এখানে। এই এমবেডিং লেয়ারে যে লার্নিং চলছে সেটা শব্দগুলোকে একটা ইউক্লুডিয়ান স্পেসে সংখ্যাগুলোর মধ্যে একটা রিলেশনশিপ তৈরি হচ্ছে।\n",
        "\n",
        "মডেলটা দেখুন।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBM4Eu7OrjLJ",
        "colab_type": "code",
        "outputId": "7fbcd3f1-2741-4830-9054-34bed275f5e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model = Sequential()\n",
        "embedding_layer = Embedding(VOCAB_SIZE, 8, input_length=MAX_LENGTH)\n",
        "model.add(embedding_layer)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 4, 8)              400       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVDOD7q7rjLO",
        "colab_type": "code",
        "outputId": "b4fe0f82-e993-4eb2-ed2f-14ff08284f61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# মডেলকে ট্রেইন করি\n",
        "model.fit(padded_reviews, labels, epochs=100, verbose=0)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f31903cd320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQpHhhrbOhdM",
        "colab_type": "text"
      },
      "source": [
        "১০০ ইপকের পর ভালো করে লক্ষ্য করুন, প্রতিটা লাইন হচ্ছে একেকটা শব্দের এমবেডিং। এখানে ওয়েটগুলোর অর্থ এখন খুঁজতে যাবো না এমুহুর্তে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnaII1xnrjLS",
        "colab_type": "code",
        "outputId": "b5ad3ade-7909-4835-f0db-da60954d1f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(embedding_layer.get_weights()[0].shape)\n",
        "print(embedding_layer.get_weights())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 8)\n",
            "[array([[ 0.1256781 , -0.0849715 ,  0.09698268, -0.10468217,  0.1283342 ,\n",
            "         0.05046589,  0.06748302,  0.04732048],\n",
            "       [ 0.0777471 , -0.09611855,  0.1414911 , -0.07192353,  0.12908256,\n",
            "         0.08346221,  0.09116188,  0.13654621],\n",
            "       [ 0.04645501, -0.03965471,  0.0398725 ,  0.00875189,  0.00408797,\n",
            "        -0.02412827, -0.00104625,  0.02204032],\n",
            "       [ 0.01540227, -0.01657827, -0.01394738,  0.023421  ,  0.03249354,\n",
            "         0.02688712, -0.04359137,  0.04119459],\n",
            "       [-0.03352839, -0.03194626, -0.01282536, -0.03692038,  0.0276118 ,\n",
            "        -0.02005726, -0.01211269, -0.02327393],\n",
            "       [ 0.04535179, -0.02548935, -0.01710705,  0.01823108, -0.04210467,\n",
            "        -0.03587403, -0.04162676,  0.02703081],\n",
            "       [ 0.0385324 , -0.02867969,  0.0252213 , -0.0114246 , -0.00026976,\n",
            "         0.04387388,  0.03827869, -0.02247369],\n",
            "       [ 0.02465682, -0.02595166, -0.03388695, -0.04788283,  0.01279212,\n",
            "        -0.03813229, -0.04407258,  0.01925263],\n",
            "       [-0.13070442, -0.10144027, -0.1257966 , -0.06207332, -0.14746991,\n",
            "         0.05077548,  0.11499187,  0.09735893],\n",
            "       [-0.04996618, -0.01211064, -0.03198613,  0.03782478,  0.04618618,\n",
            "        -0.04958791, -0.0385098 , -0.04056631],\n",
            "       [ 0.0930217 ,  0.09115674,  0.10773411, -0.12634802,  0.08623677,\n",
            "         0.12838005, -0.11430246, -0.13461252],\n",
            "       [ 0.01562513, -0.03883123, -0.01115724,  0.00923699,  0.0074966 ,\n",
            "         0.0405896 , -0.02943468,  0.01593821],\n",
            "       [-0.03088135, -0.01391936,  0.04745976, -0.04997455, -0.02028775,\n",
            "         0.02040315, -0.04713631,  0.01502723],\n",
            "       [-0.01024216,  0.01534933,  0.0172034 ,  0.0346305 ,  0.01661612,\n",
            "         0.04841195,  0.01379189,  0.03757607],\n",
            "       [ 0.04653769,  0.04436541,  0.04524889,  0.03553441,  0.0062074 ,\n",
            "        -0.03131308,  0.00037127,  0.04879229],\n",
            "       [-0.03291177, -0.00315449,  0.04179089,  0.03352192, -0.02094762,\n",
            "        -0.02856803,  0.02305673,  0.02782942],\n",
            "       [ 0.01308671, -0.1176981 , -0.05622469,  0.05296849, -0.02854801,\n",
            "         0.06461843,  0.05114404,  0.14462344],\n",
            "       [-0.06262195, -0.12102062, -0.14889747,  0.13340196, -0.07747538,\n",
            "        -0.11878533,  0.06842703,  0.14153677],\n",
            "       [ 0.04272592,  0.03073639, -0.03444406,  0.0238331 ,  0.04077807,\n",
            "        -0.01831667, -0.00600932,  0.01012177],\n",
            "       [ 0.14376695,  0.10096759,  0.05341239, -0.07163797,  0.11046767,\n",
            "         0.07250842, -0.04525359, -0.09716886],\n",
            "       [-0.04027854, -0.01828641,  0.00421674,  0.03602836,  0.04807926,\n",
            "        -0.01249757,  0.03096839, -0.0442752 ],\n",
            "       [-0.07426608, -0.10936529, -0.1454159 ,  0.07065725,  0.11625965,\n",
            "        -0.06824956, -0.1233145 , -0.09864845],\n",
            "       [-0.1434994 ,  0.08419741,  0.07649456,  0.06988203, -0.09125084,\n",
            "        -0.11601051, -0.10019267, -0.1358407 ],\n",
            "       [ 0.00582685,  0.0262349 , -0.02862767, -0.04320464, -0.0101349 ,\n",
            "         0.0125849 , -0.04508751,  0.03280069],\n",
            "       [-0.02614131,  0.03638059, -0.04999308, -0.02935035, -0.00412785,\n",
            "        -0.02878333, -0.04184658,  0.02182417],\n",
            "       [-0.13053486, -0.10881192, -0.10122065,  0.14463934, -0.09466527,\n",
            "        -0.11847976,  0.10210238, -0.01901151],\n",
            "       [ 0.05290353,  0.0612822 ,  0.07387015, -0.10034717,  0.12755476,\n",
            "         0.10349728, -0.10836726, -0.05969263],\n",
            "       [-0.00134736,  0.00059601,  0.00529457,  0.02532728, -0.04283221,\n",
            "        -0.02442389,  0.0368304 , -0.0482261 ],\n",
            "       [-0.02736621, -0.02313894,  0.01988662, -0.04181071, -0.0082929 ,\n",
            "        -0.0310246 , -0.02255946, -0.00582033],\n",
            "       [-0.03038561,  0.04314579,  0.04105146,  0.04977825, -0.03183848,\n",
            "        -0.03974159,  0.00864762,  0.02130752],\n",
            "       [ 0.01872437,  0.00804397,  0.01346437,  0.04577835, -0.03135689,\n",
            "         0.01585818,  0.00848254,  0.03880611],\n",
            "       [ 0.03982136, -0.04356441,  0.00262879, -0.01378735,  0.03959439,\n",
            "        -0.00356619, -0.03842546, -0.04671299],\n",
            "       [-0.00818603, -0.00561078, -0.01334932, -0.02566773, -0.02952744,\n",
            "        -0.02626315, -0.03437423,  0.04811479],\n",
            "       [ 0.04668221,  0.02626361, -0.0080976 , -0.04707902,  0.04685635,\n",
            "         0.0016432 ,  0.00720495,  0.01760994],\n",
            "       [ 0.03989574,  0.01630758, -0.02270503, -0.02250581,  0.04890212,\n",
            "        -0.02992758, -0.04737411,  0.03796655],\n",
            "       [-0.03355211, -0.01759509,  0.00975188, -0.04073963, -0.03984587,\n",
            "         0.04472116,  0.02391939,  0.04951629],\n",
            "       [ 0.01714549, -0.01852693, -0.04229248, -0.01379827,  0.02387346,\n",
            "        -0.01497694,  0.03733141,  0.0301755 ],\n",
            "       [-0.02336334, -0.00121554,  0.0391754 , -0.00661545, -0.00381979,\n",
            "         0.02669719,  0.01802782, -0.04511888],\n",
            "       [-0.13060652,  0.0994371 , -0.15213782,  0.12252556, -0.1279295 ,\n",
            "        -0.08872686, -0.10766762, -0.09045568],\n",
            "       [ 0.00235343,  0.01661121,  0.04978878,  0.04474569, -0.03385625,\n",
            "         0.03435797,  0.02427559, -0.02876582],\n",
            "       [ 0.00289179,  0.02258947, -0.03073791, -0.01881478,  0.00727978,\n",
            "        -0.03200714,  0.02943229, -0.03538792],\n",
            "       [-0.12221376,  0.14009759,  0.07993479,  0.10728455, -0.07159436,\n",
            "        -0.06616177, -0.12927265, -0.14739443],\n",
            "       [-0.09027217,  0.14332537, -0.12474135,  0.10782289, -0.0905453 ,\n",
            "        -0.0780693 , -0.07868128, -0.08519091],\n",
            "       [ 0.00723616,  0.04535064, -0.01548364, -0.00083483, -0.0380283 ,\n",
            "        -0.03049867, -0.01260613, -0.02210091],\n",
            "       [-0.03197943, -0.03501765,  0.04956199,  0.01092048, -0.01276875,\n",
            "        -0.01832694,  0.02115563, -0.03598317],\n",
            "       [-0.02291756,  0.00221515, -0.02634329, -0.0076458 , -0.02589483,\n",
            "        -0.01328955, -0.03472431,  0.03567215],\n",
            "       [ 0.03785196,  0.02074387, -0.0279022 , -0.02449441,  0.03517475,\n",
            "         0.00244949, -0.03968938, -0.04773844],\n",
            "       [-0.08439546, -0.06066361, -0.09350428,  0.08577346, -0.08678576,\n",
            "        -0.13318855,  0.08347764,  0.10264909],\n",
            "       [ 0.14036462,  0.05074624,  0.09249677, -0.08472584,  0.09181633,\n",
            "         0.10181464, -0.07478791, -0.10989237],\n",
            "       [-0.02542802,  0.01632531, -0.00327851, -0.01901442,  0.02312246,\n",
            "         0.02612999,  0.03132368,  0.03031215]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQAiapl6OdFO",
        "colab_type": "text"
      },
      "source": [
        "অ্যাক্যুরেসি কেন ১ এসেছে? কারণ, দশটা বাক্যের মধ্যে সবগুলোই ইউনিক। একটার সাথে আরেকটার মিল নেই। আর, এতো ছোট উদাহরণে আর কি হবে?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPvlpHsNrjLU",
        "colab_type": "code",
        "outputId": "69efabf8-e96f-4287-bc62-7f0f60ffd99f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss, accuracy = model.evaluate(padded_reviews, labels, verbose=0)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVYFsXuIbHK0",
        "colab_type": "text"
      },
      "source": [
        "আরেকটা উদাহরণ দেখি। এসেছে স্ট্যাকওভারফ্লো থেকে। আমরা যা আলাপ করেছি সেখানে ওয়ান হট এনকোডিং একদম চোখে দেখা হয়নি। খালি চোখে হট এনকোডিং এর ভেতরে দেখতে চাই। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plzrmT4xbUrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGF-jsBsbYE2",
        "colab_type": "text"
      },
      "source": [
        "এখানেও আমরা একটা বাক্যকে ক্লাসিফাই করবো। 'ব্যাগ অফ ওয়ার্ড' (একটা ব্যাগে যতো শব্দ আছে) হচ্ছে একটা মেকানিজম যার মাধ্যমে টেক্সট থেকে ফিচার বের করা যায়। এটা আসলে আমাদের এই তিনটা উদাহরন থেকে যতো ইউনিক শব্দ আছে সেটা থেকে সে ট্রেনিং নেয়। তবে, আমরা তার থেকেও ভালো মডেল মানে সিকোয়েন্সের ওপর জোর দেবো।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9yE6vy9cAtl",
        "colab_type": "code",
        "outputId": "eaecc23f-4b1b-4d41-8795-05afada4277a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_classes = 5 \n",
        "max_words = 20\n",
        "sentences = ['আমি আর আসছি না এখানে!',\n",
        "              'কথা শোনে না ওয়েটার',\n",
        "            'একদম ঠান্ডা খাবার']\n",
        "# লেবেল তৈরি করি\n",
        "labels = np.random.randint(0, num_classes, 3)\n",
        "y = to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "words = set(w for sent in sentences for w in sent.split())\n",
        "word_map = {w : i+1 for (i, w) in enumerate(words)}\n",
        "sent_ints = [[word_map[w] for w in sent.split()] for sent in sentences]\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI3TxDorl5Hw",
        "colab_type": "text"
      },
      "source": [
        "বাক্যের শব্দের সিকোয়েন্স, ৫, ৪, ৩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1u40lNjlztt",
        "colab_type": "code",
        "outputId": "0948b5b4-1249-4a9d-f13e-8e71e1ac27cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(sent_ints)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6, 7, 11, 2, 3], [4, 10, 2, 1], [8, 5, 9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFHbTsYHiisK",
        "colab_type": "text"
      },
      "source": [
        "আমাদেরকে প্যাড করতে হবে max_words লেনথ ধরে, যা পাবো len(words) এবং ১ যোগ করে। +১ মানে হচ্ছে আমরা ০কে রিজার্ভ রাখবো যাতে সেটার বাইরে না যায়। এখানে ওয়ান হট এনকোডিং এবং প্যাডিং করছি -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx50nUsufCfN",
        "colab_type": "code",
        "outputId": "a4bf0414-4a11-47c3-8779-7129a8fdec34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ট্রেনিং ডেটার X এর হিসেব\n",
        "X = np.array([to_categorical((pad_sequences((sent,), \n",
        "    max_words)).reshape(20,),vocab_size + 1) for sent in sent_ints])\n",
        "print(X.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 20, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi9UPNAlna11",
        "colab_type": "text"
      },
      "source": [
        "এখন দেখি আমাদের ওয়ান হট এনকোডিং এর অবস্থা? হাজারো ০। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNGqytPrnOtT",
        "colab_type": "code",
        "outputId": "26cb07ea-1b96-4049-d583-53cd0856acc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltNmRAeWp-nf",
        "colab_type": "code",
        "outputId": "db8ca373-d36b-434d-f126-bc0bc39b7d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# y দেখি\n",
        "print(y)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtHeqlNVj7Oy",
        "colab_type": "text"
      },
      "source": [
        "মডেলে ডেন্স লেয়ার যোগ করছি যাতে ওয়ান হট শব্দগুলোকে ডেন্স ভেক্টরে পাল্টে নেয়া যায়। তো LSTM এর কাজ কি? আমাদের বাক্যের ভেতরে শব্দের ভেক্টরকে কনভার্ট করতে হবে ডেন্স বাক্য ভেক্টরে। একদম শেষ লাইনে সফটম্যাক্স এক্টিভেশন ফাংশন ব্যবহার করছি যাতে ক্লাসের ওপর প্রবাবিলিটি ডিস্ট্রিবিউশন চালাতে পারে। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJgHhxWYfkFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words, vocab_size + 1)))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "          optimizer='adam',\n",
        "          metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwB1YH6If5dw",
        "colab_type": "code",
        "outputId": "105bfa87-c964-4b6b-9b65-85b1c41d2655",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "model.fit(X,y)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3 samples\n",
            "3/3 [==============================] - 4s 1s/sample - loss: 1.5875 - accuracy: 0.3333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f31900c4cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bXyPHd6Ntlo",
        "colab_type": "text"
      },
      "source": [
        "## শব্দ এবং সংখ্যার কাছাকাছি এনকোডিং\n",
        "\n",
        "শব্দ যখন সংখ্যা হবে, তখন শব্দের মধ্যে যেরকম সম্পর্ক ছিলো, সেটা আরো ভালো বোঝা যাবে সংখ্যায়। শব্দে \"আমি\" \"তুমি\" যেমন কাছাকাছি, সংখ্যায় সেটা আরো পরিস্কার হবে। এমবেডিং এ এটা এমন ধরনের ‘ডেন্স’ রিপ্রেজেন্টেশন যার মধ্যে একই ধরনের শব্দের একই বা কাছাকাছি ধরনের এনকোডিং হবে। ‘পুরুষ’ এবং ‘মহিলা’ এই দুটো অক্ষরের মধ্যে আকাশ পাতাল পার্থক্য হলেও সম্পর্কের কারণে এদুটো কাছাকাছি থাকবে। সংখ্যার এনকোডিং ও একই ধরনের হবে। সবচেয়ে বড় কথা হচ্ছে এ ধরনের এনকোডিং ম্যানুয়ালি বা আমাদের হাতে লিখে দিতে হবে না। বরং যেহেতু এগুলো ট্রেনিংযোগ্য প্যারামিটার ফলে মডেলের ট্রেনিং এর সময় ওয়েটগুলো ‘এডজাস্ট’ হবে এদের সম্পর্কের ভিত্তিতে। একটা ছোট ডাটাসেটের ওয়ার্ড এম্বেডিং ৮ ডাইমেনশনাল হলেও সেটা বড় ডাটাসেটের জন্য ১০২৪ ডাইমেনশন পর্যন্ত যেতে পারে। যত বেশি ডাইমেনশনাল এম্বেডিং ততবেশি শব্দগুলোর মধ্যে সম্পর্কগুলোকে আরো ভালোভাবে বোঝা যাবে তবে তার জন্য প্রচুর ডাটা লাগবে শিখতে।\n",
        "\n",
        "## এমবেডিং এর প্রি-ট্রেইনড মডেল\n",
        "\n",
        "আমরা যখন ‘ওয়ান হট’ এনকোডিং উদাহরণ দেখছিলাম, সেখানে প্রতিটা শব্দ একটা ৪ ডাইমেনশনাল ফ্লোটিং পয়েন্ট সংখ্যা ভেক্টর দিয়ে রিপ্রেজেন্ট করা হয়েছিল। আমরা এ ধরনের এম্বেডিংকে বলতে পারি ‘লুকআপ’ টেবিল। এই ‘লুকআপ’ টেবিলের ওয়েটগুলো শিখছে যখন আমরা ‘ডেন্স’ ভেক্টরের তার করেসপন্ডিং টেবিল দেখছি। ফলে সেটা ধরে আমরা প্রতিটা শব্দ ধরে এনকোডিং করছি। তবে, এখানে একটা বড় সমস্যা আছে। আমরা যখন এনকোডিং করবো তখন সব বাংলা শব্দকে একসাথে এনকোডিং না করলে আমরা কিভাবে একটা শব্দকে আরেকটা শব্দের সাথে সিমিলারিটি, শব্দগুলোর মধ্যে দূরত্ব বের করবো? তাদের মধ্যে সিমান্টিক এনালিসিস কে করবে? সব ডাটা একসাথে না হলে আমাদের কাজ হবে না। সে সমস্যা মেটাতে এসেছে প্রি-ট্রেইনড মডেল। কোটি কোটি শব্দকে একসাথে ট্রেইন করিয়ে তারপর এনকোডিং করেছে একসাথে। নিজে বানানো কঠিন। সেটার জন্য যে রিসোর্স দরকার সেটা নেই আমাদের কাছে। ফলে এধরনের প্রি-ট্রেইনড মডেলগুলো অসাধারণ পারদর্শী, কারণ সে ট্রেনিং এর সময় সব শব্দ হাতের কাছে পেয়েছে। শুধু নামিয়ে নিতে হবে দরকারের সময়ে। তাদের শক্তি এবং দুর্বলতা নিয়ে আলাপ করবো না এই বইয়ে।\n",
        "\n",
        "এই প্রি-ট্রেইনড মডেল হিসেবে আমার প্রিয় ফেইসবুকের ফাস্টটেক্সট। ইংরেজি ছাড়াও আরো ১৫৭টা ভাষায় এর প্রি-ট্রেইনড মডেল আছে। বাংলা তো অবশ্যই। পুরোনো লাইব্রেরি হিসেবে ওয়ার্ড২ভেক এখনো জনপ্রিয়। পাশাপাশি পাইথনের জন্য স্পেসি আমার পছন্দের। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPhkHnJ2Lm4o",
        "colab_type": "text"
      },
      "source": [
        "ধারণাগুলো এসেছে টেন্সর-ফ্লো ডকুমেন্ট, জেফ হিটনের নোটবুক এবং স্ট্যাকওভারফ্লো থেকে।\n",
        "\n",
        "https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_11_03_embedding.ipynb"
      ]
    }
  ]
}