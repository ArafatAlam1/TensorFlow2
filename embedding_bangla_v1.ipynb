{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "embedding_bangla_v1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raqueeb/TensorFlow2/blob/master/embedding_bangla_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi3C6LhrrjKx",
        "colab_type": "text"
      },
      "source": [
        "## এমবেডিং, ওয়ার্ড এমবেডিং এবং বাংলায় টেক্সট অ্যানালাইসিস\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4TNlkGyLz4C",
        "colab_type": "text"
      },
      "source": [
        "একটা জিনিস ভেবে দেখেছেন কি? আমরা এ পর্যন্ত মেশিন লার্নিং মডেলের ইনপুট হিসেবে যা দিয়েছি তা সবই সংখ্যা। মনে করে দেখুন, এপর্যন্ত সব মডেলের ক্লাসিফিকেশন অথবা রিগ্রেশন এর জন্য যা দিয়েছি সব সংখ্যায় দিয়েছি। তার পাশাপাশি ইমেজ নিয়ে যখন কাজ করেছি তখনো কিন্তু ইমেজ (সেটা গ্রেস্কেল হোক আর কালার হোক - তার জন্য গ্রেস্কেল ইনটেনসিটি অথবা কালারের আরজিবি চ্যানেলের আউটপুট), সবকিছুই সংখ্যায় গিয়েছে। এর অর্থ হচ্ছে মেশিন লার্নিং/ডিপ লার্নিং মডেল সংখ্যা ছাড়া আর কিছু বোঝেনা। আর বুঝবেই বা কিভাবে? সেতো যন্ত্র। আর মানুষের সবকিছুই কমপ্লেক্স। \n",
        "\n",
        "সেদিক থেকে মানুষের ভাষা আরো অনেক কমপ্লেক্স। আমরা একেকজন একেক ভাষায় কথা বলি, ভাষাগুলোর মধ্যে সংযোগ/সিমিলারিটি এবং কি বলতে গিয়ে কি বলে ফেললাম এবং তার ফলাফল, তার পাশাপাশি অনেক শব্দ একটা ভাষায় যা বোঝায় সেটা অন্য ভাষায় তার বৈপরীত্য দেখায়। এখন আপনি বাংলায় কথা বললেও সেটার মধ্যে ৪০% বাইরের শব্দ ব্যবহার করলে তো আরো সমস্যা। আর এই কারণে টেক্সট নিয়ে কাজ করা বেশ কমপ্লেক্স। \n",
        "\n",
        "যেকোনো ল্যাঙ্গুয়েজে তার প্রতিটা শব্দের একটা অর্থ আছে। তবে এটার অর্থ অনেক সময় নির্ভর করে কনটেক্সটে বা শব্দটা বাক্যের মধ্যে কোথায় এই মুহূর্তে আছে। একই শব্দের আবার অনেকগুলো কনটেক্সচুয়াল অর্থ থাকে সে কারণে শব্দকে শুধুমাত্র শব্দ বা অক্ষর লেভেলে কাজ করলে হবে না। কারণ, ডিপ লার্নিং মডেল যেহেতু সংখ্যা ছাড়া কিছু বোঝেনা, সে কারণে একেক ভাষার একেক বুলি এবং বকাবকি এর পাশাপাশি সেই ভাষাগুলোকে ঠিকমতো সংখ্যায় ট্রান্সফার করা একটা চ্যালেঞ্জ এর কাজ অবশ্যই।\n",
        "\n",
        "## কেন টাইম সিরিজ নিয়ে আলাপ হয়নি?\n",
        "\n",
        "এই বইতে আমি ইচ্ছে করে ‘টাইম সিরিজ’ যোগ করিনি, কারণ সেটার এপ্লিকেশন লেভেল এখনো বেসিক লেভেলে নেই। তবে, এই রিকারেন্ট নিউরাল নেটওয়ার্ককে শিখিয়ে দিলে সে (আরএনএন) ফ্রি ফর্ম (ইচ্ছেমতো) টেক্সট জেনারেট করতে পারে। আমরা যেমন দেখেছি ‘এল এস টি এম’, (লঙ শর্ট টার্ম মেমোরি) নেটওয়ার্কে ‘শেক্সপিয়ার’ ক্লাসিক পড়তে দিলে, সে শেক্সপিয়ারের মতো আরেকটা ক্লাসিক লিখে ফেলেছে, যেখানে এই নেটওয়ার্কের মধ্যে শব্দ, বাক্য এবং ব্যাকরণ তৈরির কোন ধারণা নেই। কারণ, ডিপ লার্নিং প্রচুর ক্লাসিক বই পড়ে বুঝেছে কিভাবে শব্দ, বাক্য বা তার ব্যাকরণ ব্যবহার করতে হয় এর ভেতরে না ঢুকেই। টাইম সিরিজের ব্যাপারটা হচ্ছে সে পরের জিনিসটা প্রেডিক্ট করবে। আগের সময়ে কি ছিলো, সেটাকে ধরে এরপরে কি কি আসতে পারে সেটাই বলবে সে। না বুঝে। যদি শব্দ লেভেলে দেখি, ‘আমি’ এর পর কি আসতে পারে তাহলে ‘ভালো’ আসতে পারে, কারণ ‘আমি ভালো আছি’ একটা বহুল প্রচলিত বাক্য। \n",
        "\n",
        "বড় ব্যাপার হচ্ছে এই অক্ষর থেকেই ‘এল এস টি এম’ আস্তে আস্তে মানবিক ব্যাকরন এবং কিভাবে একটা বাক্য তৈরি করতে হয় সে ধরনের একটা ধারণা পেয়ে থাকে। এটা সে কোন কিছু বুঝে করে না। প্যাটার্ন থেকে করে। আর সে কারণেই এটার উপরে আমরা খুব একটা ভরসা করব না। আমরা চাইব মেশিনকে শেখাতে, যেভাবে মানুষ ভাষা, শব্দ, ব্যাকরণ শেখে। ওই একই কারণে অক্ষর লেভেলে টেক্সট জেনারেশন নিয়ে আমরা এই মুহূর্তে আলাপ করব না। ইন্টারনেটে দেখতে পারেন কিভাবে একেকটা ‘এল এস টি এম’ নেটওয়ার্ক শেক্সপিয়ারের মত বড় বড় নাটক লিখে ফেলছে। এটা আসলে সে একটা ক্লাসিক্যাল লেখার প্যাটার্ন দেখে তার পার্সপেক্টিভ থেকে লিখেছে। বুঝে লেখেনি। \n",
        "\n",
        "## ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং\n",
        "\n",
        "সেজন্য এর মধ্যে এসে যোগ হয়েছে ‘ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং’। ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং এর মধ্যে লিঙ্গুইস্টিকস, কম্পিউটার সায়েন্স, ইনফরমেশন ইঞ্জিনিয়ারিং এবং কৃত্রিম বুদ্ধিমত্তা ব্যাপারগুলো চলে এসেছে কাজের স্বার্থে। এমনিতেই মানুষ এবং যন্ত্রের মধ্যে একটা যোগসূত্র স্থাপন করা বেশ ঝামেলার ব্যাপার। ‘স্পিচ রিকগনিশন’ এর পাশাপাশি স্বয়ংক্রিয়ভাবে একটা ভাষা বুঝতে পারা এবং তার পাশাপাশি সেই ভাষায় বুঝে টেক্সট জেনারেট করা সহজ ব্যাপার নয়, যখন সবকিছুর পেছনে কাজ করে সংখ্যা। সবচেয়ে বড় কথা হচ্ছে আপনি একটা যন্ত্রকে শেখাচ্ছেন সে আপনার সাথে ন্যাচারালি যোগসুত্র স্থাপন করতে পারে। এই ‘ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং’ এর শুরুটা হচ্ছে সেই টেক্সটকে ঠিকমতো পড়তে পারা, সেটাকে নিজের মতো করে বোঝা এবং মানুষ যেভাবে একটা বাক্যের ‘ইনটেন্ট’ বুঝতে পারে সেভাবে তাকে বুঝিয়ে তার কাছ থেকে উত্তর বের করা। সহজ ব্যাপার নয়। তবে শুরু করতে হবে কোথাও। \n",
        "\n",
        "যেহেতু ‘ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং’ একটা বিশাল সাবজেক্ট, আমি এই বইতে ব্যাপারটা আনার ব্যর্থ চেষ্টা করব না। তবে কম্পিউটার কিভাবে ‘টেক্সট’ এর সাথে ‘ইন্টারঅ্যাক্ট’ করে সেটা নিয়েই দেখব আমরা সামনের চ্যাপ্টারগুলোতে। আমরা জানি মেশিন লার্নিং মডেল ভেক্টরকে ইনপুট হিসেবে নেয়। আর এই ভেক্টরগুলো হচ্ছে সংখ্যার অ্যারে। যখন আমরা টেক্সট মানে শব্দ এবং বাক্য নিয়ে আলাপ করব, তখন আমাদের প্রথম কাজ হবে এই স্ট্রিংগুলোকে কিভাবে সংখ্যায় পাল্টানো যায়। অর্থাৎ সেই বাক্য বা শব্দটি কিভাবে ‘ভেক্টরাইজ’ করা যায় মডেলে দেবার আগে। আমাদের এই চ্যাপ্টার থেকে শুরু করব কিভাবে আস্তে আস্তে ডিপ নিউরাল নেটওয়ার্ক দিয়ে বাংলার একটা অ্যাপ্লিকেশনে যাওয়া যায়।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaDQqrJg7p68",
        "colab_type": "code",
        "outputId": "cfb6e60a-4d87-4396-9308-9bf289054489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  # শুধুমাত্র টেন্সর-ফ্লো ২.x ব্যবহার করবো \n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "keras = tf.keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0un3M2_MnHe",
        "colab_type": "text"
      },
      "source": [
        "আমরা যেহেতু ডিপ নিউরাল নেটওয়ার্কের কথা বলছি সেখানে সবকিছুতেই লেয়ার কনসেপ্ট কাজ করে। আজকে এখানে আমরা নতুন একটা লেয়ার নিয়ে কথা বলবো যেটাকে আমরা বলছি এম্বেডিং লেয়ার। শুরুতেই অনেকে বলবেন এম্বেডিং মানে কি? এর সাথে সংখ্যার সম্পর্ক কি?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4bim_ShMw8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "embedding_layer = layers.Embedding(1000, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRmBcA6iMx71",
        "colab_type": "code",
        "outputId": "b0a8aceb-cc61-4848-e03f-fe15ee45e2cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# তিনটা সংখ্যা পাঠালাম, দেখি কি করে?\n",
        "# সংখ্যায় ওয়েট \n",
        "\n",
        "result = embedding_layer(tf.constant([1,2,3]))\n",
        "result.numpy()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01292027, -0.04601173,  0.01023299,  0.01819516, -0.02822559],\n",
              "       [-0.04976157, -0.04597345, -0.04188   ,  0.02008814, -0.04267219],\n",
              "       [-0.00977442, -0.03336428, -0.03086242, -0.04975804, -0.00510005]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYLOnO5Uxmmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# অথবা টেন্সর-ফ্লো দিয়ে টোকেনাইজ করে দেখি\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mezzU-TxprR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "text = ['আমি এখন বই পড়ি']\n",
        "# text = tokenizer.fit_on_texts(text)\n",
        "sequences = tokenizer.texts_to_sequences(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVVdIpcqxrxB",
        "colab_type": "code",
        "outputId": "0a8ef396-4f40-4a05-86b0-fc96ca6b55d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sequences"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHqel-xSNGyu",
        "colab_type": "text"
      },
      "source": [
        "## ‘এম্বেডিং’ সিমিলারিটি, একটা থেকে আরেকটা শব্দের দূরত্ব\n",
        "\n",
        "মেশিন লার্নিং এর ভাষায় ‘এম্বেডিং’ হচ্ছে সিমিলারিটি, যদি শব্দের কথা বলি তাহলে একটা শব্দ থেকে আরেকটা শব্দ কতটুকু যুক্ত বা একটা শব্দ থেকে আরেকটা শব্দ কত দূরে? তাদের মধ্যে সম্পর্ক আছে কিনা? যেমন, রাজা’র সাথে ‘রানী’ শব্দটা কিন্তু সম্পর্কযুক্ত। যেমন, ‘মা’ শব্দের সাথে ‘বাবা’ সম্পর্কযুক্ত। ‘বাংলাদেশ’ শব্দের সাথে ‘ঢাকা’ সম্পর্কযুক্ত। সেই থেকে ‘আকাশ’ শব্দের সাথে ‘টেবিল’ কিন্তু বহু দূরে মানে তাদের মধ্যে সম্পর্ক টানা বেশ কঠিন, যদি না কোনদিন ‘আকাশ’ থেকে ‘টেবিল’ পড়ে।\n",
        "\n",
        "এই এম্বেডিং ব্যাপারটা এসেছে কিছুটা ‘ওয়ান হট’ এনকোডিং থেকে। তবে, ব্যাপারটা ঠিক সেরকম নয়। শব্দকে যেহেতু আমাদেরকে সংখ্যায় পাল্টাতে হবে তাহলে আর বাকি উপায় কি? মনে আছে, আমাদের এই ‘ওয়ান হট’ এনকোডিং বা ‘ডামি ভেরিয়েবল’ ব্যাপারটা এসেছে শব্দ দিয়ে তৈরি ক্যাটাগরিকাল ভেরিয়েবল থেকে? মেশিন লার্নিং মডেলে আমরা যখন কিছু শব্দকে সংখ্যার ক্যাটাগরিতে ভাগ করতে চাই, যেটা এমুহুর্তে সংখ্যায় নেই। যেমন, আইরিশ ডাটাসেটের তিন প্রজাতির ফুলের জন্য ০,১,২, ভাগে ভাগ করতে পারছিনা, সেখানে চলে এসছে এই ডামি ভেরিয়েবল। আমরা যাকে বলছি শব্দকে দিয়ে তার জন্য একটা করে এনকোডিং। এর অর্থ হচ্ছে, আমাদের ভাষায় যতগুলো শব্দ আছে সেগুলোকে ‘ওয়ান হট’ এনকোডিং করা যেতে পারে। আর সেখানেই সমস্যা। \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/raqueeb/deep_learning_book/master/assets/hot.png\"> চিত্রঃ ‘ওয়ান হট’ এনকোডিং এর উদাহরণ\n",
        "\n",
        "এখানে একটা উদাহরণ দেয়া যাক। একটা বাক্য। ‘আমি এখন বই পড়ি’। এই চারটা শব্দকে যদি আমরা সংখ্যায় পাল্টাতে চাই, তাহলে আমাদের এই ভোকাবুলারির প্রতিটা শব্দকে ‘০’ ভেক্টর দিয়ে শুরু করব। আমাদের এখানে যেহেতু ৪টা ইউনিক শব্দ, সে কারণে এই শূন্য ভেক্টরের দৈর্ঘ্য হবে ৪। প্রতিটি শব্দকে ঠিকমতো রিপ্রেজেন্ট করার জন্য একেকটা শব্দের ইনডেক্সে তার করেসপন্ডিং ‘১’ বসাবো। ছবি দেখি। এখন এই টেবিল থেকে প্রতিটা শব্দের জন্য তার ভেক্টর বের করা সোজা। আমাদের এই চারটা শব্দের জন্য ভেক্টরের দৈর্ঘ্য হচ্ছে ৪ যার বাকি তিনটাই ০। এভাবে আমরা শব্দকে বিভিন্ন ক্যাটাগরিতে ভাগ করতে পারি। তবে ব্যাপারটা সেরকম এফিশিয়েন্ট নয় যখন আমাদের ভোকাবুলারিতে ১০ হাজার শব্দ থাকবে। তার মানে একেকটা ভেক্টরের দৈর্ঘ্য ১০ হাজার হবে - এর মধ্যে ৯৯.৯৯ শতাংশই হচ্ছে ০। এটা দক্ষ সিস্টেম না। আমরা যদি প্রতিটা বাংলা শব্দের জন্য ডামি ভ্যারিয়েবল বানাই, তাহলে কতো বড় ডামি ভ্যারিয়েবলের একেকটা টেবিল হবে?\n",
        "\n",
        "এই সমস্যা থেকে বের হবার উপায় কি? প্রতিটা শব্দকে একটা করে ইউনিক সংখ্যা দিয়ে দেওয়া। আমাদের আগের ভোকাবুলারিটাকে ‘আমি এখন বই পড়ি’কে আমরা একটা ‘ডেন্স’ ভেক্টরের মত আলাদা আলাদা করে সংখ্যা দিয়ে দিতে পারি। এই জিনিসটা আগের থেকে অনেকটাই এফিশিয়েন্ট। একেকটা শব্দের জন্য একেকটা আলাদা আলাদা সংখ্যা। সবচেয়ে বড় ব্যাপার হচ্ছে বিশাল ‘স্পার্স’ হাই-ডাইমেনশন স্পেস থেকে কম ডাইমেনশন স্পেসে চলে এলাম। তবে, এটার সমস্যা দুটো।\n",
        "\n",
        "* ১. আমাদের এই সংখ্যায় এনকোডিং সিস্টেমটাকে ‘ম্যানুয়ালি’ করা হয়েছে। ফলে এদের মধ্যে কোন ‘রিলেশনশিপ’ বের করা যাচ্ছে না। অংকেও এক সংখ্যা থেকে আরেক সংখ্যার মধ্যে যে রিলেশনশিপ সেটা অনুপস্থিত। ফলে আমাদের অংকের ভাষায় কো-সাইন ভেক্টরে কে কোথায় আছে সেটা বের করা মুশকিল।\n",
        "\n",
        "* ২. একটা মডেলের জন্য এই ধরনের ‘ম্যানুয়াল’ এনকোডিং শব্দগুলোর মধ্যে সম্পর্ক না বুঝলে সেটা সমস্যা হয়ে দাঁড়াবে। যেহেতু এই একেকটা ফিচারের জন্য একেকটা ‘ওয়েট’ সে কারণে একটা লিনিয়ার ক্লাসিফায়ারের পক্ষে এই ফিচার এবং ওয়েট এর কম্বিনেশন কোন সম্পর্ক দেখাবেনা। এটা বড় সমস্যা। \n",
        "\n",
        "আর সে কারণেই চলে এসেছে ওয়ার্ড এম্বেডিং।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awPlT5SRBH2G",
        "colab_type": "text"
      },
      "source": [
        "## একটা ছোট্ট উদাহরন, রেস্টুরেন্ট রিভিউ\n",
        "\n",
        "আমরা এখানে একটা এমবেডিং লেয়ারের উদাহরণ নিয়ে আসি। ইনপুট ডাইমেনশনে কতগুলো ভোকাবুলারি আছে? ধরে নিচ্ছি ৫০টা। আমি সংখ্যা না গুনেই বলছি। ৫০টা ডামি ভ্যারিয়েবল। মানে আমরা কতগুলো ক্যাটেগরিতে এনকোড করছি। আমাদের লুক আপ টেবিলে কতগুলো আইটেম আছে সেটাই এখানে আসবে। \n",
        "\n",
        "আমাদের ৫০টা ভোকাবুলারির জন্য 'ওয়ান হট' এনকোডিং ব্যবহার করছি, কেরাসের ভেতরে। এটা ব্যবহার করা ঠিক না তবে ছোটখাট উদাহরণে সেটা করা যায়। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEOvo2pjrjKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Embedding, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS7ZaxjhMJ0w",
        "colab_type": "text"
      },
      "source": [
        "আমরা এখানে ছোট একটা দশটা রেস্টুরেন্টের রিভিউ যোগ করেছি। এর পাশাপাশি তার পাঁচটা নেগেটিভ আর পাঁচটা পজেটিভ লেবেল যোগ করেছি। এর উপর আমরা নিউরাল নেটওয়ার্কে ট্রেইন করব। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ecoq69LrjK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ১০টা েস্টুরেন্ট রিভিউএর ইনপুট\n",
        "reviews = [\n",
        "    'আমি আর আসছি না এখানে!',\n",
        "    'একদম বাজে সার্ভিস',\n",
        "    'কথা শোনে না ওয়েটার',\n",
        "    'একদম ঠান্ডা খাবার',\n",
        "    'বাজে খাবার!',\n",
        "    'অসাধারণ',\n",
        "    'অসাধারণ সার্ভিস!',\n",
        "    'খুব ভালো!',\n",
        "    'মোটামুটি',\n",
        "    'এর থেকে ভালো হয়না']\n",
        "\n",
        "# লেবেল বলে দিচ্ছি (1=নেগেটিভ, 0=পজেটিভ)\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CxmAlaErjK3",
        "colab_type": "code",
        "outputId": "106e44ae-f726-4ba1-f28a-924347fb87e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(reviews[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "আমি আর আসছি না এখানে!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59EA_3KSKOos",
        "colab_type": "text"
      },
      "source": [
        "শব্দের সংখ্যা না গুনেও একটা ধারনা নিয়েছি ৫০টা মানে ভোকাবুলারি সাইজ ৫০। ছোট্ট উদাহরণ বলে আমরা ‘ওয়ান হট এনকোডিং’ ব্যবহার করছি। এর কাজ হচ্ছে এই বাক্যগুলোকে শব্দে ভেঙে ফেলা। যাকে আমরা বলছি টোকেনাইজিং। এরপর তাকে নিজস্ব ইনডেক্সে পাঠাবে। শেষে আমরা যা পাব সেটা সংখ্যার সিকোয়েন্স। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZk5g6PrjK5",
        "colab_type": "code",
        "outputId": "926552c6-b3bf-46d7-87ca-0ee118daa74e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ওয়ান হট এনকোডিং\n",
        "\n",
        "VOCAB_SIZE = 50\n",
        "encoded_reviews = [one_hot(d, VOCAB_SIZE) for d in reviews]\n",
        "print(f\"Encoded reviews: {encoded_reviews}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded reviews: [[33, 26, 47, 7, 3], [28, 29, 36], [33, 18, 7, 41], [28, 48, 8], [29, 8], [21], [21, 36], [6, 40], [39], [47, 19, 40, 49]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok0ds1g-LOOl",
        "colab_type": "text"
      },
      "source": [
        "যতগুলো শব্দ ততগুলো করে সংখ্যা একেকটা অংশে। তবে এই সিকোয়েন্সে একটা কনসিস্টেন্সি রাখার জন্য একটা লেনথ ঠিক করে দিতে হবে। শব্দ ধরে এখন কোনটা ২ অথবা ৩ এবং ৪ লেনথে, কিন্তু প্যাডিং এর জন্য আমরা MAX_LENGTH = 4 ধরে দিচ্ছি। দেখুন এখানে বাকি অংশ ০ দিয়ে ভরে দিয়েছে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFLBnh2erjK8",
        "colab_type": "code",
        "outputId": "04bcc3a8-8a09-4e3d-b43a-c5618d1b8a88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "MAX_LENGTH = 4\n",
        "\n",
        "padded_reviews = pad_sequences(encoded_reviews, maxlen=MAX_LENGTH, padding='post')\n",
        "print(padded_reviews)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[26 47  7  3]\n",
            " [28 29 36  0]\n",
            " [33 18  7 41]\n",
            " [28 48  8  0]\n",
            " [29  8  0  0]\n",
            " [21  0  0  0]\n",
            " [21 36  0  0]\n",
            " [ 6 40  0  0]\n",
            " [39  0  0  0]\n",
            " [47 19 40 49]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-10aiXydLsei",
        "colab_type": "text"
      },
      "source": [
        "আমরা আগের মতো সিকোয়েন্সিয়াল নেটওয়ার্ক তৈরি করছি। আউটপুট ডাইমেনশনে কমিয়ে এনেছি সেটা। এখানে একটা ডেন্স লেয়ার, তার মানে একটা ওয়েট ম্যাট্রিক্স, একটা লার্নিং চলছে এখানে। এই এমবেডিং লেয়ারে যে লার্নিং চলছে সেটা শব্দগুলোকে একটা ইউক্লুডিয়ান স্পেসে সংখ্যাগুলোর মধ্যে একটা রিলেশনশিপ তৈরি হচ্ছে।\n",
        "\n",
        "মডেলটা দেখুন।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBM4Eu7OrjLJ",
        "colab_type": "code",
        "outputId": "bafb427e-f2e0-4173-c665-28c9c812d837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model = Sequential()\n",
        "embedding_layer = Embedding(VOCAB_SIZE, 8, input_length=MAX_LENGTH)\n",
        "model.add(embedding_layer)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 4, 8)              400       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVDOD7q7rjLO",
        "colab_type": "code",
        "outputId": "5dfded5c-9faf-4cf3-97ff-29d16ad07984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# মডেলকে ট্রেইন করি\n",
        "model.fit(padded_reviews, labels, epochs=100, verbose=0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcf10032cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQpHhhrbOhdM",
        "colab_type": "text"
      },
      "source": [
        "১০০ ইপকের পর ভালো করে লক্ষ্য করুন, প্রতিটা লাইন হচ্ছে একেকটা শব্দের এমবেডিং। এখানে ওয়েটগুলোর অর্থ এখন খুঁজতে যাবো না এমুহুর্তে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnaII1xnrjLS",
        "colab_type": "code",
        "outputId": "64d9db9c-5bd5-48ca-b92d-790de206bf8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(embedding_layer.get_weights()[0].shape)\n",
        "print(embedding_layer.get_weights())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 8)\n",
            "[array([[ 0.12585934,  0.12055044, -0.13184611, -0.12842496,  0.08517194,\n",
            "         0.08335733,  0.12692465,  0.06182508],\n",
            "       [-0.02510858,  0.01512161, -0.00961005, -0.03354134,  0.04331979,\n",
            "         0.00620731, -0.04257011,  0.00396841],\n",
            "       [-0.0043382 ,  0.02794323,  0.01058098, -0.02588817, -0.04823387,\n",
            "         0.00962473, -0.01157993, -0.04490024],\n",
            "       [ 0.09689204, -0.09647593, -0.14184956, -0.09461554, -0.04772292,\n",
            "        -0.07266354, -0.14519414,  0.06013146],\n",
            "       [ 0.02390437, -0.02710233,  0.02101711,  0.04199151, -0.00082118,\n",
            "         0.02394939, -0.03862994,  0.0419798 ],\n",
            "       [ 0.02385707,  0.04749665, -0.02588153,  0.02790846, -0.04810146,\n",
            "         0.01625298, -0.01855359, -0.04789291],\n",
            "       [-0.11053139,  0.12058411,  0.14462997,  0.09678406, -0.05960013,\n",
            "         0.13244301, -0.06912795, -0.11505311],\n",
            "       [-0.08924689, -0.16025941,  0.10376618,  0.14182462, -0.10332751,\n",
            "        -0.11989013, -0.15043773, -0.13111217],\n",
            "       [-0.10767378, -0.12229363,  0.13915391,  0.09303327,  0.06528834,\n",
            "         0.11732177, -0.11123375, -0.08615942],\n",
            "       [ 0.00078512,  0.03374106,  0.01039839,  0.01407851, -0.03474419,\n",
            "        -0.03154556, -0.03942076,  0.03452155],\n",
            "       [-0.03357013, -0.03755023, -0.04401132, -0.02539574, -0.02127308,\n",
            "        -0.00853587,  0.02502284, -0.0337041 ],\n",
            "       [ 0.03255418,  0.02293009, -0.01622772, -0.00792742,  0.0017046 ,\n",
            "        -0.04960076,  0.00595092,  0.01986482],\n",
            "       [ 0.01770392,  0.03141787, -0.04961336,  0.03937615, -0.00128938,\n",
            "         0.01157277,  0.01967349, -0.00792638],\n",
            "       [ 0.04116689,  0.01714827, -0.0071905 , -0.04033635, -0.04980139,\n",
            "        -0.02200198, -0.02207324, -0.01473129],\n",
            "       [-0.02314523, -0.04644245,  0.01132416,  0.02080125, -0.03367877,\n",
            "         0.00230474,  0.0307363 , -0.04098042],\n",
            "       [-0.0226465 ,  0.03733725, -0.03040946, -0.00985539,  0.01515964,\n",
            "         0.01397368,  0.03529909, -0.02482829],\n",
            "       [-0.01711975,  0.02923555, -0.01945517, -0.02331653, -0.03012857,\n",
            "        -0.02463033,  0.0365198 ,  0.02350761],\n",
            "       [-0.02511218,  0.02495143, -0.00600879, -0.0112867 , -0.01701869,\n",
            "        -0.00831179, -0.0325021 ,  0.02817961],\n",
            "       [-0.0580092 , -0.09587915,  0.09868997,  0.08951486,  0.09782542,\n",
            "         0.11739578, -0.04537   ,  0.06912342],\n",
            "       [ 0.09762927,  0.09702345, -0.12825866, -0.11821704, -0.1271324 ,\n",
            "        -0.14786595,  0.04967824, -0.11354149],\n",
            "       [-0.02699174,  0.02781152, -0.04908258,  0.04587496, -0.01058986,\n",
            "        -0.01871498,  0.00381222, -0.02170757],\n",
            "       [-0.14256369,  0.1460447 ,  0.06777636,  0.0842182 , -0.08970033,\n",
            "         0.10388549, -0.13939105, -0.05456903],\n",
            "       [-0.04907137, -0.0486896 ,  0.02285666,  0.03765473, -0.01748024,\n",
            "        -0.04802768, -0.00915806,  0.00514667],\n",
            "       [-0.00483714,  0.02093203, -0.01393574, -0.0285486 , -0.00228792,\n",
            "         0.01775204, -0.0034479 ,  0.030395  ],\n",
            "       [-0.03264743, -0.00440086, -0.02724249,  0.01603914,  0.01939446,\n",
            "        -0.04348319, -0.03255069,  0.03157691],\n",
            "       [-0.03935496, -0.0218528 , -0.01859043,  0.02756765, -0.00705523,\n",
            "        -0.01913015,  0.0083534 ,  0.04576856],\n",
            "       [ 0.13776137, -0.1091646 , -0.1409199 , -0.14056896,  0.09335829,\n",
            "        -0.12397435,  0.05401891,  0.07777404],\n",
            "       [ 0.04593649,  0.00740385,  0.01111765, -0.04677341, -0.04136374,\n",
            "         0.02103357, -0.04437355,  0.00101636],\n",
            "       [ 0.15169   , -0.08441468, -0.08927321, -0.06057505,  0.13865127,\n",
            "        -0.07322386,  0.14475928,  0.09401865],\n",
            "       [-0.08529086, -0.1380179 , -0.00900828, -0.13620839,  0.12071522,\n",
            "         0.14203376,  0.01127126,  0.10427311],\n",
            "       [ 0.00709166,  0.00220091,  0.03455779, -0.04086407, -0.04661652,\n",
            "        -0.03274933, -0.03730441,  0.00588872],\n",
            "       [ 0.03132317,  0.04575843,  0.00747728,  0.0258765 , -0.02810062,\n",
            "        -0.04581932, -0.01859308,  0.0049762 ],\n",
            "       [-0.0048681 ,  0.02336894,  0.01697141, -0.00795113,  0.01324851,\n",
            "        -0.04297772, -0.04267685,  0.02695424],\n",
            "       [ 0.15970856, -0.10604892, -0.13639046, -0.08684369,  0.08532782,\n",
            "        -0.05485988,  0.08797276,  0.12547919],\n",
            "       [ 0.02493975, -0.01338899, -0.03966431, -0.02157658,  0.03184387,\n",
            "         0.04801034,  0.04194418, -0.03345615],\n",
            "       [-0.02357293, -0.01371934, -0.03993354, -0.02289983, -0.02045364,\n",
            "         0.02561199,  0.03071867,  0.03868734],\n",
            "       [ 0.09720244,  0.13364343, -0.02517971,  0.086222  , -0.1035857 ,\n",
            "        -0.12593098,  0.1269148 , -0.06722855],\n",
            "       [ 0.03300831,  0.02005691,  0.01075125, -0.00474572, -0.0171191 ,\n",
            "         0.01557655, -0.00956552, -0.01014336],\n",
            "       [ 0.03024937,  0.01577714,  0.02501651, -0.03086859,  0.00573951,\n",
            "         0.03811372, -0.03655785,  0.0323948 ],\n",
            "       [-0.13116969,  0.14435111,  0.1041678 ,  0.12807593, -0.09661414,\n",
            "         0.14373836, -0.0628288 , -0.11592697],\n",
            "       [ 0.11044235,  0.05259141, -0.08399598, -0.06504916, -0.11493375,\n",
            "        -0.13101445,  0.07924069,  0.09497181],\n",
            "       [ 0.13806476, -0.12509154, -0.09453797, -0.04683473, -0.13776185,\n",
            "        -0.09476886, -0.16483386,  0.13979712],\n",
            "       [-0.01238567, -0.04416471,  0.03494268, -0.02925347, -0.04506516,\n",
            "        -0.00450524, -0.04968673,  0.03818781],\n",
            "       [ 0.03891751, -0.01892077, -0.03894953,  0.02608346, -0.01682641,\n",
            "        -0.02621101, -0.00509353,  0.01351656],\n",
            "       [-0.01509633, -0.01566013, -0.03905464, -0.04267634,  0.02434549,\n",
            "         0.01119398, -0.02615578,  0.03903032],\n",
            "       [ 0.02143017,  0.04385931, -0.03874875,  0.01848653,  0.00212628,\n",
            "        -0.0387346 , -0.03866417,  0.00167637],\n",
            "       [-0.01329233, -0.04429894, -0.00548081, -0.02943977, -0.0212207 ,\n",
            "        -0.01763246,  0.03238174, -0.01965308],\n",
            "       [-0.08270587, -0.03947104,  0.10609672,  0.06588684,  0.06072551,\n",
            "         0.05862922, -0.05385091, -0.09339146],\n",
            "       [-0.08516324, -0.05727753,  0.097989  ,  0.10180256,  0.08363444,\n",
            "         0.0546281 , -0.12069859,  0.14920378],\n",
            "       [-0.0531799 ,  0.11855838,  0.10158937,  0.0360407 ,  0.13609888,\n",
            "         0.15171577,  0.08968089, -0.13166519]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQAiapl6OdFO",
        "colab_type": "text"
      },
      "source": [
        "অ্যাক্যুরেসি কেন ১ এসেছে? কারণ, দশটা বাক্যের মধ্যে সবগুলোই ইউনিক। একটার সাথে আরেকটার মিল নেই। আর, এতো ছোট উদাহরণে আর কি হবে?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPvlpHsNrjLU",
        "colab_type": "code",
        "outputId": "2ce1ef4d-4a0c-45a0-ab10-476d765f3557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss, accuracy = model.evaluate(padded_reviews, labels, verbose=0)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVYFsXuIbHK0",
        "colab_type": "text"
      },
      "source": [
        "আরেকটা উদাহরণ দেখি। এসেছে স্ট্যাকওভারফ্লো থেকে। আমরা যা আলাপ করেছি সেখানে ওয়ান হট এনকোডিং একদম চোখে দেখা হয়নি। খালি চোখে হট এনকোডিং এর ভেতরে দেখতে চাই। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plzrmT4xbUrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGF-jsBsbYE2",
        "colab_type": "text"
      },
      "source": [
        "এখানেও আমরা একটা বাক্যকে ক্লাসিফাই করবো। 'ব্যাগ অফ ওয়ার্ড' (একটা ব্যাগে যতো শব্দ আছে) হচ্ছে একটা মেকানিজম যার মাধ্যমে টেক্সট থেকে ফিচার বের করা যায়। এটা আসলে আমাদের এই তিনটা উদাহরন থেকে যতো ইউনিক শব্দ আছে সেটা থেকে সে ট্রেনিং নেয়। তবে, আমরা তার থেকেও ভালো মডেল মানে সিকোয়েন্সের ওপর জোর দেবো।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9yE6vy9cAtl",
        "colab_type": "code",
        "outputId": "324c597b-4d90-4f3d-b20b-1704e260c556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_classes = 5 \n",
        "max_words = 20\n",
        "sentences = ['আমি আর আসছি না এখানে!',\n",
        "              'কথা শোনে না ওয়েটার',\n",
        "            'একদম ঠান্ডা খাবার']\n",
        "# লেবেল তৈরি করি\n",
        "labels = np.random.randint(0, num_classes, 3)\n",
        "y = to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "words = set(w for sent in sentences for w in sent.split())\n",
        "word_map = {w : i+1 for (i, w) in enumerate(words)}\n",
        "sent_ints = [[word_map[w] for w in sent.split()] for sent in sentences]\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI3TxDorl5Hw",
        "colab_type": "text"
      },
      "source": [
        "বাক্যের শব্দের সিকোয়েন্স, ৫, ৪, ৩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1u40lNjlztt",
        "colab_type": "code",
        "outputId": "a11f429f-571a-4d04-8faa-1b49078d0df4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(sent_ints)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7, 3, 11, 4, 9], [10, 6, 4, 1], [2, 8, 5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFHbTsYHiisK",
        "colab_type": "text"
      },
      "source": [
        "আমাদেরকে প্যাড করতে হবে max_words লেনথ ধরে, যা পাবো len(words) এবং ১ যোগ করে। +১ মানে হচ্ছে আমরা ০কে রিজার্ভ রাখবো যাতে সেটার বাইরে না যায়। এখানে ওয়ান হট এনকোডিং এবং প্যাডিং করছি -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx50nUsufCfN",
        "colab_type": "code",
        "outputId": "58af5aa8-03f7-498b-ef3b-1944c4e2346b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ট্রেনিং ডেটার X এর হিসেব\n",
        "X = np.array([to_categorical((pad_sequences((sent,), \n",
        "    max_words)).reshape(20,),vocab_size + 1) for sent in sent_ints])\n",
        "print(X.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 20, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi9UPNAlna11",
        "colab_type": "text"
      },
      "source": [
        "এখন দেখি আমাদের ওয়ান হট এনকোডিং এর অবস্থা? হাজারো ০। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNGqytPrnOtT",
        "colab_type": "code",
        "outputId": "4caf4cc4-c7d4-4518-f9a6-182a996aefe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltNmRAeWp-nf",
        "colab_type": "code",
        "outputId": "8f5bcc64-9cfd-4789-9422-57d3bfafc47e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# y দেখি\n",
        "print(y)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtHeqlNVj7Oy",
        "colab_type": "text"
      },
      "source": [
        "মডেলে ডেন্স লেয়ার যোগ করছি যাতে ওয়ান হট শব্দগুলোকে ডেন্স ভেক্টরে পাল্টে নেয়া যায়। তো LSTM এর কাজ কি? আমাদের বাক্যের ভেতরে শব্দের ভেক্টরকে কনভার্ট করতে হবে ডেন্স বাক্য ভেক্টরে। একদম শেষ লাইনে সফটম্যাক্স এক্টিভেশন ফাংশন ব্যবহার করছি যাতে ক্লাসের ওপর প্রবাবিলিটি ডিস্ট্রিবিউশন চালাতে পারে। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJgHhxWYfkFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words, vocab_size + 1)))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "          optimizer='adam',\n",
        "          metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwB1YH6If5dw",
        "colab_type": "code",
        "outputId": "1a41ed60-5517-42ce-d6e7-50e60fe09e8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "model.fit(X,y)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3 samples\n",
            "3/3 [==============================] - 4s 1s/sample - loss: 1.6137 - accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcec6f497b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bXyPHd6Ntlo",
        "colab_type": "text"
      },
      "source": [
        "## শব্দ এবং সংখ্যার কাছাকাছি এনকোডিং\n",
        "\n",
        "শব্দ যখন সংখ্যা হবে, তখন শব্দের মধ্যে যেরকম সম্পর্ক ছিলো, সেটা আরো ভালো বোঝা যাবে সংখ্যায়। শব্দে \"আমি\" \"তুমি\" যেমন কাছাকাছি, সংখ্যায় সেটা আরো পরিস্কার হবে। এমবেডিং এ এটা এমন ধরনের ‘ডেন্স’ রিপ্রেজেন্টেশন যার মধ্যে একই ধরনের শব্দের একই বা কাছাকাছি ধরনের এনকোডিং হবে। ‘পুরুষ’ এবং ‘মহিলা’ এই দুটো অক্ষরের মধ্যে আকাশ পাতাল পার্থক্য হলেও সম্পর্কের কারণে এদুটো কাছাকাছি থাকবে। সংখ্যার এনকোডিং ও একই ধরনের হবে। সবচেয়ে বড় কথা হচ্ছে এ ধরনের এনকোডিং ম্যানুয়ালি বা আমাদের হাতে লিখে দিতে হবে না। বরং যেহেতু এগুলো ট্রেনিংযোগ্য প্যারামিটার ফলে মডেলের ট্রেনিং এর সময় ওয়েটগুলো ‘এডজাস্ট’ হবে এদের সম্পর্কের ভিত্তিতে। একটা ছোট ডাটাসেটের ওয়ার্ড এম্বেডিং ৮ ডাইমেনশনাল হলেও সেটা বড় ডাটাসেটের জন্য ১০২৪ ডাইমেনশন পর্যন্ত যেতে পারে। যত বেশি ডাইমেনশনাল এম্বেডিং ততবেশি শব্দগুলোর মধ্যে সম্পর্কগুলোকে আরো ভালোভাবে বোঝা যাবে তবে তার জন্য প্রচুর ডাটা লাগবে শিখতে।\n",
        "\n",
        "## এমবেডিং এর প্রি-ট্রেইনড মডেল\n",
        "\n",
        "আমরা যখন ‘ওয়ান হট’ এনকোডিং উদাহরণ দেখছিলাম, সেখানে প্রতিটা শব্দ একটা ৪ ডাইমেনশনাল ফ্লোটিং পয়েন্ট সংখ্যা ভেক্টর দিয়ে রিপ্রেজেন্ট করা হয়েছিল। আমরা এ ধরনের এম্বেডিংকে বলতে পারি ‘লুকআপ’ টেবিল। এই ‘লুকআপ’ টেবিলের ওয়েটগুলো শিখছে যখন আমরা ‘ডেন্স’ ভেক্টরের তার করেসপন্ডিং টেবিল দেখছি। ফলে সেটা ধরে আমরা প্রতিটা শব্দ ধরে এনকোডিং করছি। তবে, এখানে একটা বড় সমস্যা আছে। আমরা যখন এনকোডিং করবো তখন সব বাংলা শব্দকে একসাথে এনকোডিং না করলে আমরা কিভাবে একটা শব্দকে আরেকটা শব্দের সাথে সিমিলারিটি, শব্দগুলোর মধ্যে দূরত্ব বের করবো? তাদের মধ্যে সিমান্টিক এনালিসিস কে করবে? সব ডাটা একসাথে না হলে আমাদের কাজ হবে না। সে সমস্যা মেটাতে এসেছে প্রি-ট্রেইনড মডেল। কোটি কোটি শব্দকে একসাথে ট্রেইন করিয়ে তারপর এনকোডিং করেছে একসাথে। নিজে বানানো কঠিন। সেটার জন্য যে রিসোর্স দরকার সেটা নেই আমাদের কাছে। ফলে এধরনের প্রি-ট্রেইনড মডেলগুলো অসাধারণ পারদর্শী, কারণ সে ট্রেনিং এর সময় সব শব্দ হাতের কাছে পেয়েছে। শুধু নামিয়ে নিতে হবে দরকারের সময়ে। তাদের শক্তি এবং দুর্বলতা নিয়ে আলাপ করবো না এই বইয়ে।\n",
        "\n",
        "এই প্রি-ট্রেইনড মডেল হিসেবে আমার প্রিয় ফেইসবুকের ফাস্টটেক্সট। ইংরেজি ছাড়াও আরো ১৫৭টা ভাষায় এর প্রি-ট্রেইনড মডেল আছে। বাংলা তো অবশ্যই। পুরোনো লাইব্রেরি হিসেবে ওয়ার্ড২ভেক এখনো জনপ্রিয়। পাশাপাশি পাইথনের জন্য স্পেসি আমার পছন্দের। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPhkHnJ2Lm4o",
        "colab_type": "text"
      },
      "source": [
        "ধারণাগুলো এসেছে জেফ হিটনের নোটবুক এবং স্ট্যাকওভারফ্লো থেকে।\n",
        "\n",
        "https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_11_03_embedding.ipynb"
      ]
    }
  ]
}