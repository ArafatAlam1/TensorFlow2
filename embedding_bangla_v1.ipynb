{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "embedding_bangla_v1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raqueeb/TensorFlow2/blob/master/embedding_bangla_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi3C6LhrrjKx",
        "colab_type": "text"
      },
      "source": [
        "## এমবেডিং, ওয়ার্ড এমবেডিং এবং বাংলায় টেক্সট অ্যানালাইসিস\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4TNlkGyLz4C",
        "colab_type": "text"
      },
      "source": [
        "একটা জিনিস ভেবে দেখেছেন কি? আমরা এ পর্যন্ত মেশিন লার্নিং মডেলের ইনপুট হিসেবে যা দিয়েছি তা সবই সংখ্যা। মনে করে দেখুন, এপর্যন্ত সব মডেলের ক্লাসিফিকেশন অথবা রিগ্রেশন এর জন্য যা দিয়েছি সব সংখ্যায় দিয়েছি। তার পাশাপাশি ইমেজ নিয়ে যখন কাজ করেছি তখনো কিন্তু ইমেজ (সেটা গ্রেস্কেল হোক আর কালার হোক - তার জন্য গ্রেস্কেল ইনটেনসিটি অথবা কালারের আরজিবি চ্যানেলের আউটপুট), সবকিছুই সংখ্যায় গিয়েছে। এর অর্থ হচ্ছে মেশিন লার্নিং/ডিপ লার্নিং মডেল সংখ্যা ছাড়া আর কিছু বোঝেনা। আর বুঝবেই বা কিভাবে? সেতো যন্ত্র। আর মানুষের সবকিছুই কমপ্লেক্স। \n",
        "\n",
        "সেদিক থেকে মানুষের ভাষা আরো অনেক কমপ্লেক্স। আমরা একেকজন একেক ভাষায় কথা বলি, ভাষাগুলোর মধ্যে সংযোগ/সিমিলারিটি এবং কি বলতে গিয়ে কি বলে ফেললাম এবং তার ফলাফল, তার পাশাপাশি অনেক শব্দ একটা ভাষায় যা বোঝায় সেটা অন্য ভাষায় তার বৈপরীত্য দেখায়। এখন আপনি বাংলায় কথা বললেও সেটার মধ্যে ৪০% বাইরের শব্দ ব্যবহার করলে তো আরো সমস্যা। আর এই কারণে টেক্সট নিয়ে কাজ করা বেশ কমপ্লেক্স। \n",
        "\n",
        "যেকোনো ল্যাঙ্গুয়েজে তার প্রতিটা শব্দের একটা অর্থ আছে। তবে এটার অর্থ অনেক সময় নির্ভর করে কনটেক্সটে বা শব্দটা বাক্যের মধ্যে কোথায় এই মুহূর্তে আছে। একই শব্দের আবার অনেকগুলো কনটেক্সচুয়াল অর্থ থাকে সে কারণে শব্দকে শুধুমাত্র শব্দ বা অক্ষর লেভেলে কাজ করলে হবে না। কারণ, ডিপ লার্নিং মডেল যেহেতু সংখ্যা ছাড়া কিছু বোঝেনা, সে কারণে একেক ভাষার একেক বুলি এবং বকাবকি এর পাশাপাশি সেই ভাষাগুলোকে ঠিকমতো সংখ্যায় ট্রান্সফার করা একটা চ্যালেঞ্জ এর কাজ অবশ্যই।\n",
        "\n",
        "## কেন টাইম সিরিজ নিয়ে আলাপ হয়নি?\n",
        "\n",
        "এই বইতে আমি ইচ্ছে করে ‘টাইম সিরিজ’ যোগ করিনি, কারণ সেটার এপ্লিকেশন লেভেল এখনো বেসিক লেভেলে নেই। তবে, এই রিকারেন্ট নিউরাল নেটওয়ার্ককে শিখিয়ে দিলে সে (আরএনএন) ফ্রি ফর্ম (ইচ্ছেমতো) টেক্সট জেনারেট করতে পারে। আমরা যেমন দেখেছি ‘এল এস টি এম’, (লঙ শর্ট টার্ম মেমোরি) নেটওয়ার্কে ‘শেক্সপিয়ার’ ক্লাসিক পড়তে দিলে, সে শেক্সপিয়ারের মতো আরেকটা ক্লাসিক লিখে ফেলেছে, যেখানে এই নেটওয়ার্কের মধ্যে শব্দ, বাক্য এবং ব্যাকরণ তৈরির কোন ধারণা নেই। কারণ, ডিপ লার্নিং প্রচুর ক্লাসিক বই পড়ে বুঝেছে কিভাবে শব্দ, বাক্য বা তার ব্যাকরণ ব্যবহার করতে হয় এর ভেতরে না ঢুকেই। টাইম সিরিজের ব্যাপারটা হচ্ছে সে পরের জিনিসটা প্রেডিক্ট করবে। আগের সময়ে কি ছিলো, সেটাকে ধরে এরপরে কি কি আসতে পারে সেটাই বলবে সে। না বুঝে। যদি শব্দ লেভেলে দেখি, ‘আমি’ এর পর কি আসতে পারে তাহলে ‘ভালো’ আসতে পারে, কারণ ‘আমি ভালো আছি’ একটা বহুল প্রচলিত বাক্য। \n",
        "\n",
        "বড় ব্যাপার হচ্ছে এই অক্ষর থেকেই ‘এল এস টি এম’ আস্তে আস্তে মানবিক ব্যাকরন এবং কিভাবে একটা বাক্য তৈরি করতে হয় সে ধরনের একটা ধারণা পেয়ে থাকে। এটা সে কোন কিছু বুঝে করে না। প্যাটার্ন থেকে করে। আর সে কারণেই এটার উপরে আমরা খুব একটা ভরসা করব না। আমরা চাইব মেশিনকে শেখাতে, যেভাবে মানুষ ভাষা, শব্দ, ব্যাকরণ শেখে। ওই একই কারণে অক্ষর লেভেলে টেক্সট জেনারেশন নিয়ে আমরা এই মুহূর্তে আলাপ করব না। ইন্টারনেটে দেখতে পারেন কিভাবে একেকটা ‘এল এস টি এম’ নেটওয়ার্ক শেক্সপিয়ারের মত বড় বড় নাটক লিখে ফেলছে। এটা আসলে সে একটা ক্লাসিক্যাল লেখার প্যাটার্ন দেখে তার পার্সপেক্টিভ থেকে লিখেছে। বুঝে লেখেনি। \n",
        "\n",
        "## ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং\n",
        "\n",
        "সেজন্য এর মধ্যে এসে যোগ হয়েছে ‘ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং’। ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং এর মধ্যে লিঙ্গুইস্টিকস, কম্পিউটার সায়েন্স, ইনফরমেশন ইঞ্জিনিয়ারিং এবং কৃত্রিম বুদ্ধিমত্তা ব্যাপারগুলো চলে এসেছে কাজের স্বার্থে। এমনিতেই মানুষ এবং যন্ত্রের মধ্যে একটা যোগসূত্র স্থাপন করা বেশ ঝামেলার ব্যাপার। ‘স্পিচ রিকগনিশন’ এর পাশাপাশি স্বয়ংক্রিয়ভাবে একটা ভাষা বুঝতে পারা এবং তার পাশাপাশি সেই ভাষায় বুঝে টেক্সট জেনারেট করা সহজ ব্যাপার নয়, যখন সবকিছুর পেছনে কাজ করে সংখ্যা। সবচেয়ে বড় কথা হচ্ছে আপনি একটা যন্ত্রকে শেখাচ্ছেন সে আপনার সাথে ন্যাচারালি যোগসুত্র স্থাপন করতে পারে। এই ‘ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং’ এর শুরুটা হচ্ছে সেই টেক্সটকে ঠিকমতো পড়তে পারা, সেটাকে নিজের মতো করে বোঝা এবং মানুষ যেভাবে একটা বাক্যের ‘ইনটেন্ট’ বুঝতে পারে সেভাবে তাকে বুঝিয়ে তার কাছ থেকে উত্তর বের করা। সহজ ব্যাপার নয়। তবে শুরু করতে হবে কোথাও। \n",
        "\n",
        "যেহেতু ‘ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং’ একটা বিশাল সাবজেক্ট, আমি এই বইতে ব্যাপারটা আনার ব্যর্থ চেষ্টা করব না। তবে কম্পিউটার কিভাবে ‘টেক্সট’ এর সাথে ‘ইন্টারঅ্যাক্ট’ করে সেটা নিয়েই দেখব আমরা সামনের চ্যাপ্টারগুলোতে। আমরা জানি মেশিন লার্নিং মডেল ভেক্টরকে ইনপুট হিসেবে নেয়। আর এই ভেক্টরগুলো হচ্ছে সংখ্যার অ্যারে। যখন আমরা টেক্সট মানে শব্দ এবং বাক্য নিয়ে আলাপ করব, তখন আমাদের প্রথম কাজ হবে এই স্ট্রিংগুলোকে কিভাবে সংখ্যায় পাল্টানো যায়। অর্থাৎ সেই বাক্য বা শব্দটি কিভাবে ‘ভেক্টরাইজ’ করা যায় মডেলে দেবার আগে। আমাদের এই চ্যাপ্টার থেকে শুরু করব কিভাবে আস্তে আস্তে ডিপ নিউরাল নেটওয়ার্ক দিয়ে বাংলার একটা অ্যাপ্লিকেশনে যাওয়া যায়।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaDQqrJg7p68",
        "colab_type": "code",
        "outputId": "5e180a3a-429f-436e-ffaa-bf4937adaabf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  # শুধুমাত্র টেন্সর-ফ্লো ২.x ব্যবহার করবো \n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "keras = tf.keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0un3M2_MnHe",
        "colab_type": "text"
      },
      "source": [
        "আমরা যেহেতু ডিপ নিউরাল নেটওয়ার্কের কথা বলছি সেখানে সবকিছুতেই লেয়ার কনসেপ্ট কাজ করে। আজকে এখানে আমরা নতুন একটা লেয়ার নিয়ে কথা বলবো যেটাকে আমরা বলছি এম্বেডিং লেয়ার। শুরুতেই অনেকে বলবেন এম্বেডিং মানে কি? এর সাথে সংখ্যার সম্পর্ক কি?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4bim_ShMw8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "embedding_layer = layers.Embedding(1000, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRmBcA6iMx71",
        "colab_type": "code",
        "outputId": "212978f8-763e-4363-ff29-5e7ab8397ebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# তিনটা সংখ্যা পাঠালাম, দেখি কি করে?\n",
        "# সংখ্যায় ওয়েট \n",
        "\n",
        "result = embedding_layer(tf.constant([1,2,3]))\n",
        "result.numpy()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.01187975,  0.03535623,  0.01777541, -0.02199599, -0.04960341],\n",
              "       [ 0.02448957, -0.02763765, -0.03665041,  0.03637788, -0.01790854],\n",
              "       [ 0.02317014, -0.02551311, -0.03900546, -0.03208224, -0.0314441 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYLOnO5Uxmmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# অথবা টেন্সর-ফ্লো দিয়ে টোকেনাইজ করে দেখি\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mezzU-TxprR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "text = ['আমি এখন বই পড়ি', 'এটি আমার অনেক পছন্দের একটা বই']\n",
        "tokenizer.fit_on_texts(text)\n",
        "sequences = tokenizer.texts_to_sequences(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVVdIpcqxrxB",
        "colab_type": "code",
        "outputId": "9941b321-2be2-4724-d9df-9b3c8e565de1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sequences"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 3, 1, 4], [5, 6, 7, 8, 9, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZxexs73hmRo",
        "colab_type": "text"
      },
      "source": [
        "ইউনিকোড স্ট্রিং দুভাবে টেন্সর-ফ্লোতে দেখানো যায়। স্ট্রিং স্কেলারে কোড পয়েন্টের সিকোয়েন্সকে এনকোড করবে জানা ক্যারেক্টার এনকোডিং দিয়ে। আমরা ভেক্টর নিয়ে দেখালে। এখানে প্রতিটা পজিশনে একটা সিঙ্গেল কোড পয়েন্ট আছে।\n",
        "\n",
        "এখানে কয়েকটা উদাহরণ দেখালাম, তবে মাথা খারাপ করার দরকার নেই। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXxHWH-We9Vg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "06358d78-fb61-4f17-ffc3-5efa02de12b5"
      },
      "source": [
        "text_chars = tf.constant([ord(char) for char in u\"আমি এখন বই পড়ি\"])\n",
        "text_chars"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=16, shape=(15,), dtype=int32, numpy=\n",
              "array([2438, 2478, 2495,   32, 2447, 2454, 2472,   32, 2476, 2439,   32,\n",
              "       2474, 2465, 2492, 2495], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dxKTGEZkRwZ",
        "colab_type": "text"
      },
      "source": [
        "যা দেখছেন, ইউনিকোড স্ট্রিং, দেখাচ্ছে ইউনিকোড কোড পয়েন্ট ভেক্টরে। ব্যাচে দেখি। সব সংখ্যার খেলা।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AutcJ9tfe9Sh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "93ae75fc-162d-4c73-ab05-883aeb0a6a93"
      },
      "source": [
        "batch_utf8 = [s.encode('UTF-8') for s in\n",
        "              [u'প্রিয় বন্ধু',  u'কেমন চলছে তোমার?',  u'আমি ভালো', u'😊']]\n",
        "batch_chars_ragged = tf.strings.unicode_decode(batch_utf8,\n",
        "                                               input_encoding='UTF-8')\n",
        "for sentence_chars in batch_chars_ragged.to_list():\n",
        "  print(sentence_chars)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2474, 2509, 2480, 2495, 2527, 32, 2476, 2472, 2509, 2471, 2497]\n",
            "[2453, 2503, 2478, 2472, 32, 2458, 2482, 2459, 2503, 32, 2468, 2507, 2478, 2494, 2480, 63]\n",
            "[2438, 2478, 2495, 32, 2477, 2494, 2482, 2507]\n",
            "[128522]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89HMTwJagkYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f9835156-00c0-4ede-88ea-f7c6d4a810fb"
      },
      "source": [
        "batch_chars_padded = batch_chars_ragged.to_tensor(default_value=-1)\n",
        "print(batch_chars_padded.numpy())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  2474   2509   2480   2495   2527     32   2476   2472   2509   2471\n",
            "    2497     -1     -1     -1     -1     -1]\n",
            " [  2453   2503   2478   2472     32   2458   2482   2459   2503     32\n",
            "    2468   2507   2478   2494   2480     63]\n",
            " [  2438   2478   2495     32   2477   2494   2482   2507     -1     -1\n",
            "      -1     -1     -1     -1     -1     -1]\n",
            " [128522     -1     -1     -1     -1     -1     -1     -1     -1     -1\n",
            "      -1     -1     -1     -1     -1     -1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuzAjhepliiv",
        "colab_type": "text"
      },
      "source": [
        "মাল্টি ডাইমেনশনে দেখি।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY5U_UCMg8MS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "19488ce1-8af0-4a41-9a26-78f0c851927c"
      },
      "source": [
        "print(tf.strings.unicode_script(batch_chars_ragged))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.RaggedTensor [[4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4], [4, 4, 4, 4, 0, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 0], [4, 4, 4, 0, 4, 4, 4, 4], [0]]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHqel-xSNGyu",
        "colab_type": "text"
      },
      "source": [
        "## ‘এম্বেডিং’ সিমিলারিটি, একটা থেকে আরেকটা শব্দের দূরত্ব\n",
        "\n",
        "মেশিন লার্নিং এর ভাষায় ‘এম্বেডিং’ হচ্ছে সিমিলারিটি, যদি শব্দের কথা বলি তাহলে একটা শব্দ থেকে আরেকটা শব্দ কতটুকু যুক্ত বা একটা শব্দ থেকে আরেকটা শব্দ কত দূরে? তাদের মধ্যে সম্পর্ক আছে কিনা? যেমন, রাজা’র সাথে ‘রানী’ শব্দটা কিন্তু সম্পর্কযুক্ত। যেমন, ‘মা’ শব্দের সাথে ‘বাবা’ সম্পর্কযুক্ত। ‘বাংলাদেশ’ শব্দের সাথে ‘ঢাকা’ সম্পর্কযুক্ত। সেই থেকে ‘আকাশ’ শব্দের সাথে ‘টেবিল’ কিন্তু বহু দূরে মানে তাদের মধ্যে সম্পর্ক টানা বেশ কঠিন, যদি না কোনদিন ‘আকাশ’ থেকে ‘টেবিল’ পড়ে।\n",
        "\n",
        "এই এম্বেডিং ব্যাপারটা এসেছে কিছুটা ‘ওয়ান হট’ এনকোডিং থেকে। তবে, ব্যাপারটা ঠিক সেরকম নয়। শব্দকে যেহেতু আমাদেরকে সংখ্যায় পাল্টাতে হবে তাহলে আর বাকি উপায় কি? মনে আছে, আমাদের এই ‘ওয়ান হট’ এনকোডিং বা ‘ডামি ভেরিয়েবল’ ব্যাপারটা এসেছে শব্দ দিয়ে তৈরি ক্যাটাগরিকাল ভেরিয়েবল থেকে? মেশিন লার্নিং মডেলে আমরা যখন কিছু শব্দকে সংখ্যার ক্যাটাগরিতে ভাগ করতে চাই, যেটা এমুহুর্তে সংখ্যায় নেই। যেমন, আইরিশ ডাটাসেটের তিন প্রজাতির ফুলের জন্য ০,১,২, ভাগে ভাগ করতে পারছিনা, সেখানে চলে এসছে এই ডামি ভেরিয়েবল। আমরা যাকে বলছি শব্দকে দিয়ে তার জন্য একটা করে এনকোডিং। এর অর্থ হচ্ছে, আমাদের ভাষায় যতগুলো শব্দ আছে সেগুলোকে ‘ওয়ান হট’ এনকোডিং করা যেতে পারে। আর সেখানেই সমস্যা। \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/raqueeb/deep_learning_book/master/assets/hot.png\"> চিত্রঃ ‘ওয়ান হট’ এনকোডিং এর উদাহরণ\n",
        "\n",
        "এখানে একটা উদাহরণ দেয়া যাক। একটা বাক্য। ‘আমি এখন বই পড়ি’। এই চারটা শব্দকে যদি আমরা সংখ্যায় পাল্টাতে চাই, তাহলে আমাদের এই ভোকাবুলারির প্রতিটা শব্দকে ‘০’ ভেক্টর দিয়ে শুরু করব। আমাদের এখানে যেহেতু ৪টা ইউনিক শব্দ, সে কারণে এই শূন্য ভেক্টরের দৈর্ঘ্য হবে ৪। প্রতিটি শব্দকে ঠিকমতো রিপ্রেজেন্ট করার জন্য একেকটা শব্দের ইনডেক্সে তার করেসপন্ডিং ‘১’ বসাবো। ছবি দেখি। এখন এই টেবিল থেকে প্রতিটা শব্দের জন্য তার ভেক্টর বের করা সোজা। আমাদের এই চারটা শব্দের জন্য ভেক্টরের দৈর্ঘ্য হচ্ছে ৪ যার বাকি তিনটাই ০। এভাবে আমরা শব্দকে বিভিন্ন ক্যাটাগরিতে ভাগ করতে পারি। তবে ব্যাপারটা সেরকম এফিশিয়েন্ট নয় যখন আমাদের ভোকাবুলারিতে ১০ হাজার শব্দ থাকবে। তার মানে একেকটা ভেক্টরের দৈর্ঘ্য ১০ হাজার হবে - এর মধ্যে ৯৯.৯৯ শতাংশই হচ্ছে ০। এটা দক্ষ সিস্টেম না। আমরা যদি প্রতিটা বাংলা শব্দের জন্য ডামি ভ্যারিয়েবল বানাই, তাহলে কতো বড় ডামি ভ্যারিয়েবলের একেকটা টেবিল হবে?\n",
        "\n",
        "এই সমস্যা থেকে বের হবার উপায় কি? প্রতিটা শব্দকে একটা করে ইউনিক সংখ্যা দিয়ে দেওয়া। আমাদের আগের ভোকাবুলারিটাকে ‘আমি এখন বই পড়ি’কে আমরা একটা ‘ডেন্স’ ভেক্টরের মত আলাদা আলাদা করে সংখ্যা দিয়ে দিতে পারি। এই জিনিসটা আগের থেকে অনেকটাই এফিশিয়েন্ট। একেকটা শব্দের জন্য একেকটা আলাদা আলাদা সংখ্যা। সবচেয়ে বড় ব্যাপার হচ্ছে বিশাল ‘স্পার্স’ হাই-ডাইমেনশন স্পেস থেকে কম ডাইমেনশন স্পেসে চলে এলাম। তবে, এটার সমস্যা দুটো।\n",
        "\n",
        "* ১. আমাদের এই সংখ্যায় এনকোডিং সিস্টেমটাকে ‘ম্যানুয়ালি’ করা হয়েছে। ফলে এদের মধ্যে কোন ‘রিলেশনশিপ’ বের করা যাচ্ছে না। অংকেও এক সংখ্যা থেকে আরেক সংখ্যার মধ্যে যে রিলেশনশিপ সেটা অনুপস্থিত। ফলে আমাদের অংকের ভাষায় কো-সাইন ভেক্টরে কে কোথায় আছে সেটা বের করা মুশকিল।\n",
        "\n",
        "* ২. একটা মডেলের জন্য এই ধরনের ‘ম্যানুয়াল’ এনকোডিং শব্দগুলোর মধ্যে সম্পর্ক না বুঝলে সেটা সমস্যা হয়ে দাঁড়াবে। যেহেতু এই একেকটা ফিচারের জন্য একেকটা ‘ওয়েট’ সে কারণে একটা লিনিয়ার ক্লাসিফায়ারের পক্ষে এই ফিচার এবং ওয়েট এর কম্বিনেশন কোন সম্পর্ক দেখাবেনা। এটা বড় সমস্যা। \n",
        "\n",
        "আর সে কারণেই চলে এসেছে ওয়ার্ড এম্বেডিং।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awPlT5SRBH2G",
        "colab_type": "text"
      },
      "source": [
        "## একটা ছোট্ট উদাহরন, রেস্টুরেন্ট রিভিউ\n",
        "\n",
        "আমরা এখানে একটা এমবেডিং লেয়ারের উদাহরণ নিয়ে আসি। ইনপুট ডাইমেনশনে কতগুলো ভোকাবুলারি আছে? ধরে নিচ্ছি ৫০টা। আমি সংখ্যা না গুনেই বলছি। ৫০টা ডামি ভ্যারিয়েবল। মানে আমরা কতগুলো ক্যাটেগরিতে এনকোড করছি। আমাদের লুক আপ টেবিলে কতগুলো আইটেম আছে সেটাই এখানে আসবে। \n",
        "\n",
        "আমাদের ৫০টা ভোকাবুলারির জন্য 'ওয়ান হট' এনকোডিং ব্যবহার করছি, কেরাসের ভেতরে। এটা ব্যবহার করা ঠিক না তবে ছোটখাট উদাহরণে সেটা করা যায়। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEOvo2pjrjKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Embedding, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS7ZaxjhMJ0w",
        "colab_type": "text"
      },
      "source": [
        "আমরা এখানে ছোট একটা দশটা রেস্টুরেন্টের রিভিউ যোগ করেছি। এর পাশাপাশি তার পাঁচটা নেগেটিভ আর পাঁচটা পজেটিভ লেবেল যোগ করেছি। এর উপর আমরা নিউরাল নেটওয়ার্কে ট্রেইন করব। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ecoq69LrjK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ১০টা েস্টুরেন্ট রিভিউএর ইনপুট\n",
        "reviews = [\n",
        "    'আমি আর আসছি না এখানে!',\n",
        "    'একদম বাজে সার্ভিস',\n",
        "    'কথা শোনে না ওয়েটার',\n",
        "    'একদম ঠান্ডা খাবার',\n",
        "    'বাজে খাবার!',\n",
        "    'অসাধারণ',\n",
        "    'অসাধারণ সার্ভিস!',\n",
        "    'খুব ভালো!',\n",
        "    'মোটামুটি',\n",
        "    'এর থেকে ভালো হয়না']\n",
        "\n",
        "# লেবেল বলে দিচ্ছি (1=নেগেটিভ, 0=পজেটিভ)\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CxmAlaErjK3",
        "colab_type": "code",
        "outputId": "870b681d-9981-4018-a263-35b06cee431f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(reviews[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "আমি আর আসছি না এখানে!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59EA_3KSKOos",
        "colab_type": "text"
      },
      "source": [
        "শব্দের সংখ্যা না গুনেও একটা ধারনা নিয়েছি ৫০টা মানে ভোকাবুলারি সাইজ ৫০। ছোট্ট উদাহরণ বলে আমরা ‘ওয়ান হট এনকোডিং’ ব্যবহার করছি। এর কাজ হচ্ছে এই বাক্যগুলোকে শব্দে ভেঙে ফেলা। যাকে আমরা বলছি টোকেনাইজিং। এরপর তাকে নিজস্ব ইনডেক্সে পাঠাবে। শেষে আমরা যা পাব সেটা সংখ্যার সিকোয়েন্স। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZk5g6PrjK5",
        "colab_type": "code",
        "outputId": "9fc22ee0-8737-4628-9312-83d2048d6ce4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ওয়ান হট এনকোডিং\n",
        "\n",
        "VOCAB_SIZE = 50\n",
        "encoded_reviews = [one_hot(d, VOCAB_SIZE) for d in reviews]\n",
        "print(f\"Encoded reviews: {encoded_reviews}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded reviews: [[21, 2, 26, 32, 20], [40, 45, 8], [3, 28, 32, 23], [40, 43, 27], [45, 27], [22], [22, 8], [23, 17], [46], [9, 24, 17, 47]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok0ds1g-LOOl",
        "colab_type": "text"
      },
      "source": [
        "যতগুলো শব্দ ততগুলো করে সংখ্যা একেকটা অংশে। তবে এই সিকোয়েন্সে একটা কনসিস্টেন্সি রাখার জন্য একটা লেনথ ঠিক করে দিতে হবে। শব্দ ধরে এখন কোনটা ২ অথবা ৩ এবং ৪ লেনথে, কিন্তু প্যাডিং এর জন্য আমরা MAX_LENGTH = 4 ধরে দিচ্ছি। দেখুন এখানে বাকি অংশ ০ দিয়ে ভরে দিয়েছে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFLBnh2erjK8",
        "colab_type": "code",
        "outputId": "e23c5075-d7f6-4606-ddba-2d6ccda0c5da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "MAX_LENGTH = 4\n",
        "\n",
        "padded_reviews = pad_sequences(encoded_reviews, maxlen=MAX_LENGTH, padding='post')\n",
        "print(padded_reviews)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 2 26 32 20]\n",
            " [40 45  8  0]\n",
            " [ 3 28 32 23]\n",
            " [40 43 27  0]\n",
            " [45 27  0  0]\n",
            " [22  0  0  0]\n",
            " [22  8  0  0]\n",
            " [23 17  0  0]\n",
            " [46  0  0  0]\n",
            " [ 9 24 17 47]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-10aiXydLsei",
        "colab_type": "text"
      },
      "source": [
        "আমরা আগের মতো সিকোয়েন্সিয়াল নেটওয়ার্ক তৈরি করছি। আউটপুট ডাইমেনশনে কমিয়ে এনেছি সেটা। এখানে একটা ডেন্স লেয়ার, তার মানে একটা ওয়েট ম্যাট্রিক্স, একটা লার্নিং চলছে এখানে। এই এমবেডিং লেয়ারে যে লার্নিং চলছে সেটা শব্দগুলোকে একটা ইউক্লুডিয়ান স্পেসে সংখ্যাগুলোর মধ্যে একটা রিলেশনশিপ তৈরি হচ্ছে।\n",
        "\n",
        "মডেলটা দেখুন।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBM4Eu7OrjLJ",
        "colab_type": "code",
        "outputId": "5e5bbf5f-e56f-40e1-e9a0-09a993cf0c72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model = Sequential()\n",
        "embedding_layer = Embedding(VOCAB_SIZE, 8, input_length=MAX_LENGTH)\n",
        "model.add(embedding_layer)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 4, 8)              400       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVDOD7q7rjLO",
        "colab_type": "code",
        "outputId": "649511f1-1053-4925-de82-5c465b45a861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# মডেলকে ট্রেইন করি\n",
        "model.fit(padded_reviews, labels, epochs=100, verbose=0)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efe943e62e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQpHhhrbOhdM",
        "colab_type": "text"
      },
      "source": [
        "১০০ ইপকের পর ভালো করে লক্ষ্য করুন, প্রতিটা লাইন হচ্ছে একেকটা শব্দের এমবেডিং। এখানে ওয়েটগুলোর অর্থ এখন খুঁজতে যাবো না এমুহুর্তে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnaII1xnrjLS",
        "colab_type": "code",
        "outputId": "eb72f9d7-b621-4e50-e55f-87156c573811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(embedding_layer.get_weights()[0].shape)\n",
        "print(embedding_layer.get_weights())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 8)\n",
            "[array([[-0.05265737,  0.11382533,  0.13469607, -0.1567499 ,  0.13913564,\n",
            "        -0.08606423, -0.02678686, -0.07698093],\n",
            "       [ 0.02344643,  0.03931029,  0.04675907, -0.01921111,  0.02968276,\n",
            "        -0.00453656,  0.01263637,  0.01475035],\n",
            "       [-0.09201057,  0.13638408, -0.10323548,  0.06830094, -0.09231077,\n",
            "        -0.1324037 , -0.11571135, -0.07475247],\n",
            "       [-0.08681908,  0.10543869, -0.1482822 ,  0.1491603 , -0.05974562,\n",
            "        -0.14459705, -0.11688408, -0.06188915],\n",
            "       [-0.02679553, -0.00559205,  0.01724127, -0.01354901, -0.02006083,\n",
            "        -0.04333702, -0.03456335,  0.04879484],\n",
            "       [ 0.01048509,  0.04804746,  0.01675353, -0.00327244,  0.00998007,\n",
            "         0.03570483,  0.00859393, -0.01275213],\n",
            "       [ 0.03552392, -0.00229199, -0.03412475,  0.04219795,  0.00482456,\n",
            "        -0.03159416, -0.00589075, -0.01066098],\n",
            "       [-0.02867593,  0.03261894, -0.00464242, -0.03158412,  0.0191541 ,\n",
            "        -0.03579491,  0.03718572,  0.03181786],\n",
            "       [ 0.12123187,  0.06596823, -0.12166881,  0.06698894, -0.14485735,\n",
            "         0.07797965, -0.11583629, -0.13997374],\n",
            "       [ 0.12766133, -0.14226912,  0.1280246 , -0.13345785,  0.10734078,\n",
            "         0.13667764,  0.10611239,  0.12766172],\n",
            "       [-0.01263506,  0.0456824 ,  0.01305434, -0.03433765, -0.03328185,\n",
            "         0.03810361, -0.00140941, -0.03408922],\n",
            "       [ 0.02760527, -0.03518636, -0.0055925 , -0.01955928,  0.01609213,\n",
            "         0.01068318, -0.04387803, -0.00512721],\n",
            "       [ 0.00079768,  0.03258968, -0.01003956,  0.01334674,  0.03598468,\n",
            "        -0.02812053, -0.01648907, -0.04035139],\n",
            "       [ 0.03973031, -0.00060577,  0.00952489,  0.04756439, -0.03742932,\n",
            "        -0.00386493,  0.0414752 , -0.02521375],\n",
            "       [-0.03779197, -0.0071651 ,  0.04498693,  0.00519955,  0.01166051,\n",
            "        -0.026939  ,  0.01931213,  0.03768314],\n",
            "       [ 0.03159748,  0.04023388, -0.01872511, -0.03790357, -0.03320817,\n",
            "         0.02230166,  0.03325121,  0.00783906],\n",
            "       [ 0.04694238, -0.00746017, -0.03214586, -0.0041917 , -0.00440829,\n",
            "        -0.03374051, -0.00904058,  0.02770592],\n",
            "       [-0.12453386,  0.13165063,  0.07132025, -0.15561348, -0.13765155,\n",
            "        -0.13624804, -0.13847286, -0.0962723 ],\n",
            "       [-0.00603335, -0.04079239,  0.02889638, -0.03909565,  0.02205982,\n",
            "         0.00081826,  0.02584895,  0.0165041 ],\n",
            "       [ 0.01130301, -0.00623764, -0.02375997,  0.00419625,  0.04207266,\n",
            "        -0.0431019 , -0.04712569,  0.02074375],\n",
            "       [ 0.07099983, -0.16610928,  0.12713604, -0.1435597 , -0.13515739,\n",
            "         0.08648594, -0.14594217,  0.09531832],\n",
            "       [ 0.01898981,  0.04618858, -0.02122737,  0.00475402, -0.00572928,\n",
            "         0.03538061, -0.04839328, -0.02000936],\n",
            "       [ 0.0784552 , -0.09470227,  0.11757675, -0.13445877,  0.07080629,\n",
            "         0.0535766 ,  0.05397027,  0.10108405],\n",
            "       [ 0.10054937, -0.13610163,  0.14758286, -0.07658476,  0.13340388,\n",
            "         0.10960875, -0.03239028,  0.09675319],\n",
            "       [ 0.12317961,  0.11413539,  0.04975319,  0.08235742, -0.11445854,\n",
            "         0.06858716, -0.04958973, -0.11873239],\n",
            "       [ 0.03394673,  0.00385653, -0.02558159, -0.0073772 , -0.00029033,\n",
            "        -0.02474689,  0.00900037, -0.00586033],\n",
            "       [-0.12778008, -0.06277162, -0.10699323, -0.13106872,  0.09945107,\n",
            "        -0.05960787,  0.05962683,  0.05685133],\n",
            "       [ 0.10931217, -0.0762489 , -0.1376228 ,  0.07703073,  0.09215294,\n",
            "         0.13204688,  0.10688408,  0.14205813],\n",
            "       [-0.10745707, -0.05804486, -0.12429571, -0.08470646,  0.14617923,\n",
            "        -0.06040149,  0.08539802,  0.09242864],\n",
            "       [ 0.0166362 ,  0.0112709 , -0.00660858,  0.00249185, -0.03338488,\n",
            "        -0.04912266, -0.00457531, -0.04159953],\n",
            "       [-0.01698687, -0.03746825, -0.01272637, -0.04153359,  0.02364327,\n",
            "        -0.01289871,  0.04172332, -0.02040068],\n",
            "       [ 0.01510055, -0.01989294, -0.00753764,  0.0194471 ,  0.02778578,\n",
            "        -0.03580745, -0.00438815, -0.00738833],\n",
            "       [ 0.08136037, -0.07938047, -0.10438767,  0.10918117, -0.10893015,\n",
            "         0.07247414, -0.06342073,  0.1207428 ],\n",
            "       [ 0.01033301,  0.00350778, -0.0171818 ,  0.04386062, -0.01932318,\n",
            "         0.04446277,  0.0385475 , -0.00931989],\n",
            "       [-0.01464739, -0.00180029,  0.01730946, -0.04577483,  0.02382242,\n",
            "         0.03708467,  0.00188329, -0.03966125],\n",
            "       [ 0.04023136,  0.02459527,  0.00907837,  0.01315198,  0.04567131,\n",
            "         0.02874405, -0.01617128, -0.01109319],\n",
            "       [-0.02977507,  0.02139706,  0.00186957,  0.01791834,  0.00894784,\n",
            "         0.04122653, -0.03935617,  0.04265356],\n",
            "       [-0.02135285, -0.00426116,  0.04024558, -0.01178813,  0.04960049,\n",
            "        -0.01828406, -0.02143255, -0.03571258],\n",
            "       [-0.04034765,  0.00618743, -0.03516948,  0.00280232,  0.02742325,\n",
            "        -0.03113339,  0.01847934,  0.01025729],\n",
            "       [-0.00046166,  0.01020212,  0.02667919, -0.01975745,  0.01335793,\n",
            "         0.0131151 ,  0.00852842,  0.01841596],\n",
            "       [-0.14782608,  0.08766243, -0.10372343,  0.10650612, -0.12792553,\n",
            "        -0.0718106 , -0.10053294, -0.05392171],\n",
            "       [ 0.0225699 , -0.01571887, -0.03575055, -0.01409886,  0.04570072,\n",
            "        -0.0201815 , -0.03145782, -0.0124071 ],\n",
            "       [-0.00920724,  0.03542478, -0.04200239, -0.00488131, -0.04657997,\n",
            "         0.00091873,  0.00453923,  0.03274084],\n",
            "       [-0.14760956, -0.09304588, -0.14819124, -0.12078402,  0.09811702,\n",
            "        -0.06661346,  0.10395604,  0.10184725],\n",
            "       [-0.03379432,  0.02242908, -0.02023163,  0.0476172 ,  0.01018156,\n",
            "         0.0101733 , -0.03601196,  0.03673008],\n",
            "       [-0.09486267, -0.07893033, -0.09789907,  0.06893685, -0.10443884,\n",
            "        -0.07708196,  0.14307283,  0.04352848],\n",
            "       [ 0.06240794, -0.07816311,  0.07161161, -0.12877683,  0.08804813,\n",
            "         0.06551442,  0.05856513,  0.12588711],\n",
            "       [-0.08864138,  0.10459234, -0.08691484,  0.10793619,  0.10224804,\n",
            "        -0.11210673,  0.10654733, -0.05361843],\n",
            "       [-0.04803792,  0.0181796 ,  0.04984308, -0.00219319, -0.00834718,\n",
            "        -0.00299985,  0.01194783,  0.01612731],\n",
            "       [-0.02486729, -0.03019065,  0.00816693,  0.01028023,  0.04019772,\n",
            "         0.02498468, -0.01943016,  0.03106484]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQAiapl6OdFO",
        "colab_type": "text"
      },
      "source": [
        "অ্যাক্যুরেসি কেন ১ এসেছে? কারণ, দশটা বাক্যের মধ্যে সবগুলোই ইউনিক। একটার সাথে আরেকটার মিল নেই। আর, এতো ছোট উদাহরণে আর কি হবে?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPvlpHsNrjLU",
        "colab_type": "code",
        "outputId": "12f12c86-3fd1-4676-ba56-e5cb232b4bd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss, accuracy = model.evaluate(padded_reviews, labels, verbose=0)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVYFsXuIbHK0",
        "colab_type": "text"
      },
      "source": [
        "আরেকটা উদাহরণ দেখি। এসেছে স্ট্যাকওভারফ্লো থেকে। আমরা যা আলাপ করেছি সেখানে ওয়ান হট এনকোডিং একদম চোখে দেখা হয়নি। খালি চোখে হট এনকোডিং এর ভেতরে দেখতে চাই। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plzrmT4xbUrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGF-jsBsbYE2",
        "colab_type": "text"
      },
      "source": [
        "এখানেও আমরা একটা বাক্যকে ক্লাসিফাই করবো। 'ব্যাগ অফ ওয়ার্ড' (একটা ব্যাগে যতো শব্দ আছে) হচ্ছে একটা মেকানিজম যার মাধ্যমে টেক্সট থেকে ফিচার বের করা যায়। এটা আসলে আমাদের এই তিনটা উদাহরন থেকে যতো ইউনিক শব্দ আছে সেটা থেকে সে ট্রেনিং নেয়। তবে, আমরা তার থেকেও ভালো মডেল মানে সিকোয়েন্সের ওপর জোর দেবো।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9yE6vy9cAtl",
        "colab_type": "code",
        "outputId": "7ae21825-6455-40c7-872e-d8e7fe3a21d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_classes = 5 \n",
        "max_words = 20\n",
        "sentences = ['আমি আর আসছি না এখানে!',\n",
        "              'কথা শোনে না ওয়েটার',\n",
        "            'একদম ঠান্ডা খাবার']\n",
        "# লেবেল তৈরি করি\n",
        "labels = np.random.randint(0, num_classes, 3)\n",
        "y = to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "words = set(w for sent in sentences for w in sent.split())\n",
        "word_map = {w : i+1 for (i, w) in enumerate(words)}\n",
        "sent_ints = [[word_map[w] for w in sent.split()] for sent in sentences]\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI3TxDorl5Hw",
        "colab_type": "text"
      },
      "source": [
        "বাক্যের শব্দের সিকোয়েন্স, ৫, ৪, ৩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1u40lNjlztt",
        "colab_type": "code",
        "outputId": "794a519e-4582-4449-8153-978305e162e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(sent_ints)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6, 9, 8, 2, 7], [3, 5, 2, 1], [4, 11, 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFHbTsYHiisK",
        "colab_type": "text"
      },
      "source": [
        "আমাদেরকে প্যাড করতে হবে max_words লেনথ ধরে, যা পাবো len(words) এবং ১ যোগ করে। +১ মানে হচ্ছে আমরা ০কে রিজার্ভ রাখবো যাতে সেটার বাইরে না যায়। এখানে ওয়ান হট এনকোডিং এবং প্যাডিং করছি -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx50nUsufCfN",
        "colab_type": "code",
        "outputId": "a99e750d-4d8a-498f-c848-b276095a8ec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ট্রেনিং ডেটার X এর হিসেব\n",
        "X = np.array([to_categorical((pad_sequences((sent,), \n",
        "    max_words)).reshape(20,),vocab_size + 1) for sent in sent_ints])\n",
        "print(X.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 20, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi9UPNAlna11",
        "colab_type": "text"
      },
      "source": [
        "এখন দেখি আমাদের ওয়ান হট এনকোডিং এর অবস্থা? হাজারো ০। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNGqytPrnOtT",
        "colab_type": "code",
        "outputId": "e4e5430a-7c85-435a-fc0d-e7a4116b6f0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltNmRAeWp-nf",
        "colab_type": "code",
        "outputId": "544b940f-607c-42bc-d137-9c61d36a256d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# y দেখি\n",
        "print(y)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtHeqlNVj7Oy",
        "colab_type": "text"
      },
      "source": [
        "মডেলে ডেন্স লেয়ার যোগ করছি যাতে ওয়ান হট শব্দগুলোকে ডেন্স ভেক্টরে পাল্টে নেয়া যায়। তো LSTM এর কাজ কি? আমাদের বাক্যের ভেতরে শব্দের ভেক্টরকে কনভার্ট করতে হবে ডেন্স বাক্য ভেক্টরে। একদম শেষ লাইনে সফটম্যাক্স এক্টিভেশন ফাংশন ব্যবহার করছি যাতে ক্লাসের ওপর প্রবাবিলিটি ডিস্ট্রিবিউশন চালাতে পারে। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJgHhxWYfkFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words, vocab_size + 1)))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "          optimizer='adam',\n",
        "          metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwB1YH6If5dw",
        "colab_type": "code",
        "outputId": "e59c2d01-62eb-4b16-d058-6806cc34a84b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "model.fit(X,y)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3 samples\n",
            "3/3 [==============================] - 4s 1s/sample - loss: 1.5783 - accuracy: 0.3333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efe930dda90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bXyPHd6Ntlo",
        "colab_type": "text"
      },
      "source": [
        "## শব্দ এবং সংখ্যার কাছাকাছি এনকোডিং\n",
        "\n",
        "শব্দ যখন সংখ্যা হবে, তখন শব্দের মধ্যে যেরকম সম্পর্ক ছিলো, সেটা আরো ভালো বোঝা যাবে সংখ্যায়। শব্দে \"আমি\" \"তুমি\" যেমন কাছাকাছি, সংখ্যায় সেটা আরো পরিস্কার হবে। এমবেডিং এ এটা এমন ধরনের ‘ডেন্স’ রিপ্রেজেন্টেশন যার মধ্যে একই ধরনের শব্দের একই বা কাছাকাছি ধরনের এনকোডিং হবে। ‘পুরুষ’ এবং ‘মহিলা’ এই দুটো অক্ষরের মধ্যে আকাশ পাতাল পার্থক্য হলেও সম্পর্কের কারণে এদুটো কাছাকাছি থাকবে। সংখ্যার এনকোডিং ও একই ধরনের হবে। সবচেয়ে বড় কথা হচ্ছে এ ধরনের এনকোডিং ম্যানুয়ালি বা আমাদের হাতে লিখে দিতে হবে না। বরং যেহেতু এগুলো ট্রেনিংযোগ্য প্যারামিটার ফলে মডেলের ট্রেনিং এর সময় ওয়েটগুলো ‘এডজাস্ট’ হবে এদের সম্পর্কের ভিত্তিতে। একটা ছোট ডাটাসেটের ওয়ার্ড এম্বেডিং ৮ ডাইমেনশনাল হলেও সেটা বড় ডাটাসেটের জন্য ১০২৪ ডাইমেনশন পর্যন্ত যেতে পারে। যত বেশি ডাইমেনশনাল এম্বেডিং ততবেশি শব্দগুলোর মধ্যে সম্পর্কগুলোকে আরো ভালোভাবে বোঝা যাবে তবে তার জন্য প্রচুর ডাটা লাগবে শিখতে।\n",
        "\n",
        "## এমবেডিং এর প্রি-ট্রেইনড মডেল\n",
        "\n",
        "আমরা যখন ‘ওয়ান হট’ এনকোডিং উদাহরণ দেখছিলাম, সেখানে প্রতিটা শব্দ একটা ৪ ডাইমেনশনাল ফ্লোটিং পয়েন্ট সংখ্যা ভেক্টর দিয়ে রিপ্রেজেন্ট করা হয়েছিল। আমরা এ ধরনের এম্বেডিংকে বলতে পারি ‘লুকআপ’ টেবিল। এই ‘লুকআপ’ টেবিলের ওয়েটগুলো শিখছে যখন আমরা ‘ডেন্স’ ভেক্টরের তার করেসপন্ডিং টেবিল দেখছি। ফলে সেটা ধরে আমরা প্রতিটা শব্দ ধরে এনকোডিং করছি। তবে, এখানে একটা বড় সমস্যা আছে। আমরা যখন এনকোডিং করবো তখন সব বাংলা শব্দকে একসাথে এনকোডিং না করলে আমরা কিভাবে একটা শব্দকে আরেকটা শব্দের সাথে সিমিলারিটি, শব্দগুলোর মধ্যে দূরত্ব বের করবো? তাদের মধ্যে সিমান্টিক এনালিসিস কে করবে? সব ডাটা একসাথে না হলে আমাদের কাজ হবে না। সে সমস্যা মেটাতে এসেছে প্রি-ট্রেইনড মডেল। কোটি কোটি শব্দকে একসাথে ট্রেইন করিয়ে তারপর এনকোডিং করেছে একসাথে। নিজে বানানো কঠিন। সেটার জন্য যে রিসোর্স দরকার সেটা নেই আমাদের কাছে। ফলে এধরনের প্রি-ট্রেইনড মডেলগুলো অসাধারণ পারদর্শী, কারণ সে ট্রেনিং এর সময় সব শব্দ হাতের কাছে পেয়েছে। শুধু নামিয়ে নিতে হবে দরকারের সময়ে। তাদের শক্তি এবং দুর্বলতা নিয়ে আলাপ করবো না এই বইয়ে।\n",
        "\n",
        "এই প্রি-ট্রেইনড মডেল হিসেবে আমার প্রিয় ফেইসবুকের ফাস্টটেক্সট। ইংরেজি ছাড়াও আরো ১৫৭টা ভাষায় এর প্রি-ট্রেইনড মডেল আছে। বাংলা তো অবশ্যই। পুরোনো লাইব্রেরি হিসেবে ওয়ার্ড২ভেক এখনো জনপ্রিয়। পাশাপাশি পাইথনের জন্য স্পেসি আমার পছন্দের। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPhkHnJ2Lm4o",
        "colab_type": "text"
      },
      "source": [
        "ধারণাগুলো এসেছে টেন্সর-ফ্লো ডকুমেন্ট, জেফ হিটনের নোটবুক এবং স্ট্যাকওভারফ্লো থেকে।\n",
        "\n",
        "https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_11_03_embedding.ipynb"
      ]
    }
  ]
}