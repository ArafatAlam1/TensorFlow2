{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3.6 (tensorflow)",
      "language": "python",
      "name": "rga"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "embedding_bangla_v1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raqueeb/TensorFlow2/blob/master/embedding_bangla_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi3C6LhrrjKx",
        "colab_type": "text"
      },
      "source": [
        "## এমবেডিং, ওয়ার্ড এমবেডিং বাংলায় টেক্সট অ্যানালাইসিস\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4TNlkGyLz4C",
        "colab_type": "text"
      },
      "source": [
        "একটা জিনিস ভেবে দেখেছেন কি? আমরা এ পর্যন্ত মেশিন লার্নিং মডেলের ইনপুট হিসেবে যা দিয়েছি তা সবই সংখ্যা। মনে করে দেখুন, এপর্যন্ত সব মডেলের ক্লাসিফিকেশন অথবা রিগ্রেশন এর জন্য যা দিয়েছি সব সংখ্যায় দিয়েছি। তার পাশাপাশি ইমেজ নিয়ে যখন কাজ করেছি তখনো কিন্তু ইমেজ (সেটা গ্রেস্কেল হোক আর কালার হোক - তার জন্য গ্রেস্কেল ইনটেনসিটি অথবা কালারের আরজিবি চ্যানেলের আউটপুট), সবকিছুই সংখ্যায় গিয়েছে। এর অর্থ হচ্ছে মেশিন লার্নিং/ডিপ লার্নিং মডেল সংখ্যা ছাড়া আর কিছু বোঝেনা। আর বুঝবেই বা কিভাবে? সেতো যন্ত্র। আর মানুষের সবকিছুই কমপ্লেক্স। \n",
        "\n",
        "সেদিক থেকে মানুষের ভাষা আরো অনেক কমপ্লেক্স। আমরা একেকজন একেক ভাষায় কথা বলি, ভাষাগুলোর মধ্যে সংযোগ/সিমিলারিটি এবং কি বলতে গিয়ে কি বলে ফেললাম এবং তার ফলাফল, তার পাশাপাশি অনেক শব্দ একটা ভাষায় যা বোঝায় সেটা অন্য ভাষায় তার বৈপরীত্য দেখায়। এখন আপনি বাংলায় কথা বললেও সেটার মধ্যে ৪০% বাইরের শব্দ ব্যবহার করলে তো আরো সমস্যা। আর এই কারণে টেক্সট নিয়ে কাজ করা বেশ কমপ্লেক্স। \n",
        "\n",
        "যেকোনো ল্যাঙ্গুয়েজে তার প্রতিটা শব্দের একটা অর্থ আছে। তবে এটার অর্থ অনেক সময় নির্ভর করে কনটেক্সটে বা শব্দটা বাক্যের মধ্যে কোথায় এই মুহূর্তে আছে। একই শব্দের আবার অনেকগুলো কনটেক্সচুয়াল অর্থ থাকে সে কারণে শব্দকে শুধুমাত্র শব্দ বা অক্ষর লেভেলে কাজ করলে হবে না। কারণ, ডিপ লার্নিং মডেল যেহেতু সংখ্যা ছাড়া কিছু বোঝেনা, সে কারণে একেক ভাষার একেক বুলি এবং বকাবকি এর পাশাপাশি সেই ভাষাগুলোকে ঠিকমতো সংখ্যায় ট্রান্সফার করা একটা চ্যালেঞ্জ এর কাজ অবশ্যই।\n",
        "\n",
        "এই বইতে আমি ইচ্ছে করে ‘টাইম সিরিজ’ যোগ করিনি, কারণ সেটার এপ্লিকেশন লেভেল এখনো বেসিক লেভেলে নেই। তবে, এই রিকারেন্ট নিউরাল নেটওয়ার্ককে শিখিয়ে দিলে সে (আরএনএন) ফ্রি ফর্ম (ইচ্ছেমতো) টেক্সট জেনারেট করতে পারে। আমরা যেমন দেখেছি ‘এল এস টি এম’, (লঙ শর্ট টার্ম মেমোরি) নেটওয়ার্কে ‘শেক্সপিয়ার’ ক্লাসিক পড়তে দিলে, সে শেক্সপিয়ারের মতো আরেকটা ক্লাসিক লিখে ফেলেছে, যেখানে এই নেটওয়ার্কের মধ্যে শব্দ, বাক্য এবং ব্যাকরণ তৈরির কোন ধারণা নেই। কারণ, ডিপ লার্নিং প্রচুর ক্লাসিক বই পড়ে বুঝেছে কিভাবে শব্দ, বাক্য বা তার ব্যাকরণ ব্যবহার করতে হয় এর ভেতরে না ঢুকেই। টাইম সিরিজের ব্যাপারটা হচ্ছে সে পরের জিনিসটা প্রেডিক্ট করবে। আগের সময়ে কি ছিলো, সেটাকে ধরে এরপরে কি কি আসতে পারে সেটাই বলবে সে। না বুঝে। যদি শব্দ লেভেলে দেখি, ‘আমি’ এর পর কি আসতে পারে তাহলে ‘ভালো’ আসতে পারে, কারণ ‘আমি ভালো আছি’ একটা বহুল প্রচলিত বাক্য। \n",
        "\n",
        "বড় ব্যাপার হচ্ছে এই অক্ষর থেকেই ‘এল এস টি এম’ আস্তে আস্তে মানবিক ব্যাকরন এবং কিভাবে একটা বাক্য তৈরি করতে হয় সে ধরনের একটা ধারণা পেয়ে থাকে। এটা সে কোন কিছু বুঝে করে না। প্যাটার্ন থেকে করে। আর সে কারণেই এটার উপরে আমরা খুব একটা ভরসা করব না। আমরা চাইব মেশিনকে শেখাতে, যেভাবে মানুষ ভাষা, শব্দ, ব্যাকরণ শেখে। ওই একই কারণে অক্ষর লেভেলে টেক্সট জেনারেশন নিয়ে আমরা এই মুহূর্তে আলাপ করব না। ইন্টারনেটে দেখতে পারেন কিভাবে একেকটা ‘এল এস টি এম’ নেটওয়ার্ক শেক্সপিয়ারের মত বড় বড় নাটক লিখে ফেলছে। এটা আসলে সে একটা ক্লাসিক্যাল লেখার প্যাটার্ন দেখে তার পার্সপেক্টিভ থেকে লিখেছে। বুঝে লেখেনি। \n",
        "\n",
        "সেজন্য এর মধ্যে এসে যোগ হয়েছে ‘ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং’। ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং এর মধ্যে লিঙ্গুইস্টিকস, কম্পিউটার সায়েন্স, ইনফরমেশন ইঞ্জিনিয়ারিং এবং কৃত্রিম বুদ্ধিমত্তা ব্যাপারগুলো চলে এসেছে কাজের স্বার্থে। এমনিতেই মানুষ এবং যন্ত্রের মধ্যে একটা যোগসূত্র স্থাপন করা বেশ ঝামেলার ব্যাপার। ‘স্পিচ রিকগনিশন’ এর পাশাপাশি স্বয়ংক্রিয়ভাবে একটা ভাষা বুঝতে পারা এবং তার পাশাপাশি সেই ভাষায় বুঝে টেক্সট জেনারেট করা সহজ ব্যাপার নয়, যখন সবকিছুর পেছনে কাজ করে সংখ্যা। সবচেয়ে বড় কথা হচ্ছে আপনি একটা যন্ত্রকে শেখাচ্ছেন সে আপনার সাথে ন্যাচারালি যোগসুত্র স্থাপন করতে পারে। এই ‘ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং’ এর শুরুটা হচ্ছে সেই টেক্সটকে ঠিকমতো পড়তে পারা, সেটাকে নিজের মতো করে বোঝা এবং মানুষ যেভাবে একটা বাক্যের ‘ইনটেন্ট’ বুঝতে পারে সেভাবে তাকে বুঝিয়ে তার কাছ থেকে উত্তর বের করা। সহজ ব্যাপার নয়। তবে শুরু করতে হবে কোথাও। \n",
        "\n",
        "যেহেতু ‘ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং’ একটা বিশাল সাবজেক্ট, আমি এই বইতে ব্যাপারটা আনার ব্যর্থ চেষ্টা করব না। তবে কম্পিউটার কিভাবে ‘টেক্সট’ এর সাথে ‘ইন্টারঅ্যাক্ট’ করে সেটা নিয়েই দেখব আমরা সামনের চ্যাপ্টারগুলোতে। আমরা জানি মেশিন লার্নিং মডেল ভেক্টরকে ইনপুট হিসেবে নেয়। আর এই ভেক্টরগুলো হচ্ছে সংখ্যার অ্যারে। যখন আমরা টেক্সট মানে শব্দ এবং বাক্য নিয়ে আলাপ করব, তখন আমাদের প্রথম কাজ হবে এই স্ট্রিংগুলোকে কিভাবে সংখ্যায় পাল্টানো যায়। অর্থাৎ সেই বাক্য বা শব্দটি কিভাবে ‘ভেক্টরাইজ’ করা যায় মডেলে দেবার আগে। আমাদের এই চ্যাপ্টার থেকে শুরু করব কিভাবে আস্তে আস্তে ডিপ নিউরাল নেটওয়ার্ক দিয়ে বাংলার একটা অ্যাপ্লিকেশনে যাওয়া যায়।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaDQqrJg7p68",
        "colab_type": "code",
        "outputId": "4e132df9-eb5c-4e47-ff0b-a99fe4ebe2f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  # শুধুমাত্র টেন্সর-ফ্লো ২.x ব্যবহার করবো \n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "keras = tf.keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0un3M2_MnHe",
        "colab_type": "text"
      },
      "source": [
        "আমরা যেহেতু ডিপ নিউরাল নেটওয়ার্কের কথা বলছি সেখানে সবকিছুতেই লেয়ার কনসেপ্ট কাজ করে। আজকে এখানে আমরা নতুন একটা লেয়ার নিয়ে কথা বলবো যেটাকে আমরা বলছি এম্বেডিং লেয়ার। শুরুতেই অনেকে বলবেন এম্বেডিং মানে কি? এর সাথে সংখ্যার সম্পর্ক কি?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4bim_ShMw8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_layer = layers.Embedding(1000, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRmBcA6iMx71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = embedding_layer(tf.constant([1,2,3]))\n",
        "result.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYLOnO5Uxmmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# অথবা টেন্সর-ফ্লো দিয়ে টোকেনাইজ করে দেখি\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mezzU-TxprR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "text = ['আমি এখন বই পড়ি']\n",
        "# text = tokenizer.fit_on_texts(text)\n",
        "sequences = tokenizer.texts_to_sequences(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVVdIpcqxrxB",
        "colab_type": "code",
        "outputId": "c8c469cd-a636-4102-9a25-5e79a567cab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sequences"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHqel-xSNGyu",
        "colab_type": "text"
      },
      "source": [
        "মেশিন লার্নিং এর ভাষায় ‘এম্বেডিং’ হচ্ছে সিমিলারিটি, যদি শব্দের কথা বলি তাহলে একটা শব্দ থেকে আরেকটা শব্দ কতটুকু যুক্ত বা একটা শব্দ থেকে আরেকটা শব্দ কত দূরে? তাদের মধ্যে সম্পর্ক আছে কিনা? যেমন, রাজা’র সাথে ‘রানী’ শব্দটা কিন্তু সম্পর্কযুক্ত। যেমন, ‘মা’ শব্দের সাথে ‘বাবা’ সম্পর্কযুক্ত। ‘বাংলাদেশ’ শব্দের সাথে ‘ঢাকা’ সম্পর্কযুক্ত। সেই থেকে ‘আকাশ’ শব্দের সাথে ‘টেবিল’ কিন্তু বহু দূরে মানে তাদের মধ্যে সম্পর্ক টানা বেশ কঠিন, যদি না কোনদিন ‘আকাশ’ থেকে ‘টেবিল’ পড়ে।\n",
        "\n",
        "এই এম্বেডিং ব্যাপারটা এসেছে কিছুটা ‘ওয়ান হট’ এনকোডিং থেকে। তবে, ব্যাপারটা ঠিক সেরকম নয়। শব্দকে যেহেতু আমাদেরকে সংখ্যায় পাল্টাতে হবে তাহলে আর বাকি উপায় কি? মনে আছে, আমাদের এই ‘ওয়ান হট’ এনকোডিং বা ‘ডামি ভেরিয়েবল’ ব্যাপারটা এসেছে শব্দ দিয়ে তৈরি ক্যাটাগরিকাল ভেরিয়েবল থেকে? মেশিন লার্নিং মডেলে আমরা যখন কিছু শব্দকে সংখ্যার ক্যাটাগরিতে ভাগ করতে চাই, যেটা এমুহুর্তে সংখ্যায় নেই। যেমন, আইরিশ ডাটাসেটের তিন প্রজাতির ফুলের জন্য ০,১,২, ভাগে ভাগ করতে পারছিনা, সেখানে চলে এসছে এই ডামি ভেরিয়েবল। আমরা যাকে বলছি শব্দকে দিয়ে তার জন্য একটা করে এনকোডিং। এর অর্থ হচ্ছে, আমাদের ভাষায় যতগুলো শব্দ আছে সেগুলোকে ‘ওয়ান হট’ এনকোডিং করা যেতে পারে। আর সেখানেই সমস্যা। \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/raqueeb/deep_learning_book/master/assets/hot.png\"> চিত্রঃ ‘ওয়ান হট’ এনকোডিং এর উদাহরণ\n",
        "\n",
        "এখানে একটা উদাহরণ দেয়া যাক। একটা বাক্য। ‘আমি এখন বই পড়ি’। এই চারটা শব্দকে যদি আমরা সংখ্যায় পাল্টাতে চাই, তাহলে আমাদের এই ভোকাবুলারির প্রতিটা শব্দকে ‘০’ ভেক্টর দিয়ে শুরু করব। আমাদের এখানে যেহেতু ৪টা ইউনিক শব্দ, সে কারণে এই শূন্য ভেক্টরের দৈর্ঘ্য হবে ৪। প্রতিটি শব্দকে ঠিকমতো রিপ্রেজেন্ট করার জন্য একেকটা শব্দের ইনডেক্সে তার করেসপন্ডিং ‘১’ বসাবো। ছবি দেখি। এখন এই টেবিল থেকে প্রতিটা শব্দের জন্য তার ভেক্টর বের করা সোজা। আমাদের এই চারটা শব্দের জন্য ভেক্টরের দৈর্ঘ্য হচ্ছে ৪ যার বাকি তিনটাই ০। এভাবে আমরা শব্দকে বিভিন্ন ক্যাটাগরিতে ভাগ করতে পারি। তবে ব্যাপারটা সেরকম এফিশিয়েন্ট নয় যখন আমাদের ভোকাবুলারিতে ১০ হাজার শব্দ থাকবে। তার মানে একেকটা ভেক্টরের দৈর্ঘ্য ১০ হাজার হবে - এর মধ্যে ৯৯.৯৯ শতাংশই হচ্ছে ০। এটা দক্ষ সিস্টেম না। \n",
        "\n",
        "এই সমস্যা থেকে বের হবার উপায় কি? প্রতিটা শব্দকে একটা করে ইউনিক সংখ্যা দিয়ে দেওয়া। আমাদের আগের ভোকাবুলারিটাকে ‘আমি এখন বই পড়ি’কে আমরা একটা ‘ডেন্স’ ভেক্টরের মত আলাদা আলাদা করে সংখ্যা দিয়ে দিতে পারি। এই জিনিসটা আগের থেকে অনেকটাই এফিশিয়েন্ট। একেকটা শব্দের জন্য একেকটা আলাদা আলাদা সংখ্যা। সবচেয়ে বড় ব্যাপার হচ্ছে বিশাল ‘স্পার্স’ হাই-ডাইমেনশন স্পেস থেকে কম ডাইমেনশন স্পেসে চলে এলাম। তবে, এটার সমস্যা দুটো।\n",
        "\n",
        "* ১. আমাদের এই সংখ্যায় এনকোডিং সিস্টেমটাকে ‘ম্যানুয়ালি’ করা হয়েছে। ফলে এদের মধ্যে কোন ‘রিলেশনশিপ’ বের করা যাচ্ছে না। অংকেও এক সংখ্যা থেকে আরেক সংখ্যার মধ্যে যে রিলেশনশিপ সেটা অনুপস্থিত। ফলে আমাদের অংকের ভাষায় কো-সাইন ভেক্টরে কে কোথায় আছে সেটা বের করা মুশকিল।\n",
        "\n",
        "* ২. একটা মডেলের জন্য এই ধরনের ‘ম্যানুয়াল’ এনকোডিং শব্দগুলোর মধ্যে সম্পর্ক না বুঝলে সেটা সমস্যা হয়ে দাঁড়াবে। যেহেতু এই একেকটা ফিচারের জন্য একেকটা ‘ওয়েট’ সে কারণে একটা লিনিয়ার ক্লাসিফায়ারের পক্ষে এই ফিচার এবং ওয়েট এর কম্বিনেশন কোন সম্পর্ক দেখাবেনা। এটা বড় সমস্যা। \n",
        "\n",
        "আর সে কারণেই চলে এসেছে ওয়ার্ড এম্বেডিং।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEOvo2pjrjKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Embedding, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ecoq69LrjK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define 10 resturant reviews.\n",
        "reviews = [\n",
        "    'আমি আর আসছি না এখানে!',\n",
        "    'একদম বাজে সার্ভিস',\n",
        "    'কথা শোনে না ওয়েটার',\n",
        "    'একদম ঠান্ডা খাবার',\n",
        "    'বাজে খাবার!',\n",
        "    'অসাধারণ',\n",
        "    'অসাধারণ সার্ভিস!',\n",
        "    'খুব ভালো!',\n",
        "    'মোটামুটি',\n",
        "    'এর থেকে ভালো হয়না']\n",
        "\n",
        "# Define labels (1=negative, 0=positive)\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CxmAlaErjK3",
        "colab_type": "code",
        "outputId": "da0f1f6d-2686-4688-c73c-b0962c863fc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(reviews[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "আমি আর আসছি না এখানে!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZk5g6PrjK5",
        "colab_type": "code",
        "outputId": "638a81c3-3b32-414c-a9dc-261351abf154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# ফিরে আসি আগের উদাহরণে \n",
        "\n",
        "VOCAB_SIZE = 50\n",
        "encoded_reviews = [one_hot(d, VOCAB_SIZE) for d in reviews]\n",
        "print(f\"Encoded reviews: {encoded_reviews}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded reviews: [[23, 6, 33, 17, 25], [6, 10, 45], [36, 43, 17, 13], [6, 28, 16], [10, 16], [29], [29, 45], [47, 1], [25], [9, 5, 1, 46]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFLBnh2erjK8",
        "colab_type": "code",
        "outputId": "b23882f2-bb7b-4b70-87fa-18266deb2988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "MAX_LENGTH = 4\n",
        "\n",
        "padded_reviews = pad_sequences(encoded_reviews, maxlen=MAX_LENGTH, padding='post')\n",
        "print(padded_reviews)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 6 33 17 25]\n",
            " [ 6 10 45  0]\n",
            " [36 43 17 13]\n",
            " [ 6 28 16  0]\n",
            " [10 16  0  0]\n",
            " [29  0  0  0]\n",
            " [29 45  0  0]\n",
            " [47  1  0  0]\n",
            " [25  0  0  0]\n",
            " [ 9  5  1 46]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBM4Eu7OrjLJ",
        "colab_type": "code",
        "outputId": "b5efd6d7-965b-4edf-bef5-502bab0121c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "model = Sequential()\n",
        "embedding_layer = Embedding(VOCAB_SIZE, 8, input_length=MAX_LENGTH)\n",
        "model.add(embedding_layer)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 4, 8)              400       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVDOD7q7rjLO",
        "colab_type": "code",
        "outputId": "8f22d6cf-36c3-4262-e9f9-d83d01934a09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# fit the model\n",
        "model.fit(padded_reviews, labels, epochs=100, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fab68933b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnaII1xnrjLS",
        "colab_type": "code",
        "outputId": "5b3559c0-78e5-4cbd-fd0d-ae64f639dc22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(embedding_layer.get_weights()[0].shape)\n",
        "print(embedding_layer.get_weights())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 8)\n",
            "[array([[-3.12827975e-02,  4.86732945e-02,  8.46145451e-02,\n",
            "        -5.70723526e-02, -1.34908348e-01, -5.42748198e-02,\n",
            "         1.45759553e-01,  1.47508457e-01],\n",
            "       [-1.48706838e-01,  1.17172904e-01,  1.29112393e-01,\n",
            "        -1.00727588e-01,  9.15443078e-02, -9.17942896e-02,\n",
            "         6.99876472e-02,  1.34916037e-01],\n",
            "       [-3.20193172e-02,  2.68795751e-02, -3.82953882e-02,\n",
            "         3.66718657e-02, -7.72655010e-03, -8.14471394e-03,\n",
            "         2.41601206e-02,  1.09758005e-02],\n",
            "       [ 4.45945971e-02,  4.41175140e-02, -3.72553244e-02,\n",
            "         4.93328609e-02,  1.09423324e-03, -3.85251641e-02,\n",
            "        -2.79844403e-02, -4.79998589e-02],\n",
            "       [-3.53379846e-02, -2.19841134e-02,  3.79212387e-02,\n",
            "         4.84202765e-02,  2.16694586e-02, -1.90777425e-02,\n",
            "         1.50343031e-03,  1.26581267e-03],\n",
            "       [-1.20961986e-01,  1.07883282e-01,  1.25951886e-01,\n",
            "         1.08194232e-01,  1.12215348e-01,  8.30505267e-02,\n",
            "         1.06055588e-01,  7.65256658e-02],\n",
            "       [-9.78901833e-02, -8.30083489e-02,  1.54981136e-01,\n",
            "        -8.48533362e-02, -5.65595292e-02, -1.30571783e-01,\n",
            "         1.40502363e-01, -9.89027694e-02],\n",
            "       [-9.50986147e-03,  3.15503143e-02,  3.67205404e-02,\n",
            "        -4.64918725e-02, -2.80485991e-02, -1.07221827e-02,\n",
            "        -2.03371178e-02,  8.44855234e-03],\n",
            "       [-3.80005129e-02, -2.29729656e-02,  1.86263397e-03,\n",
            "        -2.47452743e-02,  3.85826342e-02, -1.43837929e-02,\n",
            "         2.23647989e-02,  3.44700702e-02],\n",
            "       [ 1.43978998e-01,  7.77649954e-02, -7.67953843e-02,\n",
            "         7.43022487e-02,  1.32804230e-01,  8.38480890e-02,\n",
            "        -1.10272393e-01,  1.38938293e-01],\n",
            "       [-8.63388032e-02, -7.35940710e-02, -8.41129795e-02,\n",
            "        -1.23037346e-01, -1.15046985e-01, -8.81355852e-02,\n",
            "         1.35665491e-01, -8.51980597e-02],\n",
            "       [ 2.91255824e-02, -4.31005247e-02,  2.75298990e-02,\n",
            "         1.02104992e-03,  1.14730746e-03,  1.14784241e-02,\n",
            "        -1.53874867e-02, -4.92824428e-02],\n",
            "       [-4.77080718e-02, -3.50533240e-02,  3.30642797e-02,\n",
            "        -2.97150500e-02,  2.02489607e-02,  4.98612262e-02,\n",
            "        -3.38231474e-02, -2.59610415e-02],\n",
            "       [-7.76748657e-02, -5.36021590e-02,  1.40253142e-01,\n",
            "        -2.57928576e-02,  6.59959838e-02,  9.07000303e-02,\n",
            "         4.41062376e-02, -1.42669469e-01],\n",
            "       [ 4.79746796e-02,  4.95952629e-02, -4.02512550e-02,\n",
            "        -1.35450438e-03, -3.20835710e-02, -3.87439951e-02,\n",
            "         1.43563785e-02, -1.85356364e-02],\n",
            "       [-3.67409810e-02,  1.81857683e-02, -3.08886524e-02,\n",
            "        -4.93101850e-02,  3.69660892e-02,  2.28514522e-03,\n",
            "         2.76108496e-02, -3.14195901e-02],\n",
            "       [ 1.63777307e-01, -1.02899902e-01, -1.41749725e-01,\n",
            "         8.28228369e-02, -1.28003418e-01,  6.03478067e-02,\n",
            "        -1.18106864e-01, -6.52204901e-02],\n",
            "       [-2.37972420e-02, -1.39089793e-01,  4.01246585e-02,\n",
            "         9.65172723e-02,  5.77870943e-02,  8.05139095e-02,\n",
            "        -5.94455786e-02, -1.11659020e-01],\n",
            "       [-1.65620446e-02,  3.19216736e-02,  3.70798260e-03,\n",
            "        -1.79175958e-02,  3.69175710e-02, -1.39384381e-02,\n",
            "         4.93459590e-02,  3.16854753e-02],\n",
            "       [ 4.15650867e-02, -3.40788588e-02, -2.62124464e-03,\n",
            "        -4.34845574e-02,  5.09727001e-03,  2.13312842e-02,\n",
            "         4.16586511e-02, -6.53807074e-03],\n",
            "       [-9.56041738e-03, -2.59694699e-02,  3.22527625e-02,\n",
            "         3.99740599e-02,  1.79673173e-02, -4.47652228e-02,\n",
            "        -8.57801363e-03,  2.78905146e-02],\n",
            "       [ 1.33128278e-02,  2.26833113e-02, -4.69481610e-02,\n",
            "        -4.21058424e-02, -4.32778262e-02,  2.35590376e-02,\n",
            "        -3.90113108e-02,  4.42229025e-02],\n",
            "       [-3.77410650e-02, -4.58985828e-02,  2.13272907e-02,\n",
            "         4.24480475e-02,  1.62667073e-02,  4.26698811e-02,\n",
            "        -4.86984514e-02,  3.99917997e-02],\n",
            "       [-3.94610278e-02, -3.20037454e-03, -4.67751995e-02,\n",
            "        -2.36420985e-02, -2.09311247e-02, -6.02340698e-03,\n",
            "         1.02162138e-02, -1.84900872e-02],\n",
            "       [ 4.22659405e-02,  4.49827202e-02, -2.96708234e-02,\n",
            "         4.10751440e-02,  1.37511156e-02, -1.27643831e-02,\n",
            "         4.17039283e-02,  2.42638588e-03],\n",
            "       [-1.31950513e-01,  8.52100700e-02,  6.01514429e-02,\n",
            "         1.46328717e-01,  1.47546038e-01,  1.28255948e-01,\n",
            "        -7.63836205e-02,  1.60672754e-01],\n",
            "       [-1.59944072e-02,  2.94095613e-02, -4.35983650e-02,\n",
            "         9.52556729e-06,  1.60083883e-02, -2.56452560e-02,\n",
            "        -1.42543539e-02, -1.59958117e-02],\n",
            "       [ 4.63161506e-02, -1.19306557e-02, -4.46849838e-02,\n",
            "         1.68263912e-03,  2.49470733e-02,  1.52162574e-02,\n",
            "        -9.37336683e-03,  5.93433529e-03],\n",
            "       [ 8.98637027e-02, -9.55684483e-02, -5.36873266e-02,\n",
            "        -1.43766373e-01, -1.48407176e-01, -8.86933953e-02,\n",
            "        -1.51109219e-01, -1.04635485e-01],\n",
            "       [ 1.12091467e-01,  1.24842808e-01, -1.45467177e-01,\n",
            "         1.47904903e-01,  5.22453822e-02,  1.41335443e-01,\n",
            "        -8.61590579e-02,  1.43122092e-01],\n",
            "       [ 1.84568018e-03, -3.72439846e-02, -4.52964380e-03,\n",
            "        -2.54258998e-02, -2.16805935e-03, -4.12435643e-02,\n",
            "        -3.11765671e-02, -4.04833779e-02],\n",
            "       [ 2.64458694e-02, -2.93187853e-02,  1.17771253e-02,\n",
            "         9.38232988e-03,  9.84104723e-03, -3.64797823e-02,\n",
            "         3.02903168e-02, -3.14683206e-02],\n",
            "       [ 3.72844003e-02,  4.45021056e-02,  4.30976190e-02,\n",
            "        -2.36887820e-02, -2.07202081e-02,  2.40571424e-03,\n",
            "        -4.30698171e-02, -1.91685315e-02],\n",
            "       [ 8.00724700e-02, -6.30359873e-02, -7.82158896e-02,\n",
            "        -1.47597238e-01, -1.35095820e-01, -1.61058292e-01,\n",
            "        -1.33117676e-01, -1.19129837e-01],\n",
            "       [-2.74040587e-02, -7.40329176e-03, -3.38975191e-02,\n",
            "         2.93814801e-02,  3.22967432e-02,  2.37963684e-02,\n",
            "        -3.41804996e-02, -6.20305538e-04],\n",
            "       [-3.23768482e-02,  4.27284129e-02,  3.89952324e-02,\n",
            "        -9.87530872e-03, -3.60295549e-02,  4.28140424e-02,\n",
            "        -3.57913747e-02,  4.84260172e-03],\n",
            "       [-1.37737691e-01, -1.33016035e-01,  8.52675140e-02,\n",
            "        -8.50796476e-02, -1.15624540e-01, -6.89688846e-02,\n",
            "         7.71770701e-02, -1.17766321e-01],\n",
            "       [ 4.07333709e-02,  2.05680244e-02,  1.40895359e-02,\n",
            "        -3.23412307e-02,  3.19074057e-02, -4.08536680e-02,\n",
            "        -3.81762385e-02,  4.65225428e-04],\n",
            "       [-3.08622364e-02, -3.90039906e-02, -1.73445940e-02,\n",
            "         4.61107492e-03,  1.65609233e-02,  4.39327620e-02,\n",
            "        -3.89082059e-02,  4.82879616e-02],\n",
            "       [-4.69980724e-02,  1.61854066e-02, -3.21857817e-02,\n",
            "         9.51193646e-03, -1.10671893e-02, -3.92886996e-02,\n",
            "        -4.97408956e-03,  1.65278949e-02],\n",
            "       [ 4.30606678e-03,  3.43972556e-02, -1.24862567e-02,\n",
            "         4.54606451e-02, -1.24514103e-04,  1.49134658e-02,\n",
            "         2.30993144e-02,  2.14537233e-03],\n",
            "       [ 2.04539187e-02, -6.21373579e-03, -5.07320091e-03,\n",
            "         1.36725977e-03,  2.24842168e-02,  4.56577875e-02,\n",
            "         4.65198271e-02, -3.66994515e-02],\n",
            "       [ 1.93170197e-02,  4.43146862e-02,  3.83126028e-02,\n",
            "        -9.15467739e-03,  2.57100798e-02,  4.06074524e-03,\n",
            "         3.04622911e-02, -4.34098728e-02],\n",
            "       [ 8.38258490e-02, -1.03696525e-01, -1.06046505e-01,\n",
            "        -8.21101069e-02, -1.11314654e-01, -1.29228905e-01,\n",
            "        -8.63557756e-02, -1.43551782e-01],\n",
            "       [-2.18651425e-02, -1.47690065e-02, -8.54279846e-03,\n",
            "        -9.05547291e-03, -9.01255757e-03,  1.26257278e-02,\n",
            "         3.90040018e-02, -1.25760958e-03],\n",
            "       [-1.22810960e-01, -1.59005195e-01,  8.84884596e-02,\n",
            "         9.17383954e-02,  6.50060400e-02,  1.23293690e-01,\n",
            "        -6.89242259e-02,  8.38046968e-02],\n",
            "       [ 1.32015422e-01,  5.46120256e-02, -1.12986706e-01,\n",
            "        -5.53525193e-03, -1.07937127e-01, -3.72328535e-02,\n",
            "        -1.07936136e-01,  1.10016771e-01],\n",
            "       [ 1.01942174e-01,  1.42384782e-01, -6.43068254e-02,\n",
            "         5.83766550e-02,  8.77915174e-02,  7.23352507e-02,\n",
            "        -9.87383053e-02,  6.84990510e-02],\n",
            "       [ 1.83918811e-02, -4.17179950e-02, -1.91044565e-02,\n",
            "         3.19568850e-02,  8.82472843e-03, -3.99077311e-02,\n",
            "        -3.76461037e-02,  2.83377208e-02],\n",
            "       [-2.98118126e-02, -2.25805119e-03, -2.99986489e-02,\n",
            "        -2.04653274e-02, -1.35057047e-03,  1.19385943e-02,\n",
            "        -4.86904383e-03, -4.85749505e-02]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPvlpHsNrjLU",
        "colab_type": "code",
        "outputId": "f9c5c22f-6588-4d65-e357-e5b67c7aab7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss, accuracy = model.evaluate(padded_reviews, labels, verbose=0)\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bXyPHd6Ntlo",
        "colab_type": "text"
      },
      "source": [
        "এটা এমন ধরনের ‘ডেন্স’ রিপ্রেজেন্টেশন যার মধ্যে একই ধরনের শব্দের একই বা কাছাকাছি ধরনের এনকোডিং হবে। ‘পুরুষ’ এবং ‘মহিলা’ এই দুটো অক্ষরের মধ্যে আকাশ পাতাল পার্থক্য হলেও সম্পর্কের কারণে এদুটো কাছাকাছি থাকবে। সংখ্যার এনকোডিং ও একই ধরনের হবে। সবচেয়ে বড় কথা হচ্ছে এ ধরনের এনকোডিং ম্যানুয়ালি বা আমাদের হাতে লিখে দিতে হবে না। বরং যেহেতু এগুলো ট্রেনিংযোগ্য প্যারামিটার ফলে মডেলের ট্রেনিং এর সময় ওয়েটগুলো ‘এডজাস্ট’ হবে এদের সম্পর্কের ভিত্তিতে। একটা ছোট ডাটাসেটের ওয়ার্ড এম্বেডিং ৮ ডাইমেনশনাল হলেও সেটা বড় ডাটাসেটের জন্য ১০২৪ ডাইমেনশন পর্যন্ত যেতে পারে। যত বেশি ডাইমেনশনাল এম্বেডিং ততবেশি শব্দগুলোর মধ্যে সম্পর্কগুলোকে আরো ভালোভাবে বোঝা যাবে তবে তার জন্য প্রচুর ডাটা লাগবে শিখতে।\n",
        "\n",
        "আমরা যখন ‘ওয়ান হট’ এনকোডিং উদাহরণ দেখছিলাম, সেখানে প্রতিটা শব্দ একটা ৪ ডাইমেনশনাল ফ্লোটিং পয়েন্ট সংখ্যা ভেক্টর দিয়ে রিপ্রেজেন্ট করা হয়েছিল। আমরা এ ধরনের এম্বেডিংকে বলতে পারি ‘লুকআপ’ টেবিল। এই ‘লুকআপ’ টেবিলের ওয়েটগুলো শিখছে যখন আমরা ‘ডেন্স’ ভেক্টরের তার করেসপন্ডিং টেবিল দেখছি। ফলে সেটা ধরে আমরা প্রতিটা শব্দ ধরে এনকোডিং করছি। তবে, এখানে একটা বড় সমস্যা আছে। আমরা যখন এনকোডিং করবো তখন সব বাংলা শব্দকে একসাথে এনকোডিং না করলে আমরা কিভাবে একটা শব্দকে আরেকটা শব্দের সাথে সিমিলারিটি, শব্দগুলোর মধ্যে দূরত্ব বের করবো? সে সমস্যা মেটাতে এসেছে প্রি-ট্রেইনড মডেল। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPhkHnJ2Lm4o",
        "colab_type": "text"
      },
      "source": [
        "জেফ হিটনের নোটবুক থেকে।\n",
        "\n",
        "https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_11_03_embedding.ipynb\n",
        "\n",
        "### বাংলায় বাক্যকে ভেক্টরাইজ করা (ওয়ার্ড২ভেক বা ফাস্টটেক্সট বাদ দিয়ে)\n",
        "টোকেনাইজ করানোটা দেখাতে চাই "
      ]
    }
  ]
}