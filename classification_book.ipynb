{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification_book.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "## লাইফস্টাইল প্রোডাক্টের ক্লাসিফিকেশন\n",
        "###  ফ্যাশন MNIST ডেটাসেট\n",
        "\n",
        "(ছবিগুলো আপডেট হবে বাংলায় - ধন্যবাদ উডাসিটি, কোর্সেরা, ডিপমাইন্ড, টেন্সর-ফ্লো সাইট)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FbVhjPpzn6BM"
      },
      "source": [
        "আমরা যে এক্সসাইজটা করলাম সেটা কিন্তু ডিপ লার্নিং এর কোন অংশ নয়, তবে সাধারণ একটা মেশিন লার্নিং সমস্যাকে ডিপ লার্নিং ফ্রেমওয়ার্কে সমাধান করার চেষ্টা করেছি মাত্র। ডিপ লার্নিং ফ্রেমওয়ার্কের কাজ আমরা দেখব অন্যান্য ক্ষেত্রে। এর আগের সমস্যাটা ছিল রিগ্রেশন, যেখানে আমরা কন্টিনিউয়াস ভেরিয়েবলের ইনপুট এর সাপেক্ষে আরেকটা কন্টিনিউয়াস আউটপুট পেয়েছি। ১৫ সেকেন্ড ডাকের সংখ্যার সাথে তাপমাত্রা। দুটোই কন্টিনিয়াস ভেরিয়েবল। এখন আমরা একটা ক্লাসিফিকেশন প্রবলেম দেখব। তবে সেটা সাধারণ ক্লাসিফিকেশন নয়, বরং ইমেজ সংক্রান্ত। ডিপ লার্নিং এর হ্যালো ওয়ার্ল্ড, মানে যে এক্সাইজ টা দিয়ে সবাই শুরু করে সেটাকে আমরা বলি MNIST ডাটা সেট। হাতে লেখা ইংরেজি অক্ষর কে ঠিকমতো চেনার জন্য এই এক্সেসাইজ টা শুরুর জন্য সবারই পছন্দের। কম্পিউটার ভিশনের সবচেয়ে প্রথম এক্সাইজ বলে সবাই চেনে তাকে।\n",
        "\n",
        "আমাদের এই এক্সারসাইজটা ব্যবহার ব্যবহার করছে কেরাস [tf.keras](https://www.tensorflow.org/guide/keras), একটা হাই-লেভেল এপিআই যা টেন্সর-ফ্লোএর কমপ্লেক্সিটি লুকিয়ে রাখে। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jL3OqFKZ9dFg",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  # শুধুমাত্র কোলাবে চেষ্টা করবো টেন্সর-ফ্লো ২.০ এর জন্য\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dzLKpmZICaWN",
        "colab": {}
      },
      "source": [
        "# টেন্সর-ফ্লো এবং কেরাসকে ইমপোর্ট করছি\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# কিছু হেলপার লাইব্রেরি\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "## ফ্যাশন MNIST ডেটাসেট ইমপোর্ট করছি"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DLdCchMdCaWQ"
      },
      "source": [
        "৭০,০০০ গ্রে-স্কেল স্যাম্পল আছে আমাদের [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)এ। এখন আমরা পুরোদস্তুর লাইফ স্টাইল ব্যবসায়ী হয়ে গেছি। আমাদের কাছে হাজারো হাজারো কাপড়চোপড়, টি শার্ট, জুতা স্নিকার্স, আরো বেশ কিছু জিনিস চলে এসেছে শোরুমে দেবার জন্য। কিন্তু আপনি নাছোড়বান্দা, আপনি বললেন ক্যাটালগিং ছাড়া এগুলো শোরুমে উঠানো টা ঠিক হবে না। আর এই ক্যাটালগিং আমরা হাতে কলমে করতে চাইনা। এক একটা জিনিস ধরবো ক্যামেরার সামনে, সে নিজে থেকেই ঠিকমতো ক্লাসিফাই করবে কোনটা কি জিনিস। মেশিনে ঠিকমতো আইডেন্টিফাই করতে পারলে আমাদের সমস্যা মিটে যাবে। আপনার দরকার একটা ডিপ লার্নিং মডেল। আপনার উর্দ্ধতন কর্মকর্তা বললেন, আমি ব্যাপারটা ঠিক বুঝি না, আমাকে কি একটা ডেমো দেখানো যাবে?\n",
        "\n",
        "\n",
        "** ২৮ X ২৮ পিক্সেল = ৭৬৮ বাইট **\n",
        "\n",
        "** ১০টা ক্যাটেগরি **\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
        "  </td></tr>\n",
        "  <tr><td align=\"center\">\n",
        "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
        "  </td></tr>\n",
        "</table>\n",
        "আর সে কারণেই আপনি দ্বারস্থ হলেন ফ্যাশন MNIST ডাটা সেটের কাছে। ব্যাপারটা সাধারণ MNIST এর সকল কোড সাপোর্ট করে, তবে ফ্যাশন MNIST বেশকিছু কমপ্লেক্স। এখানে প্রায় 70 হাজার স্যাম্পল আছে, তার মধ্যে দশ হাজার ইমেজ কে ব্যবহার করব আমাদের নেটওয়ার্ক কতটুকু শিখেছে ইমেজকে ঠিকমতো ক্লাসিফাই করার জন্য। এই ইমেজ গুলোর রেজুলেশন 28/8 800 পিক্সেল। ছবিতে দেখুন শুরুতেই টি-শার্ট টপ, মাঝখানে স্যান্ডেল, শেষের দিকে আঙ্কেল বুটস। নিচের ছবিতে দেখুন যে দশটা আইটেম আছে আমাদের এই ডাটা সেটে। এর পাশাপাশি আমরা সংখ্যায় লেভেল দিয়েছি 0 থেকে 9 পর্যন্ত। আপনি বুঝে গেছেন যে আমাদের ডিপ লার্নিং নেটওয়ার্ককে ট্রেইন করার জন্য ইমেজ আছে 60000। \n",
        "\n",
        "যেহেতু প্রতিটা ইমেজ 2828 গ্রেস্কেল ইমেজ, এর সাইজ হচ্ছে 764 বাইট (দুটোকে গুন দিলে যা হয়)। তার মানে হচ্ছে এর পিক্সএল ইনটেনসিটি শূন্য থেকে 255 পর্যন্ত। ইনপুট হিসেবে থাকবে এই 28/8 পিকচারের ছবি, মাঝখানে থাকবে আমাদের দীপ নিউড়াল নেটওয়ার্ক। এখন আমাদের নিউরাল নেটওয়ার্ক ইনপুট হিসেবে 764 বাইট কে নেবে, মাঝখানে সে জাদুর ছড়ি ঘুরিয়ে শেষে ক্লাসিফাইড করে দেবে একেকটা ইমেজ যা আসলে যেকোন দশটা আইটেমের একটা আইটেম হবে।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fKRao7xG8mT",
        "colab_type": "text"
      },
      "source": [
        "## ছবিতে কি কি আছে?\n",
        "\n",
        "ভালো করে নিচের ছবিটা দেখি। ছবিতে আলাদা করতে পারবেন এলিমেন্টগুলোকে?\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/raqueeb/deep_learning_book/master/start-page/cat1.png\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OX78HVzGzsDv"
      },
      "source": [
        "## ডেটাসেট আছে tf.keras.datasets এ fashion_mnist হিসেবে "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6Y3zaQEBzsDw"
      },
      "source": [
        "আমরা লোড ডাটাকে কল করলে নিয়ে আসবে দুই সেটের দুটো লিস্ট। ট্রেনিং এবং টেস্ট সেট। তার ভেতরে একটা কাপড় চোপড়ের ছবি, আরেকটা তার লেবেল মানে কোনটা কি?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7MqDQO0KCaWS",
        "colab": {}
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KHk65KPszsD0",
        "colab": {}
      },
      "source": [
        "# আমরা আমরা এভাবেও চেষ্টা করতে পারি\n",
        "\n",
        "# dataset, metadata = tfds.load('fashion_mnist', as_supervised=True, with_info=True)\n",
        "# train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "# ডেটা ভাগ করে ফেলি\n",
        "# num_train_examples = metadata.splits['train'].num_examples\n",
        "# num_test_examples = metadata.splits['test'].num_examples\n",
        "# print(\"Number of training examples: {}\".format(num_train_examples))\n",
        "# print(\"Number of test examples:     {}\".format(num_test_examples))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3uc7WwU5zsD3"
      },
      "source": [
        "## ডেটাসেট লোড করে কি পাবো?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t9FDsUlxCaWW"
      },
      "source": [
        "ডেটাসেট লোড করতে গিয়ে চারটা নামপাই অ্যারে পাচ্ছি। \n",
        "\n",
        "\n",
        "* এখানে `train_images` এবং `train_labels` অ্যারে দুটো আমাদের ট্রেনিং সেট — এগুলোকে আমরা ব্যবহার করবো মডেলকে ট্রেনিং করানোর জন্য। \n",
        "* মডেলকে টেস্ট করবো টেস্ট সেট দিয়ে। এই দুটো অ্যারে হিসেবে `test_images`, এবং `test_labels`।\n",
        "\n",
        "আমাদের ছবিগুলো ২৮x২৮ নামপাই অ্যারে যার পিক্সেল ভ্যালু ০ থেকে ২৫৫ এর মধ্যে। এখানে লেবেলগুলো ইন্টেজারের অ্যারে, ০ থেকে ৯ পর্যন্ত। তার করেসপন্ডিং ছবিগুলোর নাম দেখুন এখানে। \n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Label</th>\n",
        "    <th>Class</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>T-shirt/top</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Trouser</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Pullover</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Dress</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>4</td>\n",
        "    <td>Coat</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>5</td>\n",
        "    <td>Sandal</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>6</td>\n",
        "    <td>Shirt</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>7</td>\n",
        "    <td>Sneaker</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>8</td>\n",
        "    <td>Bag</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>9</td>\n",
        "    <td>Ankle boot</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "প্রতিটা ছবি কিন্তু ম্যাপিং করা আছে একেকটা লেবেলে। একটা ক্লাস নাম দিয়ে রাখি পরে ব্যবহার করতে। প্লটিং এর সময়ে লাগবে। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IjnLH5S2CaWx",
        "colab": {}
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Brm0b_KACaWX"
      },
      "source": [
        "## এক্সপ্লোরেটরি ডেটা অ্যানালাইসিস\n",
        "\n",
        "ডেটাগুলোকে একটু নেড়েচেড়ে দেখি। এটা একটা ভালো প্র্যাকটিস - ডেটাকে চিনতে। এখানে দেখছি ৬০,০০০ ছবি, ট্রেনিং সেটে। ২৮x২৮ পিক্সেল, ০ থেকে ৯ লেবেল। বাকি ১০,০০০ টেস্ট ছবি, পরীক্ষা করার জন্য। \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/raqueeb/deep_learning_book/master/start-page/cat2.png\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zW5k_xz1CaWX",
        "colab": {}
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cIAcvQqMCaWf"
      },
      "source": [
        "৬০,০০০ লেবেলও আছে সঙ্গে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TRFYHB2mCaWb",
        "colab": {}
      },
      "source": [
        "len(train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YSlYxFuRCaWk"
      },
      "source": [
        "০ থেকে ৯ লেবেল সব ইন্টেজার ভ্যালু। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XKnCTHz4CaWg",
        "colab": {}
      },
      "source": [
        "train_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TMPI88iZpO2T"
      },
      "source": [
        "১০,০০০ টেস্ট ছবি। ২৮x২৮ পিক্সেল আছে এখানেও। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2KFnYlcwCaWl",
        "colab": {}
      },
      "source": [
        "test_images.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rd0A0Iu0CaWq"
      },
      "source": [
        "আছে ১০,০০০ লেবেলও। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iJmPr5-ACaWn",
        "colab": {}
      },
      "source": [
        "len(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCZSrr0YzsER",
        "colab_type": "text"
      },
      "source": [
        "## ট্রেনিং ছবি এবং তার লেবেল\n",
        "\n",
        "আচ্ছা, এর ভেতরের ডেটাগুলো কেমন দেখতে? আমরা একটা ট্রেনিং ছবি দেখি।  তার সাথে তার লেবেল। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sfs4yNNxzsEV",
        "colab": {}
      },
      "source": [
        "print(train_labels[0])\n",
        "\n",
        "print(train_images[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ES6uQoLKCaWr"
      },
      "source": [
        "## ডেটার প্রি-প্রসেসিং \n",
        "\n",
        "মনে আছে ডেটা নরমালাইজ করার কথা? যখন ডেটা বেশি ডিস্ট্রিবিউটেড থাকে তখন সেটা কাজ করানো সমস্যা হয়। আমরা যদি প্রথম ছবিটা দেখি, তাহলে বুঝবো সেটার পিক্সেল রেঞ্জ ০ থেকে ২৫৫ পর্যন্ত। \n",
        "\n",
        "ছবিটা দেখি কালারবার সহ। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m4VEw8Ud9Quh",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wz7l27Lz9S1P"
      },
      "source": [
        "আমাদের ডাটাগুলোকে এমনভাবে প্রি-প্রসেস করতে হবে যাতে সেটা ঠিকমতো কাজ করতে পারে। যেহেতু আমাদের ইনপুট ভ্যারিয়েবলব্গুলো গ্রেস্কেল মানে ০ থেকে ২৫৫ পর্যন্ত রেঞ্জ, সে কারণে আমরা সেটাকে ০ থেকে ১ এর মধ্যে নিয়ে আসব। আমাদের ট্রেনিং এবং টেস্ট সেট দুটোকেই এই নরমালাইজেশন এর মধ্যে নিয়ে আসতে হবে। এটা করার জন্য আমাদেরকে প্রতিটা ইমেজকে ২৫৫ দিয়ে ভাগ করতে হবে। তাহলে সেটা ০ থেকে ১ এর মধ্যে চলে আসবে।\n",
        "\n",
        "পাইথনে সুবিধা হচ্ছে একটা লিস্টকে নর্মালাইজ করতে লুপিং করতে হবে না। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bW5WzIPlCaWv",
        "colab": {}
      },
      "source": [
        "train_images = train_images / 255.0\n",
        "\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Yu7t_jpzsEh",
        "colab": {}
      },
      "source": [
        "# আমরা এভাবেও করতে পারি, একটা ফাংশন ডিক্লেয়ার করে \n",
        "\n",
        "# def normalize(images, labels):\n",
        "#  images = tf.cast(images, tf.float32)\n",
        "#  images /= 255\n",
        "#  return images, labels\n",
        "\n",
        "# The map function applies the normalize function to each element in the train\n",
        "# and test datasets\n",
        "# train_dataset =  train_dataset.map(normalize)\n",
        "# test_dataset  =  test_dataset.map(normalize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ee638AlnCaWz"
      },
      "source": [
        "আমরা দেখতে চাচ্ছি ডেটাগুলো ঠিকমতো ফরম্যাটে আছে কিনা? যেহেতু আমরা এই ডাটাগুলোকে ট্রেনিংয়ে ব্যবহার করবো, সেকারণে প্রথম ২৫টা ছবিকে ডিসপ্লে করি। যেহেতু আমরা আগেই ক্লাস ডিক্লেয়ার করেছিলাম সেকারণে নিচে সেটার নাম দেখা যাবে। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oZTImqg_CaW1",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "59veuiEZCaW4"
      },
      "source": [
        "## মডেলকে বিল্ড করি \n",
        "\n",
        "আগেও দেখেছি একটা নিউরাল নেটওয়ার্ক তৈরি করতে প্রথমে মডেলের লেয়ার বানাতে হবে। এরপর আসবে কম্পাইলিং। মডেলের কাজ হচ্ছে ইনপুট থেকে ২৮ x ২৮ পিক্সেল মানে ৭৮৪ পিক্সেল থেকে এই নিউরাল নেটওয়ার্কের লেয়ারগুলো শার্ট/ব্যাগ এর মতো আইটেমগুলোকে ক্লাসিফাই করবো। \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/raqueeb/deep_learning_book/master/start-page/cat3.png\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gxg1XGm0eOBy"
      },
      "source": [
        "### লেয়ারগুলোকে সেট করি\n",
        "\n",
        "একটা নিউরাল নেটওয়ার্কের বেসিক বিল্ডিং ব্লক হচ্ছে তার লেয়ার। এই লেয়ারগুলো বিভিন্ন ফিচার এক্সট্রাক্ট করে রিপ্রেজেন্টেশন এর মাধ্যমে। ইনপুট ডাটা থেকে। যেহেতু একেকটার লেয়ার আরেকটা লেয়ারের সাথে সিকুয়েন্সিয়াল, সে কারণেই ডেন্স `tf.keras.layers.Dense` ব্যবহার করছি। এর ভেতরের ইন্টারনাল প্যারামিটারগুলো নিজে থেকেই এডজাস্ট হয় মানে শেখে এই ট্রেনিং এর সময়। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9ODch-OFCaW4",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gut8A_7rCaW6"
      },
      "source": [
        "আমাদের এই নেটওয়ার্কটা তিনটা লেয়ারের, ছবি দেখুন। \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/raqueeb/deep_learning_book/master/start-page/cat5.1.png\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
        "\n",
        "* **ইনপুট লেয়ার** - প্রথম লেয়ারটার কাজ বেশ মজার। এর কাজ হচ্ছে ইনপুটকে ফ্ল্যাট করে দেয়া - `tf.keras.layers.Flatten`। আমাদের ইনপুট হিসেবে যে ইমেজ আছে সেটা কিন্তু একটা ২ ডাইমেনশনের অ্যারে মানে ২৮ x ২৮ পিক্সেল। বেসিক মডেলগুলোতে ডাইমেনশনের অ্যারের লাইনগুলোকে একসাথে প্রসেস করা কঠিন, তাই সেগুলোকে আমরা পাশাপাশি খুলে এক লাইনে দাঁড় করাবো। এই ২৮ x ২৮ পিক্সেলকে এক লাইনে দাঁড় করালে ৭৮৪ পিক্সেল হয়। এই লেয়ারের কোন লার্নিং প্যারামিটার নেই, তার কাজ হচ্ছে এটাকে রিফর্মেট মানে নতুন ফরম্যাটে ট্রান্সফরম করে দেয়।\n",
        "\n",
        "* **\"হিডেন লেয়ার\"** — এটা 'ডেন্স' লেয়ার, একটার সাথে আরেকটা 'ডেন্সলি কানেক্টেড' মানে `tf.keras.layers.Dense`। ইনপুট লেয়ারের পর থেকে আমরা হিডেন লেয়ার বলতে পারি। এই লেয়ারটাকে আমরা কনফিগার করেছি ১২৮টা নিউরন দিয়ে। আরো কম বেশি করতে পারতাম। প্রতিটা নিউরন যাকে আমরা নোড বলছি সেটা কিন্তু ৭৮৪টা নোডকে কানেক্ট করছে আগের লেয়ার থেকে নতুন লেয়ারে। ইনপুট এর ওয়েট এর উপর ভিত্তি করে এই হিডেন প্যারামিটারগুলো শেখে ট্রেনিং এর সময়। তবে তাদের আউটপুটের যে সিঙ্গেল ভ্যালু সেটাকে সে পাঠায় পরবর্তী লেয়ারে। এখানে আমরা ইচ্ছা করলে আরো কয়েকটা লেয়ার দিতে পারতাম, তবে শেখার জন্য আমরা এটাকে একটু সিম্প্লিস্টিক রাখছি। আমাদের অ্যাক্টিভেশন ফাংশন হিসেবে এখানে ব্যবহার করেছি রেলু, যা আসলে নন লিনিয়ার ফাংশনকে ভালোভাবে সামনে এগিয়ে দিতে পারে।\n",
        "\n",
        "* আমাদের এখানে activation='relu' এর কাজ হচ্ছে \"যদি X>0 তাহলে রিটার্ন X, নাহলে রিটার্ন 0\"। এর মানে হচ্ছে এই অ্যাক্টিভেশন ফাংশন লেয়ারে X শূন্য এর বড়ো হলে সামনের লেয়ারে X এর ভ্যালু পাঠাবে। \n",
        "\n",
        "* **আউটপুট লেয়ার** `tf.keras.layers.Dense` — এর কনফিগারেশন আগের মতো। ডেন্স। আউটপুট লেয়ারে আপনাদের অবশ্যই মনে আছে আমাদেরকে ক্লাসিফাই করতে হবে দশটা অবজেক্ট। ১০ ধরনের কাপড়, চোপড়, জুতা এগুলোকে যখন ক্লাসিফাই করব তখন অ্যাক্টিভেশন লেয়ার হিসাবে ব্যবহার করব 'সফটম্যাক্স'। 'সফটম্যাক্স' এর কাজ হচ্ছে তাকে যদি দশটা ভ্যালু দেয়া হয় সে সবার ম্যাক্সিমাম ভ্যালু, সবচেয়ে বড় ভ্যালুটা পিক করবে পরের লেয়ারে পাঠাতে। এই ফাংশন আমাদের অনেক কোডিং বাঁচায়। \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/raqueeb/deep_learning_book/master/start-page/cat6.png\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
        "* প্রথম লেয়ারে ইনপুট ছিল ৭৮৪ বাইট, এরপরের লেয়ার থেকে শেষ লেয়ারে আমাদের ইনপুট হবে ১২৮ নোড থেকে। প্রতিটা নোডের ওয়েট ইনপুট এর সাথে যুক্ত হয়ে সেটা একটা শেখানো প্যারামিটার হিসেবে দাঁড়াবে। আমাদের আউটপুট ভ্যালু কিন্তু ০ থেকে ১ এর মধ্যে, আমরা যেটাকে বলি প্রবাবিলিটি ডিস্ট্রিবিউশন যেটা আমাদেরকে ওই ছবিটা কে কোন ক্লাসে পড়বে সেটা দেখাবে। এর মধ্যে সবচেয়ে বড়োটা পিক করবে সফটম্যাক্স। ছবি দেখুন, এই দশটা নোটের সবগুলোর ভ্যালু যোগ করলে এক হবে।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JamJ00chzsE0"
      },
      "source": [
        "### মডেলকে কম্পাইল করা\n",
        "\n",
        "ট্রেনিং করানোর আগে আমাদেরকে কিছু সেটিং বুঝতে হবে। এগুলোকে আমরা যোগ করবো মডেল কম্পাইলিং স্টেজে। \n",
        "\n",
        "* *লস ফাংশন* — আগেও আলাপ করেছিলাম, তবে মডেলকে ঠিক জায়গায় মানে ঠিক অ্যাক্যুরেসিতে পৌঁছাতে হলে এই ফাংশন দরকার। আমাদের এখানে আউটপুট ১০ টা ক্যাটেগরীতে একটা আরেকটার সাথে মিউচুয়ালি এক্সক্লুসিভ (যেখানে একেকটা স্যাম্পল একদম একটা ক্লাসেই থাকবে), সেকারণে ব্যবহার করছি 'sparse_categorical_crossentropy' ব্যবহার করছি লস হিসেবে। \n",
        "\n",
        "* *অপটিমাইজার* — লস ক্যালকুলেট করে সেটার কারেকশন/আপডেট পাঠানোই এর কাজ। \n",
        "\n",
        "* *মেট্রিকস* — আমরা যখন ট্রেনিং এবং টেস্ট একসাথে মনিটর করবো, তখন অ্যাক্যুরেসির দরকার পড়বে। আমাদের এখানে যখন কিছু ছবি ঠিকমতো ক্লাসিফাই হয়েছে, তখন অ্যাক্যুরেসিটা কার্যকরী। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lhan11blCaW7",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qKF6uW-BCaW-"
      },
      "source": [
        "## মডেলকে ট্রেনিং করাই\n",
        "\n",
        "আমাদের এই নিউরাল নেটওয়ার্ককে ট্রেনিং করাতে সামনের কয়েকটা স্টেপ দরকার পড়বে;\n",
        "\n",
        "১. শুরতেই ডেটাকে ফিড করতে হবে মডেলকে। আমাদের এখানে ট্রেনিং ডেটা আছে `train_images` এবং `train_labels` অ্যারেগুলোতে।\n",
        "২. মডেল শেখে তার ছবি আর লেবেলের মধ্যে সম্পর্ক থেকে। \n",
        "৩. এখন আমাদের মডেল টেস্ট করার পালা। `train_images` থেকে প্রেডিকশন করাবো? ভুলেও না। তাহলে পরীক্ষায় প্রশ্ন ফাঁস হয়ে যাবে। বরং আমরা ব্যবহার করবো আনকোরা টেস্ট সেট থেকে যাদের আলাদা করে রেখেছিলাম। আমরা ব্যবহার করবো `test_images` অ্যারে। আমাদেরকে ভেরিফাই করতে হবে `test_images` অ্যারে ম্যাচ করে কিনা `test_labels`কে। \n",
        "৪. `epochs=10` মানে পুরো ট্রেনিংসেটের ওপর ১০ বার পুরো আইটারেশন চালাবে, মানে সর্বমোট  ১০ * ৬০০০০ = ৬০০০০০ এক্সাম্পল।\n",
        "\n",
        "ট্রেনিং শুরুতেই কল করবো `model.fit` মেথডকে — এর কাজ হচ্ছে train_images এবং  train_labels এর ভেতরের সম্পর্ককে শেখা। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xvwvpA64CaW_",
        "colab": {}
      },
      "source": [
        "model.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W3ZVOhugCaXA"
      },
      "source": [
        "এই প্রসেসটা ভালো করে লক্ষ্য করুন। মডেল ট্রেনিং এর পাশাপাশি তার লস এবং অ্যাক্যুরেসির দিকে নজর রাখুন। লস কমার সাথে সাথে অ্যাক্যুরেসি বাড়ছে। এই মডেলের অ্যাক্যুরেসি বেড়ে চলে এসেছে ০.৮৮ মানে ৮৮% ট্রেনিং ডেটার ওপর। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oEw4bZgGCaXB"
      },
      "source": [
        "## অ্যাক্যুরেসিকে টেস্ট ডেটা দিয়ে ইভাল্যুয়েট করানো\n",
        "\n",
        "ট্রেনিং দেখলাম, এখন দেখতে হবে কেমন কাজ করে টেস্ট ডেটার ওপর?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VflXLEeECaXC",
        "colab": {}
      },
      "source": [
        "# model.evaluate(test_images, test_labels)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yWfgsmVXCaXG"
      },
      "source": [
        "টেস্ট ডাটাসেটের ওপর আমরা যখন অ্যাক্যুরেসি টেস্ট করলাম, তখন দেখা গেল এখানে তারা কিছুটা খারাপ করেছে। মানে ট্রেনিং ডাটাসেট থেকে টেস্টডাটা সেটের অ্যাক্যুরেসি কিছুটা খারাপ। এই ট্রেনিং অ্যাক্যুরেসি এবং টেস্টসেটের মধ্যে যে গ্যাপ সেটাকে আমরা বলি 'ওভার ফিটিং'। 'ওভার ফিটিং' হচ্ছে এমন একটা স্ট্যাটাস, যখন মেশিন লার্নিং মডেল নতুন ডাটা, যে ডাটা আগে দেখেনি - সেই ডাটাতে ট্রেনিং ডাটার অ্যাক্যুরেসি থেকে খারাপ করে। \n",
        "\n",
        "তবে এই অ্যাক্যুরেসি বাড়ানোর উপায় নিয়ে আলাপ করবো সামনে। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xsoS7CPDCaXH"
      },
      "source": [
        "## এখন প্রেডিকশনের পালা \n",
        "\n",
        "যিহেতু আমাদের মডেলের ট্রেনিং হয়ে গেছে, এখন কিছু পরীক্ষা নিরীক্ষার পালা। কিছু ছবি প্রেডিকশন করি এখন। প্রেডিক্ট করি টেস্ট ছবি নিয়ে। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gl91RPhdCaXI",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(test_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x9Kk1voUCaXJ"
      },
      "source": [
        "প্রেডিকশন শেষ। আমাদের মডেল প্রতিটা ছবিকে ঠিকমতো লেবেল করেছে কিনা, সেটা দেখা দরকার। শুরুতেই প্রথম প্রেডিকশনটা দেখি।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3DmJEUinCaXK",
        "colab": {}
      },
      "source": [
        "predictions[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-hw1hgeSCaXN"
      },
      "source": [
        "ভালো করে লক্ষ্য করুন। আমাদের আউটপুট লেয়ারে যে ১০টা প্রোবাবিলিটি ডিস্ট্রিবিউশন দিচ্ছে, সেখানে সবচেয়ে বেশি সংখ্যাটাই হচ্ছে তার লেবেলের কনফিডেন্স। সবচেয়ে বেশি সংখ্যাটা কি হতে পারে? এখানে একটা ছবি দেখতে পারেন বোঝার সুবিধার জন্য। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qsqenuPnCaXO",
        "colab": {}
      },
      "source": [
        "np.argmax(predictions[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E51yS7iCCaXO"
      },
      "source": [
        "আমাদের মডেল সবচেয়ে বেশি কনফিডেন্ট অ্যাঙ্কেল বুট হিসেবে যার ক্লাস নাম হচ্ছে `class_names[9]`. আমরা যখন চেক করলাম, তখন বোঝা গেল আমাদের ক্লাসিফিকেশন ঠিক আছে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sd7Pgsu6CaXP",
        "colab": {}
      },
      "source": [
        "test_labels[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ygh2yYC972ne"
      },
      "source": [
        "এই কোড দেখে ভয় পাবেন না। এই মুহূর্তে আপনার এটা জানতে হবে না। দশটা স্যাম্পলকে আমরা গ্রাফে দেখতে চাচ্ছি ১০টা আলাদা আলাদা ক্লাস প্রেডিকশনে। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DvYmmrpIy6Y1",
        "colab": {}
      },
      "source": [
        "def plot_image(i, predictions_array, true_label, img):\n",
        "  predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red'\n",
        "\n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  predictions_array, true_label = predictions_array, true_label[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks(range(10))\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d4Ov9OFDMmOD"
      },
      "source": [
        "ভালো করে লক্ষ্য করুন, ০তম ছবিটা, তার প্রেডিকশনএবং তার প্রেডিকশন অ্যারে। ঠিক প্রেডিকশন লেবেলগুলো নীল (মানে হালকা কালো) আর যেগুলো ঠিক প্রেডিকশন নয়, সেগুলোর লেবেল লাল (হালকা গ্রে)। শেষে ১০০ এর মধ্যে শতাংশ আকারে প্রেডিক্টেড লেবেলের ভ্যালু।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HV5jw-5HwSmO",
        "colab": {}
      },
      "source": [
        "i = 0\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, predictions[i], test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, predictions[i],  test_labels)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ko-uzOufSCSe",
        "colab": {}
      },
      "source": [
        "i = 12\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, predictions[i], test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, predictions[i],  test_labels)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kgdvGD52CaXR"
      },
      "source": [
        "কয়েকটা ছবির পাশাপাশি তার কয়েকটা প্রেডিকশন প্লট করে দেখি। অনেক সময় দেখা যায় - মডেল মাঝে মাঝে ভুল করে যদিও তারা এই ব্যাপারে বেশ 'কনফিডেন্ট' থাকে। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hQlnbqaw2Qu_",
        "colab": {}
      },
      "source": [
        "# Plot the first X test images, their predicted labels, and the true labels.\n",
        "# Color correct predictions in blue and incorrect predictions in red.\n",
        "num_rows = 5\n",
        "num_cols = 3\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  plot_image(i, predictions[i], test_labels, test_images)\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  plot_value_array(i, predictions[i], test_labels)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R32zteKHCaXT"
      },
      "source": [
        "সবশেষে আমাদের এই ট্রেইনড মডেলকে দিয়ে একটা ছবি প্রেডিক্ট করাই। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yRJ7JU7JCaXT",
        "colab": {}
      },
      "source": [
        "# Grab an image from the test dataset.\n",
        "img = test_images[1]\n",
        "\n",
        "print(img.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vz3bVp21CaXV"
      },
      "source": [
        "`tf.keras` মডেল ব্যাচে প্রেডিকশন করার জন্য অপ্টিমাইজ। মানে একটা একটা করে না। এইজন্য ব্যাচে প্রেডিকশন সবসময় সময় বাঁচায়। এখন আমরা যদি একটা ছবিকে ব্যবহার করি সেটাকেও একটা লিস্টে ফেলতে হবে। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lDFh5yF_CaXW",
        "colab": {}
      },
      "source": [
        "# Add the image to a batch where it's the only member.\n",
        "img = (np.expand_dims(img,0))\n",
        "\n",
        "print(img.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EQ5wLTkcCaXY"
      },
      "source": [
        "সঠিক লেবেলের জন্য প্রেডিক্ট করি -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o_rzNSdrCaXY",
        "colab": {}
      },
      "source": [
        "predictions_single = model.predict(img)\n",
        "\n",
        "print(predictions_single)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Ai-cpLjO-3A",
        "colab": {}
      },
      "source": [
        "plot_value_array(1, predictions_single[0], test_labels)\n",
        "_ = plt.xticks(range(10), class_names, rotation=45)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cU1Y2OAMCaXb"
      },
      "source": [
        "`model.predict` আমাদেরকে একটা লিস্টের লিস্ট দেয়, যেখানে প্রতিটা ছবি ওই ব্যাচে আছে। এখন ওই ব্যাচের একটা ছবির জন্য প্রেডিকশন দেখি। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2tRmdq_8CaXb",
        "colab": {}
      },
      "source": [
        "np.argmax(predictions_single[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YFc2HbEVCaXd"
      },
      "source": [
        "কি বোঝা গেলো? মডেল ঠিকমতো প্রেডিক্ট করতে পেরেছে বলতে হবে। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9v7D7O_4zsGB"
      },
      "source": [
        "## কেন কলব্যাক? \n",
        "\n",
        "মডেলকে ট্রেইন করতে সময় লাগে। বিশেষ করে আমরা যখন বিভিন্ন ইপক সংখ্যা নিয়ে কাজ করি। একেকটা ইপক চালাতে যেরকম কম্পিউটিং রিসোর্স ব্যবহার হয়, সেখানে আমাদেরকে পয়সা গুনতে হয় ক্লাউডএর জন্য। এমন যদি হতো আমরা একটা ট্রেনিং বন্ধ করতে পারতাম যখন আমরা একটা দরকারি অ্যাক্যুরেসি পয়েন্টে পৌঁছে গেছি। এই মুহূর্তে আমাদের প্রতিবারই একটা ইপক সংখ্যা পর্যন্ত যেতে হচ্ছে। ভালো খবর হচ্ছে, আমাদের একটা কলব্যাক ফাংশন আছে যেটা এই ট্রেনিং লুপকে দরকারি অ্যাক্যুরেসি এলেই আপনা আপনি বন্ধ করতে পারে। আমরা আসলে ওই পর্যন্ত পৌঁছাতে চাই ওই লেভেলের অ্যাক্যুরেসি পৌঁছে গেলে ট্রেনিং বন্ধ করে দিতে পারি।\n",
        "\n",
        "এখন ফ্যাশন MNIST এর মধ্যে কলব্যাক ফিচারটা চালাই। আমাদের 'মডেল.ফিট' ফাংশনটা যখন ট্রেনিং লুপের উপর কাজ করে তখন আমরা লক্ষ্য রাখি সেটার আউটপুটে। পাইথনে কাজটা করা বেশ সোজা। যদিও জিনিসটা ইমপ্লিমেন্ট করা হচ্ছে একটা আলাদা ক্লাস হিসেবে তবে সেটা ইনলাইনে থাকছে আমাদের সেই একই কোডে। ধরা যাক, আমরা চাইছি যখন লস ০.২ এর নিচে থাকতে হবে, তাহলে সেই ট্রেনিংটা এমনিতেই ক্যান্সেল হয়ে যাবে।\n",
        "\n",
        "দেখি কখন আমাদের ট্রেনিং লুপ থামে!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pa1Zw-lczsGC",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.9):\n",
        "      print(\"\\nReached 90% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "callbacks = myCallback()\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, callbacks=[callbacks])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se_338WFG8oY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}