{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_1st_chapter2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YHI3vyhv5p85"
      },
      "source": [
        "## ঝিঁঝিঁ পোকার থার্মোমিটার \n",
        "\n",
        "Rev 6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F8YVA_634OFk"
      },
      "source": [
        "মনে আছে ছোট্ট রকিব এবং তার ফুপিমার গল্পের কথা? আমি আগের বই “শূন্য থেকে পাইথন মেশিন লার্নিং” বইটার কথা বলছিলাম। সেখানে গল্পটা ছিল এরকম, কোন এক রাত্রে যখন সবাই গরমে কাহিল, তখন ওই সময়ে কত টেম্পারেচার সেটা নিয়ে কথা হচ্ছিল ছোট্ট রকিবের গ্রামের দাদা বাড়িতে। ফুঁপিমা বলছিলেন, বাইরের বসার ঘরের সেই বড় থার্মোমিটার না দেখেও তখনকার টেম্পারেচার আন্দাজ করা যাবে ঝিঁঝিঁপোকার ডাক থেকে। সবাই অবাক, সবার প্রশ্ন কিভাবে?\n",
        "\n",
        "\n",
        "বোঝা গেল যে ঝিঁঝিঁপোকার ডাকের সাথে তাপমাত্রা একটা সম্পর্ক আছে। তাপমাত্রা বাড়লে ঝিঁঝিঁপোকার ডাকার ফ্রিকোয়েন্সি বেড়ে যায়। এবং এই ডাকার ফ্রিকোয়েন্সি তাপমাত্রা সাথে অনেকটাই লিনিয়ার। মানে, তাপমাত্রা বাড়লে ডাকের ফ্রিকুয়েন্সি বাড়ে। ব্যাপারটাকে উল্টো করে ধরলে বলা যায়, ঝিঁঝিঁপোকার ডাককে ঠিকমত গুনতে পারলে ওই মুহূর্তের তাপমাত্রা বের করা সম্ভব হবে। ফুঁপিমা এর নোটবুক থেকে দেখা গেল, উনি একেকদিনের ঝিঁঝিঁপোকার ডাক এবং তাপমাত্রা পাশাপাশি লিখে সেটার মধ্যে একটা যোগসুত্র বের করেছিলেন সেগুলোকে প্লট করে। পুরো ১ মিনিটের ডাক রেকর্ড না করে তার প্রতি ১৫ সেকেন্ডের ঝিঁঝিঁপোকার ডাক এর সাথে তাপমাত্রাকে প্লটিংয়েই বোঝা গেল সেই লিনিয়ার সম্পর্ককে। \n",
        "\n",
        "ঝিঁঝিঁপোকার ডাক বেড়ে যাওয়া মানে তাপমাত্রা বেড়ে যাওয়া। সেখান থেকে একটা ফর্মুলা বের করেছিলেন ওই সময়। ওই ফর্মুলা দিয়ে আমাদেরকে কেউ ঝিঁঝিঁপোকার ডাক এর সংখ্যা বললে তার করেসপন্ডিং ওই সময়ে কত তাপমাত্রা হবে সেটা বের করা যাবে ওই ফর্মুলা দিয়ে। তাহলে তো আর সেটা মেশিন লার্নিং হলো না। ফর্মুলা হচ্ছে একটা রুল বেইজড সিস্টেম, যা মেশিন ডেটা থেকে শেখে না। আমি এই মুহূর্তে ফর্মুলাটা আমাদের মেশিনের কাছে আগে থেকে বলছি না, কারণ আমাদের ফুঁপিমা নোটবুক থেকে ডেটা সরাসরি মেশিনে দিয়ে দেবো - সে তার ফর্মুলা বের করতে পারে কিনা? যদি সে ইনপুট ডেটা থেকেই ফর্মুলা বের করতে পারে তাহলে আমরা বুঝে যাবো আমাদের মেশিন শিখছে। সে একটা লার্নিং মেশিন। ডেটা পাল্টে গেলে আবার সে নতুন ফর্মুলা দেবে। \n",
        "\n",
        "রাজি তো? আবারো বলছি - আমরা মেশিনকে আগে থেকে ফর্মুলা বলবো না। দেখি সে ফর্মুলা বের করতে পারে কিনা?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AC3EQFi20buB"
      },
      "source": [
        "## প্রবলেম স্টেটমেন্ট\n",
        "\n",
        "আমরা ঝিঝি পোকার 15 সেকেন্ডের ডাকের সংখ্যা বলবো, মেশিন কে আমাদেরকে বলতে হবে এখনকার তাপমাত্রা কত? এই মুহূর্তে আমাদের কাছে 55 টা রেকর্ড আছে যেখানে 15 সেকেন্ডের ঝিঝি পোকার ডাকের করেসপন্ডিং তাপমাত্রা দেয়া আছে টেবিলে। আপনারা শূন্য থেকে পাইথন মেশিন লার্নিং বইটা দেখতে পারেন। পাশাপাশি সেই ডাটা সেটের লিংক নিচে দেয়া হল।\n",
        "\n",
        "ব্যাপারটাকে আমি তিন ভাবে করতে পারি। \n",
        "\n",
        "শুধুমাত্র এটুকু বলতে পারি, প্রথম দুটো মেশিন লার্নিং নয়। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fA93WUy1zzWf"
      },
      "source": [
        "## শুরুতেই ডিপেন্ডেন্সিগুলোকে ইমপোর্ট \n",
        "\n",
        "১. প্রথমেই টেন্সর-ফ্লো, এটাকে আমরা `tf` বলবো সুবিধার জন্য। \n",
        "\n",
        "২. টেন্সর-ফ্লো আর নামপাই খুব কাছের জিনিস। নামপাইকে শর্ট করে `np`, যা আমাদেরকে দেবে C++ এর গতি। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X9uIpOS2zx7k",
        "colab": {}
      },
      "source": [
        "import tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQOoosIKmx4y",
        "colab_type": "text"
      },
      "source": [
        "# টেন্সর-ফ্লো ২.১\n",
        "\n",
        "বইটা যখন বাজারে যাবে, আমি ধারণা করি তখন টেন্সর-ফ্লো ২.x.x সব জায়গায় আপডেট হয়ে যাবে। আমি ধরে নিচ্ছি আমাদের কোথাও (বিশেষ করে জুপিটার নোটবুকে পুরানো ভার্সন ইনস্টল করা আছে। সেকারণে আগে দেখে নেই আমাদের টেন্সর-ফ্লো এর কতো ভার্সন ইনস্টল করা আছে। অন্য ভার্সন থাকলে সেটাকে আপগ্রেড করে নেবো নতুন ভার্শনে। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y_WQEM5MGmg3",
        "outputId": "7333fdb8-08c3-43ab-e950-212a196818f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tensorflow.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fO4IBRk0Epf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "7a003597-79d9-46f7-e402-01cb85bf60af"
      },
      "source": [
        "%tensorflow_version"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently selected TF version: 1.x\n",
            "Available versions:\n",
            "* 1.x\n",
            "* 2.x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxNZxGXBmx43",
        "colab_type": "text"
      },
      "source": [
        "আপগ্রেড করে নিচ্ছি ২.০তে। এমুহুর্তে দরকার না থাকলেও আমরা পুরো বইটা টেন্সর-ফ্লো ২.০ দিয়ে সাঁজাতে চাই প্লাটফর্মের কনসিস্টেন্সির জন্য। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kanaYotimx43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q tensorflow==2.1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbTedodSoseW",
        "colab_type": "text"
      },
      "source": [
        "রানটাইম এনভায়রনমেন্ট রিস্টার্ট করি "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGce_BWknhcf",
        "colab_type": "code",
        "outputId": "e3ea480e-52bc-4821-9786-e8344b067e39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow\n",
        "tensorflow.__version__"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-Sop5lyDVwL",
        "colab_type": "text"
      },
      "source": [
        "## টেন্সর-ফ্লো ২.x সিলেকশন\n",
        "\n",
        "পুরো বইকে কনসিস্টেন্ট রাখার জন্য আমরা নিচের এই কোড ব্যবহার করবো যাতে গুগল কোলাব/জুপিটার নোটবুকে টেন্সর-ফ্লো ২.x সিলেক্ট করতে পারে। রিসেট করে নিন গুগল কোলাবের সব রানটাইম। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alFHey8JD_E4",
        "colab_type": "code",
        "outputId": "d0d12e5e-aed4-4e1e-c7fc-46d788ceb8c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "try:\n",
        "  # শুধুমাত্র টেন্সর-ফ্লো ২.x ব্যবহার করবো \n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "keras = tf.keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_e6ixySwRx3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5056b0f3-032d-44dd-e76c-0bcc1f06fbd1"
      },
      "source": [
        "# আমাদের জিপিউ কয়টা এখানে?\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BK7k3vIUhoqZ"
      },
      "source": [
        "## ডেটা প্লটিং\n",
        "\n",
        "শুরুতেই আমরা ডাটাগুলোকে দুটো লিস্টে ভাগ করি। প্রথম লিস্ট 'chirp15s' যেখানে আমরা ১৫ সেকেন্ডে ঝিঁঝিঁ পোকার ডাকের সংখ্যা রেকর্ড করেছি। প্রথম দুটো রেকর্ড দেখলে বোঝা যায় ঝিঁঝিঁপোকা ১৫ সেকেন্ডে ৪৪ এবং ৪৬ বার ডেকেছে। পরের লিস্টে হচ্ছে তাপমাত্রা, যা সেলসিয়াস \"temp_celsius\" রেকর্ড করা হয়েছে। সেলসিয়াস মানে আমাদের সেন্টিগ্রেড। বোঝার সুবিধার জন্য এখানে একটা ফর লুপে আমরা ঝিঁঝিঁপোকার ডাক এর পাশাপাশি তাপমাত্রা ফেলে দিচ্ছি। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gg4pn6aI1vms",
        "outputId": "ab54e35b-5cff-4bec-a3ba-2a36fb7fc2ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "chips_15s    = np.array([44.000,46.400,43.600,35.000,35.000,32.600,28.900,27.700,25.500,20.375,12.500,37.000,37.500,36.500,36.200,33.000,43.000,46.000,29.000,31.700,31.000,28.750,23.500,32.400,31.000,29.500,22.500,20.600,35.000,33.100,31.500,28.800,21.300,37.800,37.000,37.100,36.200,31.400,30.200,31.300,26.100,25.200,23.660,22.250,17.500,15.500,14.750,15.000,14.000,18.500,27.700,26.000,21.700,12.500,12.500],  dtype=float)\n",
        "temp_celsius = np.array([26.944, 25.833, 25.556, 23.056, 21.389, 20.000, 18.889, 18.333, 16.389, 13.889, 12.778, 24.583, 23.333, 23.333, 22.500, 18.889, 25.278, 25.833, 20.278, 20.278, 20.000, 18.889, 15.000, 21.111, 20.556, 19.444, 16.250, 14.722, 22.222, 21.667, 20.556, 19.167, 15.556, 23.889, 22.917, 22.500, 21.111, 19.722, 18.889, 20.556, 17.222, 17.222, 16.111, 16.667, 13.611, 12.778, 11.111, 11.667, 10.000, 11.111, 18.333, 17.222, 15.000, 10.417, 9.5833],  dtype=float)\n",
        "\n",
        "# আলাদাভাবে দেখি পাশাপাশি\n",
        "for i,c in enumerate(chips_15s):\n",
        "  print(\"{} Chirps in 15 Seconds = {} degrees Celsius (C)\".format(c, temp_celsius[i]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44.0 Chirps in 15 Seconds = 26.944 degrees Celsius (C)\n",
            "46.4 Chirps in 15 Seconds = 25.833 degrees Celsius (C)\n",
            "43.6 Chirps in 15 Seconds = 25.556 degrees Celsius (C)\n",
            "35.0 Chirps in 15 Seconds = 23.056 degrees Celsius (C)\n",
            "35.0 Chirps in 15 Seconds = 21.389 degrees Celsius (C)\n",
            "32.6 Chirps in 15 Seconds = 20.0 degrees Celsius (C)\n",
            "28.9 Chirps in 15 Seconds = 18.889 degrees Celsius (C)\n",
            "27.7 Chirps in 15 Seconds = 18.333 degrees Celsius (C)\n",
            "25.5 Chirps in 15 Seconds = 16.389 degrees Celsius (C)\n",
            "20.375 Chirps in 15 Seconds = 13.889 degrees Celsius (C)\n",
            "12.5 Chirps in 15 Seconds = 12.778 degrees Celsius (C)\n",
            "37.0 Chirps in 15 Seconds = 24.583 degrees Celsius (C)\n",
            "37.5 Chirps in 15 Seconds = 23.333 degrees Celsius (C)\n",
            "36.5 Chirps in 15 Seconds = 23.333 degrees Celsius (C)\n",
            "36.2 Chirps in 15 Seconds = 22.5 degrees Celsius (C)\n",
            "33.0 Chirps in 15 Seconds = 18.889 degrees Celsius (C)\n",
            "43.0 Chirps in 15 Seconds = 25.278 degrees Celsius (C)\n",
            "46.0 Chirps in 15 Seconds = 25.833 degrees Celsius (C)\n",
            "29.0 Chirps in 15 Seconds = 20.278 degrees Celsius (C)\n",
            "31.7 Chirps in 15 Seconds = 20.278 degrees Celsius (C)\n",
            "31.0 Chirps in 15 Seconds = 20.0 degrees Celsius (C)\n",
            "28.75 Chirps in 15 Seconds = 18.889 degrees Celsius (C)\n",
            "23.5 Chirps in 15 Seconds = 15.0 degrees Celsius (C)\n",
            "32.4 Chirps in 15 Seconds = 21.111 degrees Celsius (C)\n",
            "31.0 Chirps in 15 Seconds = 20.556 degrees Celsius (C)\n",
            "29.5 Chirps in 15 Seconds = 19.444 degrees Celsius (C)\n",
            "22.5 Chirps in 15 Seconds = 16.25 degrees Celsius (C)\n",
            "20.6 Chirps in 15 Seconds = 14.722 degrees Celsius (C)\n",
            "35.0 Chirps in 15 Seconds = 22.222 degrees Celsius (C)\n",
            "33.1 Chirps in 15 Seconds = 21.667 degrees Celsius (C)\n",
            "31.5 Chirps in 15 Seconds = 20.556 degrees Celsius (C)\n",
            "28.8 Chirps in 15 Seconds = 19.167 degrees Celsius (C)\n",
            "21.3 Chirps in 15 Seconds = 15.556 degrees Celsius (C)\n",
            "37.8 Chirps in 15 Seconds = 23.889 degrees Celsius (C)\n",
            "37.0 Chirps in 15 Seconds = 22.917 degrees Celsius (C)\n",
            "37.1 Chirps in 15 Seconds = 22.5 degrees Celsius (C)\n",
            "36.2 Chirps in 15 Seconds = 21.111 degrees Celsius (C)\n",
            "31.4 Chirps in 15 Seconds = 19.722 degrees Celsius (C)\n",
            "30.2 Chirps in 15 Seconds = 18.889 degrees Celsius (C)\n",
            "31.3 Chirps in 15 Seconds = 20.556 degrees Celsius (C)\n",
            "26.1 Chirps in 15 Seconds = 17.222 degrees Celsius (C)\n",
            "25.2 Chirps in 15 Seconds = 17.222 degrees Celsius (C)\n",
            "23.66 Chirps in 15 Seconds = 16.111 degrees Celsius (C)\n",
            "22.25 Chirps in 15 Seconds = 16.667 degrees Celsius (C)\n",
            "17.5 Chirps in 15 Seconds = 13.611 degrees Celsius (C)\n",
            "15.5 Chirps in 15 Seconds = 12.778 degrees Celsius (C)\n",
            "14.75 Chirps in 15 Seconds = 11.111 degrees Celsius (C)\n",
            "15.0 Chirps in 15 Seconds = 11.667 degrees Celsius (C)\n",
            "14.0 Chirps in 15 Seconds = 10.0 degrees Celsius (C)\n",
            "18.5 Chirps in 15 Seconds = 11.111 degrees Celsius (C)\n",
            "27.7 Chirps in 15 Seconds = 18.333 degrees Celsius (C)\n",
            "26.0 Chirps in 15 Seconds = 17.222 degrees Celsius (C)\n",
            "21.7 Chirps in 15 Seconds = 15.0 degrees Celsius (C)\n",
            "12.5 Chirps in 15 Seconds = 10.417 degrees Celsius (C)\n",
            "12.5 Chirps in 15 Seconds = 9.5833 degrees Celsius (C)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vZ33rAJJdnh5"
      },
      "source": [
        "## ডেটা ভিজ্যুয়ালাইজেশন\n",
        "\n",
        "১. আমরা পুরো ডাটাসেটকে এক্স এবং ওয়াই এক্সিসে প্লট করতে পারি। যেহেতু আমরা আগেই দেখেছি ঝিঁঝিঁপোকার ডাক এবং তাপমাত্রার সম্পর্কটা লিনিয়ার, সেখানে একটা 'বেস্ট ফিট লাইন' আমাদেরকে ভবিষ্যৎ যে কোনো ডাকের সংখ্যার করেসপন্ডিং তাপমাত্রা দেখাতে পারবে। এক্স অ্যাক্সিস যদি ঝিঁঝিঁপোকার ডাকের সংখ্যা হয় তাহলে ওয়াই এক্সিসে তারপর করেসপন্ডিং তাপমাত্রা পাওয়া যাবে। \n",
        "\n",
        "২. নতুন ঝিঁঝিঁপোকার ডাক এর সংখ্যা যেটা এখানে নেই, সেটাও এই প্লটিং এ এক্স এক্সিসের যে অংশটা ওয়াই এক্সিসের এর সঙ্গে স্পর্শ করেছে সেটাই প্রেডিক্টেড তাপমাত্রা হবে।\n",
        "\n",
        "তবে, এই ছবিতে আমরা সেটা না দেখিয়ে সামনে দেখানোর প্ল্যান করছি। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K4dyHUdhdLgE",
        "outputId": "fc57a42a-d4de-4805-c106-646064e78ca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = chips_15s\n",
        "y = temp_celsius\n",
        "\n",
        "plt.scatter(X, y, color='red')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD5CAYAAAAp8/5SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ3ozs5pzojczqzknejOzmnOiNzOruf8Pcr1MvQ2nO2AAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "idC7mALvkU1Z"
      },
      "source": [
        "৩. আমাদের যেহেতু ৫৫টা রেকর্ড আছে, আর রেকর্ডগুলো প্লট করলে প্রায় একটা সরল রেখা হয়, তাহলে কি একটা সরলরেখার ইকুয়েশন বের করতে পারি আমরা? সরলরেখার ইকুয়েশনে আমরা ওয়াই ভ্যালু, মানে আমাদের 'প্রেডিক্টেড' তাপমাত্রা বের করতে চাইলে, এক্স ভ্যালুতে ঝিঁঝিঁপোকার ডাক এর সংখ্যা বসালেই তো উত্তর পাবার কথা, ঠিক না? সরলরেখার ইকুয়েশন যদি y = mx + b হয় তাহলে তো 'm' যাকে আমরা স্লোপ বা ঢাল বলছি, সেটা বের করা যাবে না? এর পাশাপাশি ওয়াই ইন্টারসেপ্ট 'b' পাওয়াটা তো কঠিন কিছু না। এই পুরো ব্যাপারটা আমরা দু ভাবে করতে পারি। প্রথমটা সাধারণ অংক, ফর্মুলা থেকে, পরেরটা আমরা দেখবো পাইথন দিয়ে। কোনটাই মেশিন লার্নিং নয়।\n",
        "\n",
        "** আমরা সরল রেখার অংক মানে ইকুয়েশন (y = mx + b) নিয়ে আলাপ করছি। আগের বই \"শূন্য থেকে পাইথন মেশিন লার্নিং\" বইয়ে বড় করে আলাপ করেছিলাম আগে। **\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2yozKL-hrFUL"
      },
      "source": [
        "## এই ফর্মুলাটা নিরেট অংকে করি \n",
        "\n",
        "*কোন মেশিন লার্নিং নয় *\n",
        "\n",
        "আমাদের ৫৫টা ডেটাপয়েন্ট থেকে অংকে ফর্মুলাতে নিয়ে আসি। সরল রেখার অংক। ফর্মুলা নিচে দেখুন। \n",
        "\n",
        "আমাদের \"chips_15s\" মানে X এর সব ভ্যালুগুলোকে যোগ করি। \n",
        "\n",
        "X_sum =\n",
        "\n",
        "44.000+46.400+43.600+35.000+35.000+32.600+28.900+27.700+25.500+20.375+12.500+37.000+37.500+36.500+36.200+33.000+43.000+46.000+29.000+31.700+31.000+28.750+23.500+32.400+31.000+29.500+22.500+20.600+35.000+33.100+31.500+28.800+21.300+37.800+37.000+37.100+36.200+31.400+30.200+31.300+26.100+25.200+23.660+22.250+17.500+15.500+14.750+15.000+14.000+18.500+27.700+26.000+21.700+12.500+12.500= 1584.285\n",
        "\n",
        "এখন আমরা 'temp_celsius' মানে y ভ্যালুগুলোকে যোগ করি। \n",
        "\n",
        "y_sum =\n",
        "\n",
        "26.944+ 25.833+ 25.556+ 23.056+ 21.389+ 20.000+ 18.889+ 18.333+ 16.389+ 13.889+ 12.778+ 24.583+ 23.333+ 23.333+ 22.500+ 18.889+ 25.278+ 25.833+ 20.278+ 20.278+ 20.000+ 18.889+ 15.000+ 21.111+ 20.556+ 19.444+ 16.250+ 14.722+ 22.222+ 21.667+ 20.556+ 19.167+ 15.556+ 23.889+ 22.917+ 22.500+ 21.111+ 19.722+ 18.889+ 20.556+ 17.222+ 17.222+ 16.111+ 16.667+ 13.611+ 12.778+ 11.111+ 11.667+ 10.000+ 11.111+ 18.333+ 17.222+ 15.000+ 10.417+ 9.5833= 1030.1403\n",
        "\n",
        "এখন প্রতিটা ডেটা পয়েন্টের প্রোডাক্টকে যোগ করে ফেলি। \n",
        "\n",
        "Xy_sum =\n",
        "\n",
        "44.000*26.944+ 46.400*25.833+ 43.600*25.556+ 35.000*23.056+ 35.000*21.389+ 32.600*20.000+ 28.900*18.889+ 27.700*18.333+ 25.500*16.389+ 20.375*13.889+ 12.500*12.778+ 37.000*24.583+ 37.500*23.333+ 36.500*23.333+ 36.200*22.500+ 33.000*18.889+ 43.000*25.278+ 46.000*25.833+ 29.000*20.278+ 31.700*20.278+ 31.000*20.000+ 28.750*18.889+ 23.500*15.000+ 32.400*21.111+ 31.000*20.556+ 29.500*19.444+ 22.500*16.250+ 20.600*14.722+ 35.000*22.222+ 33.100*21.667+ 31.500*20.556+ 28.800*19.167+ 21.300*15.556+ 37.800*23.889+ 37.000*22.917+ 37.100*22.500+ 36.200*21.111+ 31.400*19.722+ 30.200*18.889+ 31.300*20.556+ 26.100*17.222+ 25.200*17.222+ 23.660*16.111+ 22.250*16.667+ 17.500*13.611+ 15.500*12.778+ 14.750*11.111+ 15.000*11.667+ 14.000*10.000+ 18.500*11.111+ 27.700*18.333+ 26.000*17.222+ 21.700*15.000+ 12.500*10.417+ 12.500*9.5833= 31775.986435\n",
        "\n",
        "এখন X এবং y ভ্যালুগুলোর যোগফলকে আলাদা আলাদা করে বর্গ করি।  \n",
        "\n",
        "X_square_sum =\n",
        "\n",
        "44.000^2+46.400^2+43.600^2+35.000^2+35.000^2+32.600^2+28.900^2+27.700^2+25.500^2+20.375^2+12.500^2+37.000^2+37.500^2+36.500^2+36.200^2+33.000^2+43.000^2+46.000^2+29.000^2+31.700^2+31.000^2+28.750^2+23.500^2+32.400^2+31.000^2+29.500^2+22.500^2+20.600^2+35.000^2+33.100^2+31.500^2+28.800^2+21.300^2+37.800^2+37.000^2+37.100^2+36.200^2+31.400^2+30.200^2+31.300^2+26.100^2+25.200^2+23.660^2+22.250^2+17.500^2+15.500^2+14.750^2+15.000^2+14.000^2+18.500^2+27.700^2+26.000^2+21.700^2+12.500^2+12.500^2= 49879.553725\n",
        "\n",
        "আগেই বলেছি, আমাদের ডেটাপয়েন্ট আছে ৫৫টা। মানে N=55. তাহলে বেস্ট ফিট লাইনের গ্রাডিয়েন্ট পাওয়া যাবে নিচের ফর্মুলা থেকে। আগের বইটা দেখতে পারেন। \n",
        "\n",
        "m = (N * Xy_sum) - (X_sum * y_sum) / (N * X_square_sum) - (X_sum * X_sum)\n",
        "\t = (55 * 31775.986435) - (1584.285 * 1030.1403) / (55 * 49879.553725) - (1584.285^2)\n",
        "\t = 0.49543811976\n",
        "\n",
        "এখন এই বেস্ট ফিট লাইনের ইন্টারসেপ্ট দরকার আমার লাইনের জন্য। সেটার ফর্মুলা;\n",
        "\n",
        "b = (X_square_sum * y_sum ) - (X_sum * Xy_sum) / (N * X_square_sum) - (X_sum * x_sum)\n",
        "   = (49879.553725 * 1030.1403) - (1584.285 * 31775.986435) / (55 * 49879.553725) -          (1584.285* 1584.285)\n",
        "   = 4.45863851637\n",
        "\n",
        "তাহলে আমাদের এই রিগ্রেশন লাইনের সরলরেখার ইক্যুয়েশন কি?\n",
        "\n",
        "y = 0.49543811976X + 4.45863851637"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sp8VdefCrFUM"
      },
      "source": [
        "অংকে তো মাথা খারাপ হয়ে গেলো। একটু পাইথনে দেখি। এখনো মেশিন লার্নিং নয় কিন্তু। একটা ফাংশন তৈরি করি best_fit_slope_and_intercept নাম দিয়ে। সেখানে m এবং b এর মান (X,y) থেকে বের করে নিয়ে আসি। কি দেখলাম? একই রেজাল্ট।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9xaBzGNQjAew",
        "outputId": "ba4bd205-de66-41be-bb4f-fe2aad8d1c89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from statistics import mean\n",
        "\n",
        "def best_fit_slope_and_intercept(X,y):\n",
        "    m = (((mean(X)*mean(y)) - mean(X*y)) /\n",
        "         ((mean(X)*mean(X)) - mean(X*X)))\n",
        "    \n",
        "    b = mean(y) - m*mean(X)\n",
        "    \n",
        "    return m, b\n",
        "\n",
        "m, b = best_fit_slope_and_intercept(X,y)\n",
        "\n",
        "print(m,b)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.49543811977958857 4.458638516454446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iKI0XlUmx5J",
        "colab_type": "text"
      },
      "source": [
        "y = mx + b এর হিসেবে \n",
        "\n",
        "y = 0.49543811977958857X + 4.458638516454446\n",
        "\n",
        "একদম কাছাকাছি। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ogQmit3drFUQ"
      },
      "source": [
        "## বেস্ট ফিট লাইন \n",
        "\n",
        "আমরা একটা রিগ্রেশন লাইন টেনে ফেলি। ধরুন ১৫ সেকেন্ডের ঝিঁঝিঁপোকার ৪১ ডাকের প্রেডিকশন কি হবে? Y এক্সিসের বরাবর রেখাটা এক্স এক্সিসের কোথায় স্পর্শ করেছে সেটা দেখলেই কিন্তু পাওয়া যাবে। \n",
        "\n",
        "আচ্ছা, লাইনটা কিভাবে টানলাম সেটা নিয়ে আলাপ নয় এখন। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rWJl7uh2kWn4",
        "outputId": "ed5cdb23-afe9-4b48-cbde-296dd988de28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# regression_line = [(m*x)+b for x in X]\n",
        "\n",
        "regression_line = []\n",
        "for x in X:\n",
        "    regression_line.append((m*x)+b)\n",
        "\n",
        "plt.scatter(X,y,color='red')\n",
        "plt.plot(X, regression_line)\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7NK48S41ynIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wwJGmDrQ0EoB"
      },
      "source": [
        "### মেশিন লার্নিং কিছু টার্মিনোলোজি \n",
        "\n",
        " - **ফিচার** — আমাদের মডেলের ইনপুট ডেটা। আমাদের এখানে একটা ভ্যালু - ১৫ সেকেন্ডে ঝিঁঝিঁপোকার ডাকের সংখ্যা। \n",
        " \n",
        " - **লেবেল** — যেই আউটপুটটা আমাদের মডেল শিখে প্রেডিক্ট করবে। আমাদের এখানে তাপমাত্রা। \n",
        " \n",
        " - **এক্সাম্পল** — ইনপুট/আউটপুট ডেটার একটা জোড়া, যা দরকার পড়ছে ট্রেনিং এর সময়। আমাদের এখানে chips_15s এবং temp_celsius অ্যারে থেকে দুটো ডেটা একটা ইনডেক্সে দেখলে (44.000,26.944) পাওয়া যাবে।  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ziWM65lRrFUX"
      },
      "source": [
        "## মেশিন লার্নিং মডেল \n",
        "\n",
        "আমরা সাইকিট-লার্ন দিয়েও এই জিনিসটা করতে পারতাম। তবে, যেহেতু আমরা ডিপ লার্নিং মানে টেন্সর-ফ্লো নিয়ে কাজ করতে চাই, সেকারণে আমাদের শুরুতে ফোকাস থাকবে একটা সাধারণ মডেলে। বিশেষ করে বোঝার ক্ষেত্রে। একটা লিনিয়ার মডেলে নিউরাল নেটওয়ার্ক দরকার নেই, বরং নন-লিনিয়ার মডেলের জন্য দরকার নিউরাল নেটওয়ার্ক। সেগুলো নিয়ে সামনে আলাপ হবে। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_4yDv1Y7fmmM"
      },
      "source": [
        "৪. আমাদের কাছে যেই ডাটা আছে, সেটাকে মেশিন লার্নিং মডেলে ঢুকিয়ে দেবো, দেখি মেশিন লার্নিং মডেল এর থেকে কোনো ফর্মুলা বের করতে পারে কিনা? পাশাপাশি আমরা একটা অজানা ভ্যালু দিয়ে দেখতে চাইবো সেখান থেকে প্রেডিক্টেড ভ্যালু জানাতে পারে কিনা? তাহলেই তো আমাদের কাজ হয়ে যায়, কি বলেন? আমরা যদি আমাদের ডিপ লার্নিং লাইব্রেরি টেন্সর-ফ্লোকে ঝিঁঝিঁ পোকার ডাক এর সংখ্যা এবং তার করেসপন্ডিং তাপমাত্রা দিয়ে দেই, তাহলে যদি সে ফর্মুলা দিতে পারে, তখন আমরা বলতে পারব আমাদের মেশিন লার্নিং মডেল ডেটা থেকে শিখেছে। যদি ডাটা থেকে নিজে নিজেই ফর্মুলা বের করতে পারে সেটাই কিন্তু শেখা, মানে মেশিন লার্নিং।\n",
        "\n",
        "** আমাদের এই মডেল তৈরি করতে ডিপ লার্নিং অথবা টেন্সর-ফ্লো লাগবে না। তবে ডিপ লার্নিং ফ্রেমওয়ার্ক নিয়ে ধারণা পাবার জন্য একটা অগভীর মানে 'শ্যালো' এক লেয়ারের, এক নিউরনের নেটওয়ার্ক বানানোর চেষ্টা করবো। **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzzdwSLzmx5Q",
        "colab_type": "text"
      },
      "source": [
        "## নিউরাল নেটওয়ার্কের ৫ ষ্টেপ লাইফ সাইকেল - কেরাস দিয়ে \n",
        "\n",
        "টেন্সর-ফ্লো ডিপ লার্নিং ফ্রেমওয়ার্ক হিসেবে একটু কমপ্লেক্স। সাইকিট-লার্ন এপিআইএর মতো এতো কম হাইপার-প্যারামিটার নিয়ে তাকে চালানো দুস্কর। যেহেতু ফ্রেমওয়ার্কটা অনেক শক্তিশালী, সেকারণে আমরা এটাকে এক্সেস করবো একটা হাই-লেভেল এপিআই দিয়ে যার কাজ হচ্ছে এই ভেতরের কমপ্লেক্সিটিকে লুকিয়ে রাখবে আমাদের কাছ থেকে। একারণে টেন্সর-ফ্লো ২.০ শুরুতেই যুক্ত করে নিয়েছে কেরাসকে। কেরাস দিয়ে ৯০%এর বেশি কাজ করা সম্ভব। \n",
        "\n",
        "আমরা পুরো জিনিসকে ৫টা ভাগে ভাগ করে ফেলেছি। \n",
        "\n",
        "১) একটা নেটওয়ার্ক ডিফাইন করা\n",
        "২) নেটওয়ার্ককে ঠিকমতো কম্পাইল করা \n",
        "৩) নেটওয়ার্ককে ঠিকমতো ফিট (ট্রেনিং) করা\n",
        "৪) নেটওয়ার্কের ইভ্য়ালুয়েশন করে দেখা\n",
        "৫) নেটওয়ার্ক থেকে প্রেডিক্ট করে দেখা\n",
        "<img src=\"https://raw.githubusercontent.com/raqueeb/TensorFlow2/master/assets/life.PNG\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "29ximwRWrFUZ"
      },
      "source": [
        "## মডেলের ধারণা \n",
        "\n",
        "এখন আমরা একটা মডেল তৈরি করতে চাই। মেশিন লার্নিং মডেল তবে জিনিসটা হবে খুবই সিম্প্লিস্টিক। একটা নিউরাল নেটওয়ার্ক। এখন কথা আসতে পারে নিউরাল নেটওয়ার্ক কি? (সামনে বিস্তারিত লিখেছি) নিউরন নেটওয়ার্ক আসলে কিছু অ্যালগরিদমের সেট যেটাকে কিছুটা তৈরি করা হয়েছে মানুষের মস্তিষ্কের নিউরাল এর ধারণার ওপর ভিত্তি করে। মানুষের মস্তিষ্কের সেই নিউরালকে আমরা নিয়ে এসেছি মেশিন লার্নিং এ। কারণ দুটোর কাজই এক। মেশিন লার্নিং এর যে নিউরাল নেটওয়ার্ক ব্যবহার করছি তার কাজ হচ্ছে বিভিন্ন লেয়ারে প্যাটার্ন ধরতে পারা। মানুষ যেমন তার হাজারো সেন্সর ডাটা আমাদের মস্তিষ্কে পাঠাচ্ছে, সেরকমভাবে নিউরাল নেটওয়ার্কের সবচেয়ে ছোট ইউনিট হচ্ছে একটা 'পারসেপ্ট্রন'। এই 'পারসেপ্ট্রন'গুলো যেভাবে প্যাটার্ন বুঝতে পারে তা সব সংখ্যার ভেক্টরে থাকে। আমাদের ইনপুট হিসেবে সেটা ছবি, শব্দ, লেখা অথবা টাইম সিরিজ হতে পারে - তবে সবকিছুকেই পাল্টে ফেলতে হবে সংখ্যায়।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cayJ4SUurFUa"
      },
      "source": [
        "## বিভিন্ন লেয়ারে ফিচার এক্সট্রাকশন \n",
        "\n",
        "সাধারণত: নিউরাল নেটওয়ার্কগুলো আমাদেরকে ডেটাকে বিভিন্ন গ্রুপে ক্লাস্টার করে দেয়। আমাদের ইনপুট ডাটার ওপর ভিত্তি করে সেটার ক্লাসিফিকেশন/রিগ্রেশন হবে শেষ লেয়ারে। ধরা যাক আমাদের কাছে বেশ কিছু 'লেবেল ছাড়া' ডাটা আছে। নিউরাল নেটওয়ার্কগুলো এই লেবেল ছাড়া ডাটাগুলোকে তাদের মধ্যে বিভিন্ন মিল/সঙ্গতি/অসঙ্গতি দেখে সেগুলোকে আলাদা করে গ্রুপিং করে সে। এরপর তাকে একটা লেবেল সহ ডাটাসেট দিলে সেই ট্রেনিং থেকে ফিচারগুলোকে 'এক্সট্রাক্ট' করতে পারে। নিউরাল নেটওয়ার্ক এর একটা বড় কাজ হচ্ছে বিভিন্ন লেয়ারে বিভিন্ন ফিচার এক্সট্রাক্ট করে সে। (সামনে বিস্তারিত আলাপ করেছি) সবশেষে একটা মানুষকে যদি ক্লাসিফাই করতে হয়, মানে চিনতে হয়, তাহলে সেটা শুরু হবে সেই পিক্সেল থেকে যার পর মানুষের মুখের একেকটা ফিচারের কোনা, মানে নাক, মুখ বানানোর জন্য যা যা ফিচার লাগবে সেগুলোকে সে এক্সট্রাক্ট করবে তার নিচের লেয়ার থেকে।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3XI4-3v7rFUb"
      },
      "source": [
        "## ছোট্ট একটা মডেল\n",
        "\n",
        "শুরুতেই একটু অংক। ডিপ লার্নিং একটা এন্ড টু এন্ড ইকোসিস্টেম, বিশেষ করে আমরা যখন টেন্সর-ফ্লো এনভায়রমেন্ট ব্যবহার করব। আমাদের এই নিউরাল নেটওয়ার্ক আউটপুটের সাথে ইনপুটের একটা চমৎকার ম্যাপিং করে। এর কাজ হচ্ছে ডাটার মধ্যে কোরিলেশন বের করা। অনেকে এইজন্য এর নাম বলে থাকেন 'ইউনিভার্সাল অ্যাপ্রক্সিমেটর'। কারণ এর কাজ হচ্ছে একটা অজানা ফাংশন f(x) = y এর ভ্যালুকে ধারণা করা, x আর y হচ্ছে তার ইনপুট এবং আউটপুট। আগের বইয়ে আলাপ করা হয়েছে। মেশিন লার্নিং এর এই শেখার প্রসেসে, নিউরাল নেটওয়ার্ক খুঁজে পায় তার আসল ফাংশন, যেটাকে আমরা বলতে পারি একটা প্রসেস যা x থেকে থেকে ইনপুট নিয়ে আপডেট করবে y কে। \n",
        "\n",
        "উদাহরণ কি দেব একটা? f(x) = 2x + 9 = y এর মত অসংখ্য উদাহরণ নিয়ে আলাপ করব সামনে।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VM7_9Klvq7MO"
      },
      "source": [
        "## হাতেকলমে মডেল তৈরি \n",
        "\n",
        "যেহেতু আমরা একটা খুবই সহজ মডেল তৈরি করছি সেটাকে ডিপ লার্নিং এর ভাষায় আমরা বলছি ‘ডেন্স’ নেটওয়ার্ক। মানে একটার সাথে আরেকটার সরাসরি কানেকশন। গায়ে গায়ে লেগে থাকা। নেটওয়ার্কের একেকটা পারসেপ্ট্রন আরেকটার সাথে সম্পূর্ণভাবে কানেক্টেড থাকে। এর জন্যই একে বলা হচ্ছে 'ডেন্স'। 'ডেন্স'লি কানেক্টেড। (সামনে বিস্তারিত বলেছি) ‘ডেন্স’এর মডেল একেকটার সাথে স্ট্যাক করে কানেক্টেড মডেল। সেখানে আমরা ব্যবহার করবো \"সিকোয়েন্সিয়াল\", মানে একটার পর আরেকটা।\n",
        "\n",
        "নিউরাল নেটওয়ার্ক তৈরি করতে আমাদের এটার লেয়ারগুলোর কনফিগারেশন বলতে হবে। লেয়ার ঠিকমতো কনফিগার করা হলে আমরা মডেলকে কম্পাইল করব। আগের ছবিতেও দেখেছেন এই জিনিসটা। যেহেতু আমাদের সমস্যা একটা লিনিয়ার রিগ্রেশন এর মত, মানে একটা সরল রেখার ইকুয়েশনের মত, সে কারণে আমাদের নেটওয়ার্কে একটা লেয়ার প্রয়োজন। সঙ্গে একটা নিউরন। সামনে দরকার মতো আমরা আরো কিছু লেয়ার নিয়ে আলাপ করব।\n",
        "\n",
        "## লেয়ারের কনফিগারেশন\n",
        "\n",
        "একটা নিউরাল নেটওয়ার্কের বেসিক বিল্ডিং ব্লক হচ্ছে লেয়ার। লেয়ারগুলোতে যে ডেটা ফিড করানো হয় সেখান থেকে সে ডেটার রিপ্রেজেন্টেশনগুলোকে ঠিকমতো এক্সট্রাক্ট করে নিয়ে নেয় একেকটা লেয়ারে। বেশিরভাগ ডিপ লার্নিং মডেলগুলোর লেয়ার একটার সাথে আরেকটার কানেকশন ডেটাগুলোকে ঠিকমতো বুঝতে সাহায্য করে (ভিন্ন চ্যাপ্টার আছে সামনে)। যেহেতু আমাদের সমস্যাটা খুবই সহজ সে কারণে কিন্তু আমাদের এই নেটওয়ার্কে একটা লেয়ার হলেই চলবে। এই একটা লেয়ারে আমরা একটা নিউরন চালাবো।\n",
        "\n",
        "## তৈরি করি একটা লেয়ার\n",
        "\n",
        "শুরুতেই লেয়ারটার নাম দিয়ে দিচ্ছি `l0`। মনে আছে scikit-learn এর কথা? সেই একইভাবে আমরা টেন্সর-ফ্লো একটা এপিআই কল করব আমাদের এই 'ডেন্স' লেয়ারের জন্য। যেহেতু আমরা সরাসরি টেন্সর-ফ্লো এর সাথে কথা বলতে চাইবো না এই মুহূর্তে, অবশ্যই সেটা বেশ কমপ্লেক্স, তাই একটা হাই লেভেল হেল্পার এপিআই দিয়ে এক্সেস করব নিচের কঠিন টেন্সর-ফ্লোকে। এখন আমরা শুধু মনে রাখি 'কেরাস' হচ্ছে আমাদের সেই হাই-লেভেল এপিআই যা টেন্সর-ফ্লো এর কম্প্লেক্সিটি লুকিয়ে রাখে। সবচেয়ে বড় কথা হচ্ছে টেন্সর-ফ্লো ২.০ এর সঙ্গে ইন-বিল্ট এসেছে এই 'কেরাস এপিআই'।\n",
        "\n",
        "## একটা লেয়ার তৈরি করি \n",
        "\n",
        "আমাদের লেয়ার `l0` মানে লেয়ার জিরো। এটাকে তৈরি করছি `tf.keras.layers.Dense`কে ইন্সট্যান্সিয়েট করে। ইন্সট্যান্সিয়েট ব্যাপারটা আমরা আলাপ করেছি সাইকিট-লার্নের আগের বইতে। নিচের কনফিগারেশনগুলো দেখি।\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/raqueeb/TensorFlow2/master/assets/nn1.png\">\n",
        "\n",
        "এখানে একটা ইনপুট, একটা লেয়ার, একটা নিউরন, এবং একটা আউটপুট। ছবি দেখুন। ইনপুটে একটা ভ্যালু, input_shape=[1], এটা একটা ১ ডাইমেনশনের একটা সংখ্যা।\n",
        "\n",
        "*   `input_shape=[1]` — Input এর অর্থ হচ্ছে ইনপুট লেয়ার এ একটা ভ্যালু সে এক্সপেক্ট করছে। সিঙ্গেল ভ্যালু। সেই হিসেবে আমরা এটাকে বলছি এক ডাইমেনশনের একটা অ্যারে যার একটাই সদস্য। যেহেতু এটা এই মডেলের প্রথম এবং একটাই লেয়ার সে কারণে এই ইনপুট শেপ হচ্ছে পুরো মডেলিং ইনপুট শেপ। আমাদের এই সিঙ্গেল ভ্যালু হচ্ছে একটা ফ্লোটিং পয়েন্ট সংখ্যা, যা আসলে 15 সেকেন্ডে ঝিঁঝিঁপোকার ডাকের সংখ্যা।\n",
        "\n",
        "*   `units=1` — এই সংখ্যা দিয়ে আমরা বোঝাতে চাচ্ছি কতগুলো নিউরন হবে ওই লেয়ারে। আমাদের এই নিউরনের সংখ্যা বলে দেয় কতগুলো ইন্টারনাল ভ্যারিয়েবল সেই লেয়ারকে চেষ্টা করতে হবে শিখতে সমস্যাটা সমাধান করতে। সেটা নির্ভর করছে কতগুলো ইনপুট যুক্ত আছে সেই নিউরনের সাথে। মিতু এটাই এই মডেলের সব শেষ লেয়ার, আমাদের মডেলের আউটপুটের সাইজও কিন্তু ১। সারাদিন আউটপুট হচ্ছে একটা সিঙ্গেল ফ্লোট ভ্যালু ডিগ্রী সেলসিয়াস। ঝিঝি পোকা কত ডাক দিলে তার করেসপন্ডিং তাপমাত্রা। এটা যখন কয়েকটা লেয়ারের নেটওয়ার্ক হবে তাহলে সেই লেয়ারের সাইজ এবং সেপ একি হতে হবে পরবর্তী লেয়ারের input_shape।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pRllo2HLfXiu",
        "colab": {}
      },
      "source": [
        "l0 = tf.keras.layers.Dense(units=1, input_shape=[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_F00_J9duLBD"
      },
      "source": [
        "### লেয়ারগুলোকে মডেলে অ্যাসেম্বল করা\n",
        "\n",
        "যখন আমরা লেয়ারগুলোকে ডিফাইন করে ফেললাম এখন তাদেরকে মডেল এ যোগ করার পালা। আমরা যখন সিকুয়েন্সিয়াল মডেলকে আমাদের লেয়ারের আর্গুমেন্ট চেপে ধরে নেব তখন একটার পর আরেকটা লেয়ার কানেক্টেড থাকবে যেভাবে আমরা একটা থেকে আরেকটা লিস্ট করতে থাকবো। এখানে আমরা কেরাসের \"সিকোয়েন্সিয়াল\" ক্লাস ব্যবহার করছি। \n",
        "\n",
        "কোরাসে নিউরাল নেটওয়ার্কগুলো ডিফাইন করা থাকে লেয়ারের সিকোয়েন্স হিসেবে। এই লেয়ারের কনটেইনার হচ্ছে \"সিকোয়েন্সিয়াল\" ক্লাস। \n",
        "\n",
        "এই মডেলটার কিন্তু একটাই লেয়ার l0 ।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cSp-GpLSuMRq",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([l0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t7pfHfWxust0"
      },
      "source": [
        "**আরেক ভাবেও করা যায়**\n",
        "\n",
        "আমরা যদি আগে থেকে মডেলকে ডিফাইন করি, তাহলে লেয়ারগুলোকে ভেতরে ফেলা যায়। এটা একটা ভালো প্রোগ্রামিং স্টাইল। \n",
        "\n",
        "```python\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kiZG7uhm8qCF"
      },
      "source": [
        "## মডেল কম্পাইলেশন, সঙ্গে থাকছে লস এবং অপটিমাইজার ফাংশন\n",
        "\n",
        "ট্রেনিং এর আগে আমাদের মডেলকে কম্পাইল করে নিতে হবে। মডেলকে কম্পাইল করতে গেলে আমাদের নিচের দুটো ফাংশন কে ডিফাইন করতে হবে:\n",
        "\n",
        "- **লস ফাংশন** — এটা নিয়ে আমরা আগের বইতে আলাপ করেছিলাম। আমাদের যেটা আউটকাম আসার কথা সেখান থেকে প্রেডিকশন কত দূরে? আমাদের কাজ হচ্ছে এই দূরত্বটাকে ঠিকমতো মাপা। এই দুটোর মাঝখানে যে দূরত্ব সেটা কি আমরা লস বলছি। মনে আছে আমরা এর আগে কিভাবে 'মিন স্কোয়ারড এরর' বের করেছিলাম, আগের বইতে? অংকে।\n",
        "\n",
        "- **অপটিমাইজার ফাংশন** — আমাদের নিউরাল নেটওয়ার্কের যে ইন্টার্নাল ভ্যালুগুলো আছে সেগুলোকে কমিয়ে আনার জন্য এই ফাংশন। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m8YQN1H41L-Y",
        "colab": {}
      },
      "source": [
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(0.1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "17M3Pqv4P52R"
      },
      "source": [
        "এই ফাংশনগুলো সাধারণত ব্যবহার হয় ট্রেনিং এর সময়। দুটোর কাজ কিন্তু প্রাসঙ্গিক। প্রথমটার কাজ হচ্ছে প্রতিটা পয়েন্টে কত লস হচ্ছে সেটা বের করা, পরেরটা সেটাকে ধরে সেই লসকে কমিয়ে নিয়ে আসে। এই যে আমরা প্রতিটা পয়েন্টে লস ক্যালকুলেট করি, মানে ট্রেনিং ডেটার আসল ‘আউটকাম’ থেকে প্রেডিকশন কত দূরে আছে, আর সেই দুটোকে কমিয়ে নিয়ে আসতে যা কাজ করতে হয় এই দুটো ফাংশনকে - সেটাকে আমরা ট্রেনিং বলতে পারি।\n",
        "\n",
        "আমাদের এই ট্রেনিং এর সময় \"`model.fit()`\" ‘অপটিমাইজার’ ফাংশন মডেলের যে ইন্টারনাল ভ্যারিয়েবলগুলো (ওয়েট) আছে সেগুলোর মধ্যে দূরত্বকে কমিয়ে আনার জন্য যা যা এডজাস্টমেন্ট দরকার সেগুলো করে সে। সে ততক্ষণ এই কাজ করতে থাকে (ওই ইন্টারনাল ভেরিয়েবলগুলোর মধ্যে যা যা এডজাস্টমেন্ট দরকার) যতক্ষণ পর্যন্ত আমাদের মডেলের ভেতরের আসল ইকুয়েশনের সমান না হয়। আমাদের ইকুয়েশন হচ্ছে ১৫ সেকেন্ডে ঝিঝি পোকার ডাকের সংখ্যার সাথে ওই সময়ের তাপমাত্রার একটা সম্পর্ক বের করা। এটা বের করতে পারলে আমাদের কাজ হাসিল।\n",
        "\n",
        "আমরা এতক্ষণ যে ইন্টারনাল ভেরিয়েবলগুলোর কথা বললাম সেগুলো কিন্তু ইনপুট এর সাথে তার ‘করেসপন্ডিং’ ‘ওয়েট’। এগুলো নিয়ে আমরা নিউরাল নেটওয়ার্কের ‘পারসেপট্রন’ নিয়ে যখন আলাপ করব তখন বোঝা যাবে। আমাদের ডিপ লার্নিং ফ্রেমওয়ার্ক টেন্সর-ফ্লো এর ব্যাকএন্ডে কিছু বড় বড় অংকের অ্যানালাইসিস করে এই এডজাস্টমেন্ট টিউনিং করার জন্য। এর ব্যাকএন্ডে যে অংকটা আছে সেটাকে আমরা বলছি ‘গ্রেডিয়েন্ট ডিসেন্ট’। সেটা নিয়েও আলাপ করব সামনে।\n",
        "\n",
        "আপনি যদি ভালোভাবে দেখেন আমাদের এখানে যে লস ফাংশন ব্যবহার করেছি সেটা হচ্ছে ‘মিন স্কোয়ারড এরর’। সাইকিট-লার্ন বইটাতে এটা নিয়ে বেশ বড় একটা অংক করেছিলাম। পাশাপাশি ‘অপটিমাইজার’ এর ক্ষেত্রে ‘অ্যাডাম’ ব্যবহার করেছি যা আসলে এ ধরনের মডেলের জন্য ভালোভাবেই কাজ করে। এটা একটা লেটেস্ট ট্রেন্ড, সুন্দর কাজ করে। ‘অ্যাডাম’ মানে এডাপ্টিভ মোমেন্ট এস্টিমেশন। তবে এছাড়াও আমরা অন্যান্য প্যারামিটারগুলো নিয়েও সামনে আলাপ করব।\n",
        "\n",
        "‘অপটিমাইজার’ এর ভেতরের আরেকটা অংশ নিয়ে এখনই আলাপ করলে ব্যাপারটা সামনে সহজ হয়ে যাবে। এখানে দেখুন আমরা 0.1 যে সংখ্যাটা ব্যবহার করেছি, সেটা আসলে ‘লার্নিং রেট’। কি হারে মডেল শিখছে। এটা মডেলের ভেতরে যখন ইন্টারনাল ভেরিয়েবল বা ওয়েট নিজেদের মধ্যে এডজাস্ট করে সেটা একটা স্টেপ সাইজ ধরে করে। হাটিহাটি পা পা করে। পাহাড় থেকে নামার মতো। এই সংখ্যাটা যদি খুব কম হয় তাহলে একটা মডেল ট্রেইন করতে অনেক বেশি ‘আইটারেশন’ লাগবে। আবার সংখ্যাটা বড় হলে মডেলের অ্যাক্যুরেসি কমে যাবে। তাহলে মধ্য়পন্থা। হাতে কলমের একটা বড় মজা হচ্ছে হাইপার প্যারামিটারগুলোকে টিউন করার জন্য আমাদের ভ্যালুগুলো দিয়ে কিছুটা ট্রায়াল দিতে হবে। তবে ইন্ডাস্ট্রি স্ট্যান্ডার্ড অনুযায়ী এটার একটা ডিফল্ট ভ্যালু আছে যা ০.০০১ থেকে ০.১ পর্যন্ত ভালো কাজ করে।"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c-Jk4dG91dvD"
      },
      "source": [
        "## মডেলের ট্রেনিং\n",
        "\n",
        "Scikit-learn এর মত এখানেও আমরা মডেলকে ট্রেইন করবো ফিট \"`fit`\" মেথডকে কল করে। ট্রেনিং এর সময় আমাদের মডেল ১৫ সেকেন্ডের ঝিঁঝিঁ পোকার ডাক এর সংখ্যার সাথে বর্তমান যে ইন্টার্নাল ভেরিয়েবলগুলো আছে (যাকে আমরা বলছি ওয়েট) তাদেরকে ব্যবহার করে এবং তার আউটপুট ভ্যালু যেটাকে আমরা বলছি বর্তমান তাপমাত্রা - এ দুটোর মধ্যে একটা ক্যালকুলেশন করে এদের মধ্যে রিলেশনশিপ এডজাস্ট করতে থাকে। যেহেতু শুরুতেই এই ওয়েটগুলোকে দৈব চয়নের ভিত্তিতে সেট করা হয় সে কারণে শুরুর দিকে তাদের আউটপুট আসল ভ্যালুর কাছাকাছি না আসার সম্ভাবনা বেশি। সে কারণে আমাদের যে আসল আউটপুট (ট্রেনিং ডেটা থেকে) আর যে আউটপুটটা এখন ক্যালকুলেট করা হলো লস ফাংশন দিয়ে, সেটাকে অপটিমাইজার ফাংশন পই পই করে বলে দেয় কিভাবে ওয়েটগুলোকে এডজাস্ট করবে সামনে।\n",
        "\n",
        "এই পুরো ব্যাপারটা মানে পুরো সাইকেলকে আমরা বলছি ১. একটা ক্যালকুলেশন, ২. তার সঙ্গে মিলিয়ে দেখা, ৩. এর পাশাপাশি ওয়েটগুলোর যে এডজাস্টমেন্ট এই তিনটা জিনিসকে ম্যানেজ করে আমাদের এই ফিট মেথড। Scikit-learn এর মত একই ধরনের আর্গুমেন্ট তবে সঙ্গে আরো কয়েকটা এলিমেন্ট এসেছে নতুন করে। আমাদের প্রথম আর্গুমেন্ট হচ্ছে ইনপুট আর পরের আর্গুমেন্টটা হচ্ছে আমরা যেটা পেতে চাই। একদম scikit-learn। এরপরের আর্গুমেন্ট হচ্ছে ইপক (epochs), মানে পুরো ট্রেনিং ডাটা কতবার পুরোপুরি আগা থেকে গোড়া পর্যন্ত চালাবে, শেষে হচ্ছে ভার্বস (verbose) আর্গুমেন্ট যেটা নির্ধারণ করে আমাদের আউটপুটে বাড়তি ইনফরমেশন দিবে কি দিবেনা।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lpRrl7WK10Pq",
        "outputId": "42a26d27-0e60-4401-953a-f2e3f22a45d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# X = chips_15s\n",
        "# y = temp_celsius\n",
        "\n",
        "# history = model.fit(chips_15s, temp_celsius, epochs=500, verbose=True)\n",
        "history = model.fit(X, y, epochs=500, verbose=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 55 samples\n",
            "Epoch 1/500\n",
            "55/55 [==============================] - 2s 36ms/sample - loss: 4047.1307\n",
            "Epoch 2/500\n",
            "55/55 [==============================] - 0s 140us/sample - loss: 3274.3914\n",
            "Epoch 3/500\n",
            "55/55 [==============================] - 0s 161us/sample - loss: 2634.7248\n",
            "Epoch 4/500\n",
            "55/55 [==============================] - 0s 131us/sample - loss: 2043.6359\n",
            "Epoch 5/500\n",
            "55/55 [==============================] - 0s 154us/sample - loss: 1544.0610\n",
            "Epoch 6/500\n",
            "55/55 [==============================] - 0s 167us/sample - loss: 1117.1716\n",
            "Epoch 7/500\n",
            "55/55 [==============================] - 0s 169us/sample - loss: 778.5863\n",
            "Epoch 8/500\n",
            "55/55 [==============================] - 0s 155us/sample - loss: 497.9578\n",
            "Epoch 9/500\n",
            "55/55 [==============================] - 0s 167us/sample - loss: 302.0412\n",
            "Epoch 10/500\n",
            "55/55 [==============================] - 0s 158us/sample - loss: 161.9794\n",
            "Epoch 11/500\n",
            "55/55 [==============================] - 0s 161us/sample - loss: 74.5582\n",
            "Epoch 12/500\n",
            "55/55 [==============================] - 0s 147us/sample - loss: 23.6584\n",
            "Epoch 13/500\n",
            "55/55 [==============================] - 0s 129us/sample - loss: 3.2971\n",
            "Epoch 14/500\n",
            "55/55 [==============================] - 0s 112us/sample - loss: 3.8596\n",
            "Epoch 15/500\n",
            "55/55 [==============================] - 0s 106us/sample - loss: 15.9007\n",
            "Epoch 16/500\n",
            "55/55 [==============================] - 0s 100us/sample - loss: 32.1232\n",
            "Epoch 17/500\n",
            "55/55 [==============================] - 0s 130us/sample - loss: 47.1600\n",
            "Epoch 18/500\n",
            "55/55 [==============================] - 0s 141us/sample - loss: 57.8819\n",
            "Epoch 19/500\n",
            "55/55 [==============================] - 0s 126us/sample - loss: 62.9485\n",
            "Epoch 20/500\n",
            "55/55 [==============================] - 0s 128us/sample - loss: 62.0691\n",
            "Epoch 21/500\n",
            "55/55 [==============================] - 0s 130us/sample - loss: 56.1761\n",
            "Epoch 22/500\n",
            "55/55 [==============================] - 0s 145us/sample - loss: 47.4685\n",
            "Epoch 23/500\n",
            "55/55 [==============================] - 0s 155us/sample - loss: 37.3952\n",
            "Epoch 24/500\n",
            "55/55 [==============================] - 0s 157us/sample - loss: 26.8025\n",
            "Epoch 25/500\n",
            "55/55 [==============================] - 0s 185us/sample - loss: 17.7664\n",
            "Epoch 26/500\n",
            "55/55 [==============================] - 0s 147us/sample - loss: 10.4587\n",
            "Epoch 27/500\n",
            "55/55 [==============================] - 0s 153us/sample - loss: 5.6765\n",
            "Epoch 28/500\n",
            "55/55 [==============================] - 0s 130us/sample - loss: 2.5226\n",
            "Epoch 29/500\n",
            "55/55 [==============================] - 0s 135us/sample - loss: 1.4136\n",
            "Epoch 30/500\n",
            "55/55 [==============================] - 0s 220us/sample - loss: 1.3471\n",
            "Epoch 31/500\n",
            "55/55 [==============================] - 0s 236us/sample - loss: 2.0276\n",
            "Epoch 32/500\n",
            "55/55 [==============================] - 0s 208us/sample - loss: 2.8290\n",
            "Epoch 33/500\n",
            "55/55 [==============================] - 0s 243us/sample - loss: 3.5474\n",
            "Epoch 34/500\n",
            "55/55 [==============================] - 0s 194us/sample - loss: 3.9861\n",
            "Epoch 35/500\n",
            "55/55 [==============================] - 0s 215us/sample - loss: 4.0466\n",
            "Epoch 36/500\n",
            "55/55 [==============================] - 0s 185us/sample - loss: 3.7982\n",
            "Epoch 37/500\n",
            "55/55 [==============================] - 0s 166us/sample - loss: 3.3394\n",
            "Epoch 38/500\n",
            "55/55 [==============================] - 0s 232us/sample - loss: 2.7839\n",
            "Epoch 39/500\n",
            "55/55 [==============================] - 0s 224us/sample - loss: 2.2991\n",
            "Epoch 40/500\n",
            "55/55 [==============================] - 0s 204us/sample - loss: 1.8071\n",
            "Epoch 41/500\n",
            "55/55 [==============================] - 0s 202us/sample - loss: 1.4986\n",
            "Epoch 42/500\n",
            "55/55 [==============================] - 0s 188us/sample - loss: 1.2965\n",
            "Epoch 43/500\n",
            "55/55 [==============================] - 0s 183us/sample - loss: 1.2177\n",
            "Epoch 44/500\n",
            "55/55 [==============================] - 0s 243us/sample - loss: 1.2417\n",
            "Epoch 45/500\n",
            "55/55 [==============================] - 0s 194us/sample - loss: 1.2792\n",
            "Epoch 46/500\n",
            "55/55 [==============================] - 0s 164us/sample - loss: 1.3373\n",
            "Epoch 47/500\n",
            "55/55 [==============================] - 0s 207us/sample - loss: 1.3948\n",
            "Epoch 48/500\n",
            "55/55 [==============================] - 0s 179us/sample - loss: 1.4081\n",
            "Epoch 49/500\n",
            "55/55 [==============================] - 0s 176us/sample - loss: 1.3977\n",
            "Epoch 50/500\n",
            "55/55 [==============================] - 0s 163us/sample - loss: 1.3643\n",
            "Epoch 51/500\n",
            "55/55 [==============================] - 0s 209us/sample - loss: 1.3296\n",
            "Epoch 52/500\n",
            "55/55 [==============================] - 0s 182us/sample - loss: 1.2837\n",
            "Epoch 53/500\n",
            "55/55 [==============================] - 0s 220us/sample - loss: 1.2576\n",
            "Epoch 54/500\n",
            "55/55 [==============================] - 0s 179us/sample - loss: 1.2417\n",
            "Epoch 55/500\n",
            "55/55 [==============================] - 0s 235us/sample - loss: 1.2145\n",
            "Epoch 56/500\n",
            "55/55 [==============================] - 0s 142us/sample - loss: 1.2188\n",
            "Epoch 57/500\n",
            "55/55 [==============================] - 0s 234us/sample - loss: 1.2129\n",
            "Epoch 58/500\n",
            "55/55 [==============================] - 0s 219us/sample - loss: 1.2216\n",
            "Epoch 59/500\n",
            "55/55 [==============================] - 0s 195us/sample - loss: 1.2282\n",
            "Epoch 60/500\n",
            "55/55 [==============================] - 0s 199us/sample - loss: 1.2289\n",
            "Epoch 61/500\n",
            "55/55 [==============================] - 0s 142us/sample - loss: 1.2276\n",
            "Epoch 62/500\n",
            "55/55 [==============================] - 0s 148us/sample - loss: 1.2225\n",
            "Epoch 63/500\n",
            "55/55 [==============================] - 0s 128us/sample - loss: 1.2179\n",
            "Epoch 64/500\n",
            "55/55 [==============================] - 0s 142us/sample - loss: 1.2131\n",
            "Epoch 65/500\n",
            "55/55 [==============================] - 0s 167us/sample - loss: 1.2076\n",
            "Epoch 66/500\n",
            "55/55 [==============================] - 0s 140us/sample - loss: 1.2049\n",
            "Epoch 67/500\n",
            "55/55 [==============================] - 0s 131us/sample - loss: 1.2097\n",
            "Epoch 68/500\n",
            "55/55 [==============================] - 0s 167us/sample - loss: 1.2067\n",
            "Epoch 69/500\n",
            "55/55 [==============================] - 0s 240us/sample - loss: 1.2073\n",
            "Epoch 70/500\n",
            "55/55 [==============================] - 0s 161us/sample - loss: 1.2065\n",
            "Epoch 71/500\n",
            "55/55 [==============================] - 0s 303us/sample - loss: 1.2061\n",
            "Epoch 72/500\n",
            "55/55 [==============================] - 0s 170us/sample - loss: 1.2038\n",
            "Epoch 73/500\n",
            "55/55 [==============================] - 0s 191us/sample - loss: 1.2025\n",
            "Epoch 74/500\n",
            "55/55 [==============================] - 0s 186us/sample - loss: 1.2011\n",
            "Epoch 75/500\n",
            "55/55 [==============================] - 0s 183us/sample - loss: 1.1993\n",
            "Epoch 76/500\n",
            "55/55 [==============================] - 0s 124us/sample - loss: 1.1976\n",
            "Epoch 77/500\n",
            "55/55 [==============================] - 0s 120us/sample - loss: 1.1956\n",
            "Epoch 78/500\n",
            "55/55 [==============================] - 0s 133us/sample - loss: 1.1953\n",
            "Epoch 79/500\n",
            "55/55 [==============================] - 0s 177us/sample - loss: 1.1959\n",
            "Epoch 80/500\n",
            "55/55 [==============================] - 0s 179us/sample - loss: 1.1936\n",
            "Epoch 81/500\n",
            "55/55 [==============================] - 0s 184us/sample - loss: 1.1929\n",
            "Epoch 82/500\n",
            "55/55 [==============================] - 0s 188us/sample - loss: 1.1921\n",
            "Epoch 83/500\n",
            "55/55 [==============================] - 0s 215us/sample - loss: 1.1925\n",
            "Epoch 84/500\n",
            "55/55 [==============================] - 0s 191us/sample - loss: 1.1899\n",
            "Epoch 85/500\n",
            "55/55 [==============================] - 0s 138us/sample - loss: 1.1891\n",
            "Epoch 86/500\n",
            "55/55 [==============================] - 0s 210us/sample - loss: 1.1877\n",
            "Epoch 87/500\n",
            "55/55 [==============================] - 0s 182us/sample - loss: 1.1867\n",
            "Epoch 88/500\n",
            "55/55 [==============================] - 0s 204us/sample - loss: 1.1847\n",
            "Epoch 89/500\n",
            "55/55 [==============================] - 0s 165us/sample - loss: 1.1837\n",
            "Epoch 90/500\n",
            "55/55 [==============================] - 0s 188us/sample - loss: 1.1825\n",
            "Epoch 91/500\n",
            "55/55 [==============================] - 0s 219us/sample - loss: 1.1818\n",
            "Epoch 92/500\n",
            "55/55 [==============================] - 0s 192us/sample - loss: 1.1805\n",
            "Epoch 93/500\n",
            "55/55 [==============================] - 0s 180us/sample - loss: 1.1796\n",
            "Epoch 94/500\n",
            "55/55 [==============================] - 0s 210us/sample - loss: 1.1784\n",
            "Epoch 95/500\n",
            "55/55 [==============================] - 0s 190us/sample - loss: 1.1769\n",
            "Epoch 96/500\n",
            "55/55 [==============================] - 0s 148us/sample - loss: 1.1783\n",
            "Epoch 97/500\n",
            "55/55 [==============================] - 0s 164us/sample - loss: 1.1748\n",
            "Epoch 98/500\n",
            "55/55 [==============================] - 0s 157us/sample - loss: 1.1740\n",
            "Epoch 99/500\n",
            "55/55 [==============================] - 0s 170us/sample - loss: 1.1725\n",
            "Epoch 100/500\n",
            "55/55 [==============================] - 0s 175us/sample - loss: 1.1716\n",
            "Epoch 101/500\n",
            "55/55 [==============================] - 0s 221us/sample - loss: 1.1706\n",
            "Epoch 102/500\n",
            "55/55 [==============================] - 0s 204us/sample - loss: 1.1698\n",
            "Epoch 103/500\n",
            "55/55 [==============================] - 0s 224us/sample - loss: 1.1692\n",
            "Epoch 104/500\n",
            "55/55 [==============================] - 0s 181us/sample - loss: 1.1682\n",
            "Epoch 105/500\n",
            "55/55 [==============================] - 0s 192us/sample - loss: 1.1666\n",
            "Epoch 106/500\n",
            "55/55 [==============================] - 0s 179us/sample - loss: 1.1652\n",
            "Epoch 107/500\n",
            "55/55 [==============================] - 0s 179us/sample - loss: 1.1641\n",
            "Epoch 108/500\n",
            "55/55 [==============================] - 0s 146us/sample - loss: 1.1633\n",
            "Epoch 109/500\n",
            "55/55 [==============================] - 0s 191us/sample - loss: 1.1627\n",
            "Epoch 110/500\n",
            "55/55 [==============================] - 0s 188us/sample - loss: 1.1609\n",
            "Epoch 111/500\n",
            "55/55 [==============================] - 0s 301us/sample - loss: 1.1593\n",
            "Epoch 112/500\n",
            "55/55 [==============================] - 0s 188us/sample - loss: 1.1582\n",
            "Epoch 113/500\n",
            "55/55 [==============================] - 0s 169us/sample - loss: 1.1579\n",
            "Epoch 114/500\n",
            "55/55 [==============================] - 0s 177us/sample - loss: 1.1563\n",
            "Epoch 115/500\n",
            "55/55 [==============================] - 0s 200us/sample - loss: 1.1551\n",
            "Epoch 116/500\n",
            "55/55 [==============================] - 0s 143us/sample - loss: 1.1542\n",
            "Epoch 117/500\n",
            "55/55 [==============================] - 0s 164us/sample - loss: 1.1531\n",
            "Epoch 118/500\n",
            "55/55 [==============================] - 0s 213us/sample - loss: 1.1513\n",
            "Epoch 119/500\n",
            "55/55 [==============================] - 0s 152us/sample - loss: 1.1501\n",
            "Epoch 120/500\n",
            "55/55 [==============================] - 0s 183us/sample - loss: 1.1505\n",
            "Epoch 121/500\n",
            "55/55 [==============================] - 0s 164us/sample - loss: 1.1478\n",
            "Epoch 122/500\n",
            "55/55 [==============================] - 0s 150us/sample - loss: 1.1468\n",
            "Epoch 123/500\n",
            "55/55 [==============================] - 0s 186us/sample - loss: 1.1462\n",
            "Epoch 124/500\n",
            "55/55 [==============================] - 0s 191us/sample - loss: 1.1445\n",
            "Epoch 125/500\n",
            "55/55 [==============================] - 0s 168us/sample - loss: 1.1432\n",
            "Epoch 126/500\n",
            "55/55 [==============================] - 0s 237us/sample - loss: 1.1420\n",
            "Epoch 127/500\n",
            "55/55 [==============================] - 0s 190us/sample - loss: 1.1406\n",
            "Epoch 128/500\n",
            "55/55 [==============================] - 0s 151us/sample - loss: 1.1407\n",
            "Epoch 129/500\n",
            "55/55 [==============================] - 0s 224us/sample - loss: 1.1384\n",
            "Epoch 130/500\n",
            "55/55 [==============================] - 0s 172us/sample - loss: 1.1390\n",
            "Epoch 131/500\n",
            "55/55 [==============================] - 0s 267us/sample - loss: 1.1354\n",
            "Epoch 132/500\n",
            "55/55 [==============================] - 0s 187us/sample - loss: 1.1348\n",
            "Epoch 133/500\n",
            "55/55 [==============================] - 0s 175us/sample - loss: 1.1338\n",
            "Epoch 134/500\n",
            "55/55 [==============================] - 0s 168us/sample - loss: 1.1331\n",
            "Epoch 135/500\n",
            "55/55 [==============================] - 0s 192us/sample - loss: 1.1326\n",
            "Epoch 136/500\n",
            "55/55 [==============================] - 0s 170us/sample - loss: 1.1315\n",
            "Epoch 137/500\n",
            "55/55 [==============================] - 0s 182us/sample - loss: 1.1300\n",
            "Epoch 138/500\n",
            "55/55 [==============================] - 0s 178us/sample - loss: 1.1281\n",
            "Epoch 139/500\n",
            "55/55 [==============================] - 0s 143us/sample - loss: 1.1268\n",
            "Epoch 140/500\n",
            "55/55 [==============================] - 0s 187us/sample - loss: 1.1254\n",
            "Epoch 141/500\n",
            "55/55 [==============================] - 0s 157us/sample - loss: 1.1241\n",
            "Epoch 142/500\n",
            "55/55 [==============================] - 0s 158us/sample - loss: 1.1233\n",
            "Epoch 143/500\n",
            "55/55 [==============================] - 0s 162us/sample - loss: 1.1229\n",
            "Epoch 144/500\n",
            "55/55 [==============================] - 0s 145us/sample - loss: 1.1214\n",
            "Epoch 145/500\n",
            "55/55 [==============================] - 0s 122us/sample - loss: 1.1196\n",
            "Epoch 146/500\n",
            "55/55 [==============================] - 0s 137us/sample - loss: 1.1182\n",
            "Epoch 147/500\n",
            "55/55 [==============================] - 0s 141us/sample - loss: 1.1174\n",
            "Epoch 148/500\n",
            "55/55 [==============================] - 0s 178us/sample - loss: 1.1171\n",
            "Epoch 149/500\n",
            "55/55 [==============================] - 0s 182us/sample - loss: 1.1142\n",
            "Epoch 150/500\n",
            "55/55 [==============================] - 0s 173us/sample - loss: 1.1182\n",
            "Epoch 151/500\n",
            "55/55 [==============================] - 0s 186us/sample - loss: 1.1135\n",
            "Epoch 152/500\n",
            "55/55 [==============================] - 0s 166us/sample - loss: 1.1120\n",
            "Epoch 153/500\n",
            "55/55 [==============================] - 0s 166us/sample - loss: 1.1105\n",
            "Epoch 154/500\n",
            "55/55 [==============================] - 0s 176us/sample - loss: 1.1086\n",
            "Epoch 155/500\n",
            "55/55 [==============================] - 0s 153us/sample - loss: 1.1082\n",
            "Epoch 156/500\n",
            "55/55 [==============================] - 0s 198us/sample - loss: 1.1065\n",
            "Epoch 157/500\n",
            "55/55 [==============================] - 0s 157us/sample - loss: 1.1061\n",
            "Epoch 158/500\n",
            "55/55 [==============================] - 0s 185us/sample - loss: 1.1043\n",
            "Epoch 159/500\n",
            "55/55 [==============================] - 0s 239us/sample - loss: 1.1027\n",
            "Epoch 160/500\n",
            "55/55 [==============================] - 0s 182us/sample - loss: 1.1025\n",
            "Epoch 161/500\n",
            "55/55 [==============================] - 0s 163us/sample - loss: 1.1002\n",
            "Epoch 162/500\n",
            "55/55 [==============================] - 0s 173us/sample - loss: 1.0993\n",
            "Epoch 163/500\n",
            "55/55 [==============================] - 0s 174us/sample - loss: 1.0982\n",
            "Epoch 164/500\n",
            "55/55 [==============================] - 0s 170us/sample - loss: 1.0978\n",
            "Epoch 165/500\n",
            "55/55 [==============================] - 0s 152us/sample - loss: 1.0957\n",
            "Epoch 166/500\n",
            "55/55 [==============================] - 0s 177us/sample - loss: 1.0946\n",
            "Epoch 167/500\n",
            "55/55 [==============================] - 0s 171us/sample - loss: 1.0935\n",
            "Epoch 168/500\n",
            "55/55 [==============================] - 0s 179us/sample - loss: 1.0922\n",
            "Epoch 169/500\n",
            "55/55 [==============================] - 0s 168us/sample - loss: 1.0914\n",
            "Epoch 170/500\n",
            "55/55 [==============================] - 0s 167us/sample - loss: 1.0896\n",
            "Epoch 171/500\n",
            "55/55 [==============================] - 0s 133us/sample - loss: 1.0888\n",
            "Epoch 172/500\n",
            "55/55 [==============================] - 0s 175us/sample - loss: 1.0889\n",
            "Epoch 173/500\n",
            "55/55 [==============================] - 0s 160us/sample - loss: 1.0866\n",
            "Epoch 174/500\n",
            "55/55 [==============================] - 0s 158us/sample - loss: 1.0858\n",
            "Epoch 175/500\n",
            "55/55 [==============================] - 0s 152us/sample - loss: 1.0834\n",
            "Epoch 176/500\n",
            "55/55 [==============================] - 0s 138us/sample - loss: 1.0882\n",
            "Epoch 177/500\n",
            "55/55 [==============================] - 0s 155us/sample - loss: 1.0826\n",
            "Epoch 178/500\n",
            "55/55 [==============================] - 0s 165us/sample - loss: 1.0802\n",
            "Epoch 179/500\n",
            "55/55 [==============================] - 0s 218us/sample - loss: 1.0788\n",
            "Epoch 180/500\n",
            "55/55 [==============================] - 0s 191us/sample - loss: 1.0784\n",
            "Epoch 181/500\n",
            "55/55 [==============================] - 0s 181us/sample - loss: 1.0772\n",
            "Epoch 182/500\n",
            "55/55 [==============================] - 0s 224us/sample - loss: 1.0755\n",
            "Epoch 183/500\n",
            "55/55 [==============================] - 0s 178us/sample - loss: 1.0755\n",
            "Epoch 184/500\n",
            "55/55 [==============================] - 0s 166us/sample - loss: 1.0740\n",
            "Epoch 185/500\n",
            "55/55 [==============================] - 0s 198us/sample - loss: 1.0722\n",
            "Epoch 186/500\n",
            "55/55 [==============================] - 0s 186us/sample - loss: 1.0707\n",
            "Epoch 187/500\n",
            "55/55 [==============================] - 0s 194us/sample - loss: 1.0689\n",
            "Epoch 188/500\n",
            "55/55 [==============================] - 0s 177us/sample - loss: 1.0683\n",
            "Epoch 189/500\n",
            "55/55 [==============================] - 0s 173us/sample - loss: 1.0679\n",
            "Epoch 190/500\n",
            "55/55 [==============================] - 0s 168us/sample - loss: 1.0688\n",
            "Epoch 191/500\n",
            "55/55 [==============================] - 0s 222us/sample - loss: 1.0653\n",
            "Epoch 192/500\n",
            "55/55 [==============================] - 0s 168us/sample - loss: 1.0644\n",
            "Epoch 193/500\n",
            "55/55 [==============================] - 0s 173us/sample - loss: 1.0622\n",
            "Epoch 194/500\n",
            "55/55 [==============================] - 0s 168us/sample - loss: 1.0610\n",
            "Epoch 195/500\n",
            "55/55 [==============================] - 0s 169us/sample - loss: 1.0604\n",
            "Epoch 196/500\n",
            "55/55 [==============================] - 0s 179us/sample - loss: 1.0593\n",
            "Epoch 197/500\n",
            "55/55 [==============================] - 0s 147us/sample - loss: 1.0581\n",
            "Epoch 198/500\n",
            "55/55 [==============================] - 0s 171us/sample - loss: 1.0583\n",
            "Epoch 199/500\n",
            "55/55 [==============================] - 0s 170us/sample - loss: 1.0553\n",
            "Epoch 200/500\n",
            "55/55 [==============================] - 0s 189us/sample - loss: 1.0542\n",
            "Epoch 201/500\n",
            "55/55 [==============================] - 0s 153us/sample - loss: 1.0529\n",
            "Epoch 202/500\n",
            "55/55 [==============================] - 0s 175us/sample - loss: 1.0513\n",
            "Epoch 203/500\n",
            "55/55 [==============================] - 0s 197us/sample - loss: 1.0505\n",
            "Epoch 204/500\n",
            "55/55 [==============================] - 0s 159us/sample - loss: 1.0498\n",
            "Epoch 205/500\n",
            "55/55 [==============================] - 0s 190us/sample - loss: 1.0492\n",
            "Epoch 206/500\n",
            "55/55 [==============================] - 0s 196us/sample - loss: 1.0489\n",
            "Epoch 207/500\n",
            "55/55 [==============================] - 0s 177us/sample - loss: 1.0472\n",
            "Epoch 208/500\n",
            "55/55 [==============================] - 0s 176us/sample - loss: 1.0461\n",
            "Epoch 209/500\n",
            "55/55 [==============================] - 0s 153us/sample - loss: 1.0437\n",
            "Epoch 210/500\n",
            "55/55 [==============================] - 0s 225us/sample - loss: 1.0420\n",
            "Epoch 211/500\n",
            "55/55 [==============================] - 0s 164us/sample - loss: 1.0415\n",
            "Epoch 212/500\n",
            "55/55 [==============================] - 0s 186us/sample - loss: 1.0404\n",
            "Epoch 213/500\n",
            "55/55 [==============================] - 0s 173us/sample - loss: 1.0402\n",
            "Epoch 214/500\n",
            "55/55 [==============================] - 0s 183us/sample - loss: 1.0378\n",
            "Epoch 215/500\n",
            "55/55 [==============================] - 0s 151us/sample - loss: 1.0369\n",
            "Epoch 216/500\n",
            "55/55 [==============================] - 0s 180us/sample - loss: 1.0353\n",
            "Epoch 217/500\n",
            "55/55 [==============================] - 0s 172us/sample - loss: 1.0343\n",
            "Epoch 218/500\n",
            "55/55 [==============================] - 0s 191us/sample - loss: 1.0329\n",
            "Epoch 219/500\n",
            "55/55 [==============================] - 0s 161us/sample - loss: 1.0334\n",
            "Epoch 220/500\n",
            "55/55 [==============================] - 0s 170us/sample - loss: 1.0318\n",
            "Epoch 221/500\n",
            "55/55 [==============================] - 0s 242us/sample - loss: 1.0294\n",
            "Epoch 222/500\n",
            "55/55 [==============================] - 0s 244us/sample - loss: 1.0288\n",
            "Epoch 223/500\n",
            "55/55 [==============================] - 0s 181us/sample - loss: 1.0270\n",
            "Epoch 224/500\n",
            "55/55 [==============================] - 0s 176us/sample - loss: 1.0293\n",
            "Epoch 225/500\n",
            "55/55 [==============================] - 0s 176us/sample - loss: 1.0255\n",
            "Epoch 226/500\n",
            "55/55 [==============================] - 0s 194us/sample - loss: 1.0243\n",
            "Epoch 227/500\n",
            "55/55 [==============================] - 0s 167us/sample - loss: 1.0232\n",
            "Epoch 228/500\n",
            "55/55 [==============================] - 0s 309us/sample - loss: 1.0215\n",
            "Epoch 229/500\n",
            "55/55 [==============================] - 0s 174us/sample - loss: 1.0221\n",
            "Epoch 230/500\n",
            "55/55 [==============================] - 0s 173us/sample - loss: 1.0190\n",
            "Epoch 231/500\n",
            "55/55 [==============================] - 0s 178us/sample - loss: 1.0200\n",
            "Epoch 232/500\n",
            "55/55 [==============================] - 0s 233us/sample - loss: 1.0173\n",
            "Epoch 233/500\n",
            "55/55 [==============================] - 0s 172us/sample - loss: 1.0159\n",
            "Epoch 234/500\n",
            "55/55 [==============================] - 0s 217us/sample - loss: 1.0183\n",
            "Epoch 235/500\n",
            "55/55 [==============================] - 0s 184us/sample - loss: 1.0137\n",
            "Epoch 236/500\n",
            "55/55 [==============================] - 0s 193us/sample - loss: 1.0131\n",
            "Epoch 237/500\n",
            "55/55 [==============================] - 0s 143us/sample - loss: 1.0132\n",
            "Epoch 238/500\n",
            "55/55 [==============================] - 0s 123us/sample - loss: 1.0105\n",
            "Epoch 239/500\n",
            "55/55 [==============================] - 0s 130us/sample - loss: 1.0097\n",
            "Epoch 240/500\n",
            "55/55 [==============================] - 0s 132us/sample - loss: 1.0083\n",
            "Epoch 241/500\n",
            "55/55 [==============================] - 0s 129us/sample - loss: 1.0098\n",
            "Epoch 242/500\n",
            "55/55 [==============================] - 0s 133us/sample - loss: 1.0089\n",
            "Epoch 243/500\n",
            "55/55 [==============================] - 0s 163us/sample - loss: 1.0048\n",
            "Epoch 244/500\n",
            "55/55 [==============================] - 0s 167us/sample - loss: 1.0057\n",
            "Epoch 245/500\n",
            "55/55 [==============================] - 0s 163us/sample - loss: 1.0036\n",
            "Epoch 246/500\n",
            "55/55 [==============================] - 0s 165us/sample - loss: 1.0009\n",
            "Epoch 247/500\n",
            "55/55 [==============================] - 0s 160us/sample - loss: 1.0003\n",
            "Epoch 248/500\n",
            "55/55 [==============================] - 0s 174us/sample - loss: 1.0028\n",
            "Epoch 249/500\n",
            "55/55 [==============================] - 0s 162us/sample - loss: 1.0012\n",
            "Epoch 250/500\n",
            "55/55 [==============================] - 0s 158us/sample - loss: 0.9996\n",
            "Epoch 251/500\n",
            "55/55 [==============================] - 0s 121us/sample - loss: 0.9959\n",
            "Epoch 252/500\n",
            "55/55 [==============================] - 0s 173us/sample - loss: 0.9961\n",
            "Epoch 253/500\n",
            "55/55 [==============================] - 0s 187us/sample - loss: 0.9940\n",
            "Epoch 254/500\n",
            "55/55 [==============================] - 0s 201us/sample - loss: 0.9954\n",
            "Epoch 255/500\n",
            "55/55 [==============================] - 0s 169us/sample - loss: 0.9923\n",
            "Epoch 256/500\n",
            "55/55 [==============================] - 0s 169us/sample - loss: 0.9905\n",
            "Epoch 257/500\n",
            "55/55 [==============================] - 0s 142us/sample - loss: 0.9917\n",
            "Epoch 258/500\n",
            "55/55 [==============================] - 0s 195us/sample - loss: 0.9890\n",
            "Epoch 259/500\n",
            "55/55 [==============================] - 0s 155us/sample - loss: 0.9899\n",
            "Epoch 260/500\n",
            "55/55 [==============================] - 0s 155us/sample - loss: 0.9880\n",
            "Epoch 261/500\n",
            "55/55 [==============================] - 0s 169us/sample - loss: 0.9879\n",
            "Epoch 262/500\n",
            "55/55 [==============================] - 0s 158us/sample - loss: 0.9853\n",
            "Epoch 263/500\n",
            "55/55 [==============================] - 0s 192us/sample - loss: 0.9846\n",
            "Epoch 264/500\n",
            "55/55 [==============================] - 0s 177us/sample - loss: 0.9824\n",
            "Epoch 265/500\n",
            "55/55 [==============================] - 0s 221us/sample - loss: 0.9831\n",
            "Epoch 266/500\n",
            "55/55 [==============================] - 0s 146us/sample - loss: 0.9835\n",
            "Epoch 267/500\n",
            "55/55 [==============================] - 0s 144us/sample - loss: 0.9796\n",
            "Epoch 268/500\n",
            "55/55 [==============================] - 0s 209us/sample - loss: 0.9815\n",
            "Epoch 269/500\n",
            "55/55 [==============================] - 0s 193us/sample - loss: 0.9775\n",
            "Epoch 270/500\n",
            "55/55 [==============================] - 0s 272us/sample - loss: 0.9765\n",
            "Epoch 271/500\n",
            "55/55 [==============================] - 0s 153us/sample - loss: 0.9748\n",
            "Epoch 272/500\n",
            "55/55 [==============================] - 0s 131us/sample - loss: 0.9736\n",
            "Epoch 273/500\n",
            "55/55 [==============================] - 0s 148us/sample - loss: 0.9733\n",
            "Epoch 274/500\n",
            "55/55 [==============================] - 0s 180us/sample - loss: 0.9721\n",
            "Epoch 275/500\n",
            "55/55 [==============================] - 0s 156us/sample - loss: 0.9729\n",
            "Epoch 276/500\n",
            "55/55 [==============================] - 0s 127us/sample - loss: 0.9722\n",
            "Epoch 277/500\n",
            "55/55 [==============================] - 0s 154us/sample - loss: 0.9689\n",
            "Epoch 278/500\n",
            "55/55 [==============================] - 0s 158us/sample - loss: 0.9671\n",
            "Epoch 279/500\n",
            "55/55 [==============================] - 0s 143us/sample - loss: 0.9671\n",
            "Epoch 280/500\n",
            "55/55 [==============================] - 0s 126us/sample - loss: 0.9670\n",
            "Epoch 281/500\n",
            "55/55 [==============================] - 0s 160us/sample - loss: 0.9687\n",
            "Epoch 282/500\n",
            "55/55 [==============================] - 0s 118us/sample - loss: 0.9674\n",
            "Epoch 283/500\n",
            "55/55 [==============================] - 0s 140us/sample - loss: 0.9657\n",
            "Epoch 284/500\n",
            "55/55 [==============================] - 0s 131us/sample - loss: 0.9639\n",
            "Epoch 285/500\n",
            "55/55 [==============================] - 0s 298us/sample - loss: 0.9614\n",
            "Epoch 286/500\n",
            "55/55 [==============================] - 0s 204us/sample - loss: 0.9594\n",
            "Epoch 287/500\n",
            "55/55 [==============================] - 0s 199us/sample - loss: 0.9600\n",
            "Epoch 288/500\n",
            "55/55 [==============================] - 0s 187us/sample - loss: 0.9596\n",
            "Epoch 289/500\n",
            "55/55 [==============================] - 0s 176us/sample - loss: 0.9588\n",
            "Epoch 290/500\n",
            "55/55 [==============================] - 0s 184us/sample - loss: 0.9576\n",
            "Epoch 291/500\n",
            "55/55 [==============================] - 0s 191us/sample - loss: 0.9568\n",
            "Epoch 292/500\n",
            "55/55 [==============================] - 0s 272us/sample - loss: 0.9545\n",
            "Epoch 293/500\n",
            "55/55 [==============================] - 0s 193us/sample - loss: 0.9530\n",
            "Epoch 294/500\n",
            "55/55 [==============================] - 0s 232us/sample - loss: 0.9527\n",
            "Epoch 295/500\n",
            "55/55 [==============================] - 0s 189us/sample - loss: 0.9520\n",
            "Epoch 296/500\n",
            "55/55 [==============================] - 0s 159us/sample - loss: 0.9513\n",
            "Epoch 297/500\n",
            "55/55 [==============================] - 0s 173us/sample - loss: 0.9495\n",
            "Epoch 298/500\n",
            "55/55 [==============================] - 0s 204us/sample - loss: 0.9486\n",
            "Epoch 299/500\n",
            "55/55 [==============================] - 0s 227us/sample - loss: 0.9486\n",
            "Epoch 300/500\n",
            "55/55 [==============================] - 0s 199us/sample - loss: 0.9491\n",
            "Epoch 301/500\n",
            "55/55 [==============================] - 0s 187us/sample - loss: 0.9483\n",
            "Epoch 302/500\n",
            "55/55 [==============================] - 0s 201us/sample - loss: 0.9446\n",
            "Epoch 303/500\n",
            "55/55 [==============================] - 0s 209us/sample - loss: 0.9448\n",
            "Epoch 304/500\n",
            "55/55 [==============================] - 0s 204us/sample - loss: 0.9427\n",
            "Epoch 305/500\n",
            "55/55 [==============================] - 0s 224us/sample - loss: 0.9460\n",
            "Epoch 306/500\n",
            "55/55 [==============================] - 0s 210us/sample - loss: 0.9423\n",
            "Epoch 307/500\n",
            "55/55 [==============================] - 0s 187us/sample - loss: 0.9403\n",
            "Epoch 308/500\n",
            "55/55 [==============================] - 0s 227us/sample - loss: 0.9386\n",
            "Epoch 309/500\n",
            "55/55 [==============================] - 0s 253us/sample - loss: 0.9394\n",
            "Epoch 310/500\n",
            "55/55 [==============================] - 0s 159us/sample - loss: 0.9370\n",
            "Epoch 311/500\n",
            "55/55 [==============================] - 0s 232us/sample - loss: 0.9378\n",
            "Epoch 312/500\n",
            "55/55 [==============================] - 0s 177us/sample - loss: 0.9373\n",
            "Epoch 313/500\n",
            "55/55 [==============================] - 0s 237us/sample - loss: 0.9338\n",
            "Epoch 314/500\n",
            "55/55 [==============================] - 0s 209us/sample - loss: 0.9354\n",
            "Epoch 315/500\n",
            "55/55 [==============================] - 0s 259us/sample - loss: 0.9329\n",
            "Epoch 316/500\n",
            "55/55 [==============================] - 0s 140us/sample - loss: 0.9312\n",
            "Epoch 317/500\n",
            "55/55 [==============================] - 0s 247us/sample - loss: 0.9308\n",
            "Epoch 318/500\n",
            "55/55 [==============================] - 0s 197us/sample - loss: 0.9304\n",
            "Epoch 319/500\n",
            "55/55 [==============================] - 0s 196us/sample - loss: 0.9290\n",
            "Epoch 320/500\n",
            "55/55 [==============================] - 0s 204us/sample - loss: 0.9282\n",
            "Epoch 321/500\n",
            "55/55 [==============================] - 0s 154us/sample - loss: 0.9271\n",
            "Epoch 322/500\n",
            "55/55 [==============================] - 0s 183us/sample - loss: 0.9259\n",
            "Epoch 323/500\n",
            "55/55 [==============================] - 0s 152us/sample - loss: 0.9251\n",
            "Epoch 324/500\n",
            "55/55 [==============================] - 0s 145us/sample - loss: 0.9238\n",
            "Epoch 325/500\n",
            "55/55 [==============================] - 0s 168us/sample - loss: 0.9235\n",
            "Epoch 326/500\n",
            "55/55 [==============================] - 0s 142us/sample - loss: 0.9226\n",
            "Epoch 327/500\n",
            "55/55 [==============================] - 0s 120us/sample - loss: 0.9217\n",
            "Epoch 328/500\n",
            "55/55 [==============================] - 0s 169us/sample - loss: 0.9205\n",
            "Epoch 329/500\n",
            "55/55 [==============================] - 0s 192us/sample - loss: 0.9195\n",
            "Epoch 330/500\n",
            "55/55 [==============================] - 0s 160us/sample - loss: 0.9196\n",
            "Epoch 331/500\n",
            "55/55 [==============================] - 0s 164us/sample - loss: 0.9176\n",
            "Epoch 332/500\n",
            "55/55 [==============================] - 0s 156us/sample - loss: 0.9180\n",
            "Epoch 333/500\n",
            "55/55 [==============================] - 0s 182us/sample - loss: 0.9165\n",
            "Epoch 334/500\n",
            "55/55 [==============================] - 0s 221us/sample - loss: 0.9213\n",
            "Epoch 335/500\n",
            "55/55 [==============================] - 0s 145us/sample - loss: 0.9164\n",
            "Epoch 336/500\n",
            "55/55 [==============================] - 0s 165us/sample - loss: 0.9143\n",
            "Epoch 337/500\n",
            "55/55 [==============================] - 0s 187us/sample - loss: 0.9133\n",
            "Epoch 338/500\n",
            "55/55 [==============================] - 0s 186us/sample - loss: 0.9136\n",
            "Epoch 339/500\n",
            "55/55 [==============================] - 0s 178us/sample - loss: 0.9116\n",
            "Epoch 340/500\n",
            "55/55 [==============================] - 0s 204us/sample - loss: 0.9105\n",
            "Epoch 341/500\n",
            "55/55 [==============================] - 0s 132us/sample - loss: 0.9098\n",
            "Epoch 342/500\n",
            "55/55 [==============================] - 0s 131us/sample - loss: 0.9085\n",
            "Epoch 343/500\n",
            "55/55 [==============================] - 0s 164us/sample - loss: 0.9100\n",
            "Epoch 344/500\n",
            "55/55 [==============================] - 0s 161us/sample - loss: 0.9083\n",
            "Epoch 345/500\n",
            "55/55 [==============================] - 0s 169us/sample - loss: 0.9078\n",
            "Epoch 346/500\n",
            "55/55 [==============================] - 0s 182us/sample - loss: 0.9083\n",
            "Epoch 347/500\n",
            "55/55 [==============================] - 0s 187us/sample - loss: 0.9063\n",
            "Epoch 348/500\n",
            "55/55 [==============================] - 0s 171us/sample - loss: 0.9062\n",
            "Epoch 349/500\n",
            "55/55 [==============================] - 0s 173us/sample - loss: 0.9031\n",
            "Epoch 350/500\n",
            "55/55 [==============================] - 0s 171us/sample - loss: 0.9028\n",
            "Epoch 351/500\n",
            "55/55 [==============================] - 0s 189us/sample - loss: 0.9013\n",
            "Epoch 352/500\n",
            "55/55 [==============================] - 0s 175us/sample - loss: 0.9027\n",
            "Epoch 353/500\n",
            "55/55 [==============================] - 0s 185us/sample - loss: 0.9004\n",
            "Epoch 354/500\n",
            "55/55 [==============================] - 0s 171us/sample - loss: 0.8984\n",
            "Epoch 355/500\n",
            "55/55 [==============================] - 0s 168us/sample - loss: 0.9019\n",
            "Epoch 356/500\n",
            "55/55 [==============================] - 0s 142us/sample - loss: 0.8983\n",
            "Epoch 357/500\n",
            "55/55 [==============================] - 0s 122us/sample - loss: 0.8996\n",
            "Epoch 358/500\n",
            "55/55 [==============================] - 0s 126us/sample - loss: 0.8965\n",
            "Epoch 359/500\n",
            "55/55 [==============================] - 0s 141us/sample - loss: 0.8961\n",
            "Epoch 360/500\n",
            "55/55 [==============================] - 0s 118us/sample - loss: 0.8942\n",
            "Epoch 361/500\n",
            "55/55 [==============================] - 0s 126us/sample - loss: 0.8944\n",
            "Epoch 362/500\n",
            "55/55 [==============================] - 0s 131us/sample - loss: 0.8934\n",
            "Epoch 363/500\n",
            "55/55 [==============================] - 0s 148us/sample - loss: 0.8952\n",
            "Epoch 364/500\n",
            "55/55 [==============================] - 0s 175us/sample - loss: 0.8928\n",
            "Epoch 365/500\n",
            "55/55 [==============================] - 0s 170us/sample - loss: 0.8917\n",
            "Epoch 366/500\n",
            "55/55 [==============================] - 0s 176us/sample - loss: 0.8963\n",
            "Epoch 367/500\n",
            "55/55 [==============================] - 0s 164us/sample - loss: 0.8898\n",
            "Epoch 368/500\n",
            "55/55 [==============================] - 0s 134us/sample - loss: 0.8891\n",
            "Epoch 369/500\n",
            "55/55 [==============================] - 0s 166us/sample - loss: 0.8903\n",
            "Epoch 370/500\n",
            "55/55 [==============================] - 0s 155us/sample - loss: 0.8874\n",
            "Epoch 371/500\n",
            "55/55 [==============================] - 0s 165us/sample - loss: 0.8870\n",
            "Epoch 372/500\n",
            "55/55 [==============================] - 0s 174us/sample - loss: 0.8901\n",
            "Epoch 373/500\n",
            "55/55 [==============================] - 0s 184us/sample - loss: 0.8850\n",
            "Epoch 374/500\n",
            "55/55 [==============================] - 0s 180us/sample - loss: 0.8852\n",
            "Epoch 375/500\n",
            "55/55 [==============================] - 0s 221us/sample - loss: 0.8855\n",
            "Epoch 376/500\n",
            "55/55 [==============================] - 0s 223us/sample - loss: 0.8840\n",
            "Epoch 377/500\n",
            "55/55 [==============================] - 0s 195us/sample - loss: 0.8845\n",
            "Epoch 378/500\n",
            "55/55 [==============================] - 0s 195us/sample - loss: 0.8818\n",
            "Epoch 379/500\n",
            "55/55 [==============================] - 0s 211us/sample - loss: 0.8850\n",
            "Epoch 380/500\n",
            "55/55 [==============================] - 0s 212us/sample - loss: 0.8826\n",
            "Epoch 381/500\n",
            "55/55 [==============================] - 0s 180us/sample - loss: 0.8805\n",
            "Epoch 382/500\n",
            "55/55 [==============================] - 0s 207us/sample - loss: 0.8795\n",
            "Epoch 383/500\n",
            "55/55 [==============================] - 0s 204us/sample - loss: 0.8779\n",
            "Epoch 384/500\n",
            "55/55 [==============================] - 0s 190us/sample - loss: 0.8775\n",
            "Epoch 385/500\n",
            "55/55 [==============================] - 0s 196us/sample - loss: 0.8774\n",
            "Epoch 386/500\n",
            "55/55 [==============================] - 0s 180us/sample - loss: 0.8840\n",
            "Epoch 387/500\n",
            "55/55 [==============================] - 0s 192us/sample - loss: 0.8803\n",
            "Epoch 388/500\n",
            "55/55 [==============================] - 0s 184us/sample - loss: 0.8778\n",
            "Epoch 389/500\n",
            "55/55 [==============================] - 0s 200us/sample - loss: 0.8762\n",
            "Epoch 390/500\n",
            "55/55 [==============================] - 0s 156us/sample - loss: 0.8741\n",
            "Epoch 391/500\n",
            "55/55 [==============================] - 0s 166us/sample - loss: 0.8734\n",
            "Epoch 392/500\n",
            "55/55 [==============================] - 0s 168us/sample - loss: 0.8716\n",
            "Epoch 393/500\n",
            "55/55 [==============================] - 0s 160us/sample - loss: 0.8715\n",
            "Epoch 394/500\n",
            "55/55 [==============================] - 0s 178us/sample - loss: 0.8715\n",
            "Epoch 395/500\n",
            "55/55 [==============================] - 0s 178us/sample - loss: 0.8712\n",
            "Epoch 396/500\n",
            "55/55 [==============================] - 0s 211us/sample - loss: 0.8708\n",
            "Epoch 397/500\n",
            "55/55 [==============================] - 0s 189us/sample - loss: 0.8698\n",
            "Epoch 398/500\n",
            "55/55 [==============================] - 0s 180us/sample - loss: 0.8702\n",
            "Epoch 399/500\n",
            "55/55 [==============================] - 0s 198us/sample - loss: 0.8674\n",
            "Epoch 400/500\n",
            "55/55 [==============================] - 0s 287us/sample - loss: 0.8669\n",
            "Epoch 401/500\n",
            "55/55 [==============================] - 0s 299us/sample - loss: 0.8676\n",
            "Epoch 402/500\n",
            "55/55 [==============================] - 0s 252us/sample - loss: 0.8670\n",
            "Epoch 403/500\n",
            "55/55 [==============================] - 0s 193us/sample - loss: 0.8655\n",
            "Epoch 404/500\n",
            "55/55 [==============================] - 0s 203us/sample - loss: 0.8652\n",
            "Epoch 405/500\n",
            "55/55 [==============================] - 0s 225us/sample - loss: 0.8633\n",
            "Epoch 406/500\n",
            "55/55 [==============================] - 0s 185us/sample - loss: 0.8715\n",
            "Epoch 407/500\n",
            "55/55 [==============================] - 0s 181us/sample - loss: 0.8635\n",
            "Epoch 408/500\n",
            "55/55 [==============================] - 0s 172us/sample - loss: 0.8627\n",
            "Epoch 409/500\n",
            "55/55 [==============================] - 0s 190us/sample - loss: 0.8610\n",
            "Epoch 410/500\n",
            "55/55 [==============================] - 0s 178us/sample - loss: 0.8639\n",
            "Epoch 411/500\n",
            "55/55 [==============================] - 0s 164us/sample - loss: 0.8602\n",
            "Epoch 412/500\n",
            "55/55 [==============================] - 0s 177us/sample - loss: 0.8600\n",
            "Epoch 413/500\n",
            "55/55 [==============================] - 0s 173us/sample - loss: 0.8615\n",
            "Epoch 414/500\n",
            "55/55 [==============================] - 0s 169us/sample - loss: 0.8601\n",
            "Epoch 415/500\n",
            "55/55 [==============================] - 0s 158us/sample - loss: 0.8579\n",
            "Epoch 416/500\n",
            "55/55 [==============================] - 0s 154us/sample - loss: 0.8571\n",
            "Epoch 417/500\n",
            "55/55 [==============================] - 0s 154us/sample - loss: 0.8569\n",
            "Epoch 418/500\n",
            "55/55 [==============================] - 0s 169us/sample - loss: 0.8564\n",
            "Epoch 419/500\n",
            "55/55 [==============================] - 0s 204us/sample - loss: 0.8554\n",
            "Epoch 420/500\n",
            "55/55 [==============================] - 0s 129us/sample - loss: 0.8560\n",
            "Epoch 421/500\n",
            "55/55 [==============================] - 0s 161us/sample - loss: 0.8544\n",
            "Epoch 422/500\n",
            "55/55 [==============================] - 0s 198us/sample - loss: 0.8539\n",
            "Epoch 423/500\n",
            "55/55 [==============================] - 0s 183us/sample - loss: 0.8530\n",
            "Epoch 424/500\n",
            "55/55 [==============================] - 0s 233us/sample - loss: 0.8524\n",
            "Epoch 425/500\n",
            "55/55 [==============================] - 0s 206us/sample - loss: 0.8522\n",
            "Epoch 426/500\n",
            "55/55 [==============================] - 0s 203us/sample - loss: 0.8545\n",
            "Epoch 427/500\n",
            "55/55 [==============================] - 0s 160us/sample - loss: 0.8509\n",
            "Epoch 428/500\n",
            "55/55 [==============================] - 0s 184us/sample - loss: 0.8530\n",
            "Epoch 429/500\n",
            "55/55 [==============================] - 0s 160us/sample - loss: 0.8525\n",
            "Epoch 430/500\n",
            "55/55 [==============================] - 0s 188us/sample - loss: 0.8539\n",
            "Epoch 431/500\n",
            "55/55 [==============================] - 0s 179us/sample - loss: 0.8500\n",
            "Epoch 432/500\n",
            "55/55 [==============================] - 0s 167us/sample - loss: 0.8502\n",
            "Epoch 433/500\n",
            "55/55 [==============================] - 0s 152us/sample - loss: 0.8490\n",
            "Epoch 434/500\n",
            "55/55 [==============================] - 0s 164us/sample - loss: 0.8481\n",
            "Epoch 435/500\n",
            "55/55 [==============================] - 0s 136us/sample - loss: 0.8477\n",
            "Epoch 436/500\n",
            "55/55 [==============================] - 0s 156us/sample - loss: 0.8497\n",
            "Epoch 437/500\n",
            "55/55 [==============================] - 0s 179us/sample - loss: 0.8461\n",
            "Epoch 438/500\n",
            "55/55 [==============================] - 0s 176us/sample - loss: 0.8459\n",
            "Epoch 439/500\n",
            "55/55 [==============================] - 0s 169us/sample - loss: 0.8454\n",
            "Epoch 440/500\n",
            "55/55 [==============================] - 0s 253us/sample - loss: 0.8447\n",
            "Epoch 441/500\n",
            "55/55 [==============================] - 0s 148us/sample - loss: 0.8453\n",
            "Epoch 442/500\n",
            "55/55 [==============================] - 0s 168us/sample - loss: 0.8452\n",
            "Epoch 443/500\n",
            "55/55 [==============================] - 0s 167us/sample - loss: 0.8424\n",
            "Epoch 444/500\n",
            "55/55 [==============================] - 0s 171us/sample - loss: 0.8446\n",
            "Epoch 445/500\n",
            "55/55 [==============================] - 0s 154us/sample - loss: 0.8414\n",
            "Epoch 446/500\n",
            "55/55 [==============================] - 0s 168us/sample - loss: 0.8413\n",
            "Epoch 447/500\n",
            "55/55 [==============================] - 0s 169us/sample - loss: 0.8402\n",
            "Epoch 448/500\n",
            "55/55 [==============================] - 0s 171us/sample - loss: 0.8396\n",
            "Epoch 449/500\n",
            "55/55 [==============================] - 0s 160us/sample - loss: 0.8390\n",
            "Epoch 450/500\n",
            "55/55 [==============================] - 0s 160us/sample - loss: 0.8388\n",
            "Epoch 451/500\n",
            "55/55 [==============================] - 0s 166us/sample - loss: 0.8385\n",
            "Epoch 452/500\n",
            "55/55 [==============================] - 0s 156us/sample - loss: 0.8386\n",
            "Epoch 453/500\n",
            "55/55 [==============================] - 0s 156us/sample - loss: 0.8391\n",
            "Epoch 454/500\n",
            "55/55 [==============================] - 0s 329us/sample - loss: 0.8365\n",
            "Epoch 455/500\n",
            "55/55 [==============================] - 0s 146us/sample - loss: 0.8362\n",
            "Epoch 456/500\n",
            "55/55 [==============================] - 0s 166us/sample - loss: 0.8389\n",
            "Epoch 457/500\n",
            "55/55 [==============================] - 0s 167us/sample - loss: 0.8349\n",
            "Epoch 458/500\n",
            "55/55 [==============================] - 0s 156us/sample - loss: 0.8389\n",
            "Epoch 459/500\n",
            "55/55 [==============================] - 0s 157us/sample - loss: 0.8348\n",
            "Epoch 460/500\n",
            "55/55 [==============================] - 0s 167us/sample - loss: 0.8370\n",
            "Epoch 461/500\n",
            "55/55 [==============================] - 0s 180us/sample - loss: 0.8340\n",
            "Epoch 462/500\n",
            "55/55 [==============================] - 0s 168us/sample - loss: 0.8329\n",
            "Epoch 463/500\n",
            "55/55 [==============================] - 0s 165us/sample - loss: 0.8323\n",
            "Epoch 464/500\n",
            "55/55 [==============================] - 0s 224us/sample - loss: 0.8321\n",
            "Epoch 465/500\n",
            "55/55 [==============================] - 0s 197us/sample - loss: 0.8316\n",
            "Epoch 466/500\n",
            "55/55 [==============================] - 0s 191us/sample - loss: 0.8313\n",
            "Epoch 467/500\n",
            "55/55 [==============================] - 0s 183us/sample - loss: 0.8306\n",
            "Epoch 468/500\n",
            "55/55 [==============================] - 0s 193us/sample - loss: 0.8335\n",
            "Epoch 469/500\n",
            "55/55 [==============================] - 0s 175us/sample - loss: 0.8301\n",
            "Epoch 470/500\n",
            "55/55 [==============================] - 0s 168us/sample - loss: 0.8299\n",
            "Epoch 471/500\n",
            "55/55 [==============================] - 0s 189us/sample - loss: 0.8293\n",
            "Epoch 472/500\n",
            "55/55 [==============================] - 0s 187us/sample - loss: 0.8291\n",
            "Epoch 473/500\n",
            "55/55 [==============================] - 0s 177us/sample - loss: 0.8320\n",
            "Epoch 474/500\n",
            "55/55 [==============================] - 0s 178us/sample - loss: 0.8292\n",
            "Epoch 475/500\n",
            "55/55 [==============================] - 0s 176us/sample - loss: 0.8290\n",
            "Epoch 476/500\n",
            "55/55 [==============================] - 0s 155us/sample - loss: 0.8269\n",
            "Epoch 477/500\n",
            "55/55 [==============================] - 0s 201us/sample - loss: 0.8272\n",
            "Epoch 478/500\n",
            "55/55 [==============================] - 0s 207us/sample - loss: 0.8260\n",
            "Epoch 479/500\n",
            "55/55 [==============================] - 0s 190us/sample - loss: 0.8264\n",
            "Epoch 480/500\n",
            "55/55 [==============================] - 0s 202us/sample - loss: 0.8253\n",
            "Epoch 481/500\n",
            "55/55 [==============================] - 0s 169us/sample - loss: 0.8292\n",
            "Epoch 482/500\n",
            "55/55 [==============================] - 0s 157us/sample - loss: 0.8242\n",
            "Epoch 483/500\n",
            "55/55 [==============================] - 0s 227us/sample - loss: 0.8251\n",
            "Epoch 484/500\n",
            "55/55 [==============================] - 0s 185us/sample - loss: 0.8241\n",
            "Epoch 485/500\n",
            "55/55 [==============================] - 0s 169us/sample - loss: 0.8233\n",
            "Epoch 486/500\n",
            "55/55 [==============================] - 0s 190us/sample - loss: 0.8230\n",
            "Epoch 487/500\n",
            "55/55 [==============================] - 0s 182us/sample - loss: 0.8222\n",
            "Epoch 488/500\n",
            "55/55 [==============================] - 0s 196us/sample - loss: 0.8226\n",
            "Epoch 489/500\n",
            "55/55 [==============================] - 0s 182us/sample - loss: 0.8286\n",
            "Epoch 490/500\n",
            "55/55 [==============================] - 0s 148us/sample - loss: 0.8239\n",
            "Epoch 491/500\n",
            "55/55 [==============================] - 0s 238us/sample - loss: 0.8213\n",
            "Epoch 492/500\n",
            "55/55 [==============================] - 0s 194us/sample - loss: 0.8217\n",
            "Epoch 493/500\n",
            "55/55 [==============================] - 0s 230us/sample - loss: 0.8216\n",
            "Epoch 494/500\n",
            "55/55 [==============================] - 0s 118us/sample - loss: 0.8215\n",
            "Epoch 495/500\n",
            "55/55 [==============================] - 0s 132us/sample - loss: 0.8191\n",
            "Epoch 496/500\n",
            "55/55 [==============================] - 0s 141us/sample - loss: 0.8195\n",
            "Epoch 497/500\n",
            "55/55 [==============================] - 0s 155us/sample - loss: 0.8185\n",
            "Epoch 498/500\n",
            "55/55 [==============================] - 0s 167us/sample - loss: 0.8189\n",
            "Epoch 499/500\n",
            "55/55 [==============================] - 0s 258us/sample - loss: 0.8186\n",
            "Epoch 500/500\n",
            "55/55 [==============================] - 0s 124us/sample - loss: 0.8172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GFcIU2-SdCrI"
      },
      "source": [
        "আমরা সামনের জুপিটার/কোলাব নোটবুকে আরো উদাহরণ দেখবো। ঘাবড়াবেন না। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0-QsNCLD4MJZ"
      },
      "source": [
        "## ট্রেনিং এর সাথে লস কমানোর একটা ছবি \n",
        "\n",
        "বরাবরের মতো ফিট মেথডটার আউটপুট একটা অবজেক্ট এ ফেরত পাঠাচ্ছি। এখানকার অবজেক্টের নাম বলছি হিস্ট্রি, সেটা যে কোন নামেই হতে পারে। এখন এই অবজেক্টকে ঠিকমতো প্লট করলে বোঝা যাবে প্রতিটা ট্রেনিং এর সাথে কিভাবে মডেলের লস কমে আসে। শুরুর দিকের বেশি লস - মানে হচ্ছে আমরা যে তাপমাত্রাকে প্রেডিক্ট করতে চাচ্ছি, তার থেকে ট্রেনিং ডাটাবেজে যে তাপমাত্রা আছে সেটা আসলে বেশি ছিল। বেশি ইপকের সাথে সাথে কমে এসেছে সেই লস এর মাত্রা।\n",
        "\n",
        "আগের বইয়ের মত আমরা এখানে ‘ম্যাটপ্লটলিব’ লাইব্রেরি ব্যবহার করছি ডেটা ভিজুয়ালাইজেশন এর জন্য। ভালোভাবে লক্ষ্য করলেই বোঝা যাবে আমাদের মডেল শুরুতেই কিন্তু কমিয়ে নিয়েছে লসের মাত্রা, তবে মাঝে সেটা একটু কমে গিয়েছিল যার শেষের দিকে সেটা একদম শূন্যের কাছাকাছি চলে গিয়েছে। এর মানে হচ্ছে মডেলটা অনেকটাই ভালো পারফরম্যান্স দেবে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IeK6BzfbdO6_",
        "outputId": "fc747e87-fbdf-4073-925d-65fa5cd889fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel(\"Loss Level\")\n",
        "plt.plot(history.history['loss'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fde74379b00>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LtQGDMob5LOD"
      },
      "source": [
        "## মডেলকে দিয়ে প্রেডিক্ট করাই তাপমাত্রা\n",
        "আমাদের হাতে চলে এলো এমন একটা মডেল যাকে ট্রেইন করা হয়েছে আমাদের ১৫ সেকেন্ডের ঝিঁঝিঁপোকার ডাকের সংখ্যার সাথে তার করেসপন্ডিং তাপমাত্রা। তাহলে তো আমরা একটা অজানা ১৫ সেকেন্ডের ঝিঁঝিঁ পোকার ডাক এর সংখ্যা দিলে মডেল বলে দিতে পারবে ওই মুহূর্তের তাপমাত্রা। ভুল বললাম?\n",
        "\n",
        "আমাদেরকে দেখতে হবে কোন ডাটাটা সেই ৫৫টা রেকর্ড এর মধ্যে নেই।\n",
        "\n",
        "৩৪, মানে  ১৫ সেকেন্ডের ঝিঁঝিঁপোকার ডাকের সংখ্যা = ৩৪\n",
        "\n",
        "এখন প্রেডিক্ট করতে হবে ওই সময়ে তাপমাত্রা কতো ছিলো? পারবোনা?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oxNzL4lS2Gui",
        "outputId": "b7b27511-6ed2-4ac4-d26c-ec78111eb66c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(model.predict([34]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[21.358658]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jApk6tZ1fBg1"
      },
      "source": [
        "এর আসল উত্তর হবে $34 \\times 0.49543811976 + 4.45863851637 = 21.303534$, এর মানে হচ্ছে আমাদের মডেল একদম প্রায় মিলিয়ে দিয়েছে।\n",
        "\n",
        "### আমরা কি করলাম?\n",
        "\n",
        "\n",
        "*   আমরা একটা ডেন্স লেয়ার মডেল তৈরি করেছি। \n",
        "*   আমরা সেটাকে ট্রেইন করেছি ২৭,৫০০ এক্সাম্পল দিয়ে (৫৫ জোড়া ইনপুট, ৫০০ ইপক).\n",
        "\n",
        "আমাদের মডেল ডেন্স লেয়ারে ইন্টারনাল ভ্যারিয়েবল (ওয়েট)গুলোকে সেভাবেই টিউন করেছে যাতে ঠিক তাপমাত্রাটা বলতে পারে যদি কেউ ১৫ সেকেন্ডের ওই সময়ের ঝিঁঝিঁপোকার ডাকের সংখ্যা দিতে পারেন। "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zRrOky5gm20Z"
      },
      "source": [
        "## তাহলে ফর্মুলা কোথায়?\n",
        "\n",
        "আমরা অনেক্ষন ডেন্স লেয়ারের ভেতর ইন্টারনাল ভ্যারিয়েবলের কথা বলেছি। সেটা কি খালি চোখে দেখা যাবে না? অবশ্যই যাবে। \n",
        "\n",
        "কেরাসের `লেয়ারের_নাম.get_weights()` দিলেই চলে আসবে নামপাই অ্যারের লিস্ট হিসেবে।  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kmIkVdkbnZJI",
        "outputId": "0068c4ac-8026-4552-a33e-022126640a59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "l0.get_weights()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0.518712]], dtype=float32), array([3.7224503], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlcaiDydmx57",
        "colab_type": "text"
      },
      "source": [
        "তাহলে আমাদের ফর্মুলা কি ছিলো?\n",
        "\n",
        "y = 0.49543811976X + 4.45863851637 [y = mX + b]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RSplSnMvnWC-"
      },
      "source": [
        "আমাদের প্রথম ভ্যারিয়েবল m হচ্ছে ~0.4954  আর পরেরটা মানে b হচ্ছে ~4.4586. এর অর্থ হচ্ছে আমাদের মেশিন লার্নিং মডেল ইনপুট ডেটা থেকে ফর্মুলা বের করে ফেলেছে। এটাই চাইছিলাম আমরা। যেহেতু এটা মাত্র একটা লেয়ার, একটা নিউরন - সেকারণে এর আউটকাম এসেছে একটা লাইনের ইকুয়েশনের মতো। "
      ]
    }
  ]
}